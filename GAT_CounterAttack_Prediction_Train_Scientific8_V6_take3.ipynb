{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOv5lkQs1dmw2cujcUSY4dB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryu622/gnn-counterattack-xai-v2/blob/feat%2Fnew-file/GAT_CounterAttack_Prediction_Train_Scientific8_V6_take3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "æœ€çµ‚çš„ã«å’è«–ã§ä½¿ç”¨ã—ãŸææ¡ˆãƒ¢ãƒ‡ãƒ«ã€‚\n",
        "ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ãªåˆ¶ç´„ã‚’åŠ ãˆãŸææ¡ˆæ‰‹æ³•ï¼‘ï¼ˆGATã‚’æ‹¡å¼µã—ã¦ä½œæˆã€‚ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã®å¯è¦–åŒ–ã€é †åˆ—ç‰¹å¾´é‡è¦åº¦ã€GNNExplainerã¾ã§å®Ÿè£…ï¼‰"
      ],
      "metadata": {
        "id": "3EW96ZqjgWTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ã‚·ãƒ¼ãƒ‰å€¤\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    # Pythonè‡ªä½“ã®ä¹±æ•°å›ºå®š\n",
        "    random.seed(seed)\n",
        "    # OSç’°å¢ƒã®ä¹±æ•°å›ºå®š\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    # Numpyã®ä¹±æ•°å›ºå®š\n",
        "    np.random.seed(seed)\n",
        "    # PyTorchã®ä¹±æ•°å›ºå®š\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # ãƒžãƒ«ãƒGPUã®å ´åˆ\n",
        "    # è¨ˆç®—ã®æ±ºå®šè«–çš„æŒ™å‹•ã‚’å¼·åˆ¶ï¼ˆã“ã‚Œã‚’å…¥ã‚Œã‚‹ã¨å°‘ã—é…ããªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ãŒã€å†ç¾æ€§ã¯å®Œç’§ã«ãªã‚Šã¾ã™ï¼‰\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# å¥½ããªæ•°å­—ï¼ˆ42ãŒä¸€èˆ¬çš„ï¼‰ã§å›ºå®š\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "QKyaHmiRGtjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcw4b0y1F0PK"
      },
      "outputs": [],
      "source": [
        "#GoogleDriveã‚’ãƒžã‚¦ãƒ³ãƒˆ\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Driveã‚’ä»®æƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ãƒžã‚¦ãƒ³ãƒˆ\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å¿…é ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "!pip install torch_geometric\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch import optim\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import re\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# è¡¨ç¤ºè¨­å®š\n",
        "np.set_printoptions(suppress=True, precision=3)\n",
        "pd.set_option('display.precision', 3)    # å°æ•°ç‚¹ä»¥ä¸‹ã®è¡¨ç¤ºæ¡\n",
        "pd.set_option('display.max_rows', 50)   # è¡¨ç¤ºã™ã‚‹è¡Œæ•°ã®ä¸Šé™\n",
        "pd.set_option('display.max_columns', 15)  # è¡¨ç¤ºã™ã‚‹åˆ—æ•°ã®ä¸Šé™\n",
        "%precision 3"
      ],
      "metadata": {
        "id": "K7DHFGzCGYlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿ã¯ä»¥å‰ã¾ã§ã¨åŒæ§˜"
      ],
      "metadata": {
        "id": "EpfKnRXp6DxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# ==========================================\n",
        "# 1. è£œåŠ©é–¢æ•°: è©¦åˆå˜ä½ã®ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
        "# ==========================================\n",
        "def balance_dataset_by_undersampling(data_list):\n",
        "    \"\"\"\n",
        "    ã‚·ãƒ¼ã‚¯ã‚¨ãƒ³ã‚¹å˜ä½ã§1:1ã«èª¿æ•´ã€‚è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ã®ã¿é©ç”¨ã€‚\n",
        "    \"\"\"\n",
        "    seq_groups = defaultdict(list)\n",
        "    for d in data_list:\n",
        "        sid = int(d.sequence_id.item()) if torch.is_tensor(d.sequence_id) else int(d.sequence_id)\n",
        "        seq_groups[sid].append(d)\n",
        "\n",
        "    l0_groups, l1_groups = [], []\n",
        "    for sid, frames in seq_groups.items():\n",
        "        label = int(frames[0].y.item())\n",
        "        if label == 0: l0_groups.append(frames)\n",
        "        else: l1_groups.append(frames)\n",
        "\n",
        "    # å°‘ãªã„æ–¹ï¼ˆæˆåŠŸï¼‰ã«åˆã‚ã›ã¦å¤±æ•—ã‚’å‰Šã‚‹\n",
        "    min_size = min(len(l0_groups), len(l1_groups))\n",
        "    sampled_l0 = random.sample(l0_groups, min_size)\n",
        "    sampled_l1 = l1_groups # æˆåŠŸã¯å…¨æ•°ä¿æŒ\n",
        "\n",
        "    balanced_list = [frame for group in (sampled_l0 + sampled_l1) for frame in group]\n",
        "    random.shuffle(balanced_list)\n",
        "\n",
        "    print(f\"    [Sampling] Success Seqs: {len(sampled_l1)} | Failure Seqs: {len(sampled_l0)} | Total Frames: {len(balanced_list)}\")\n",
        "    return balanced_list\n",
        "\n"
      ],
      "metadata": {
        "id": "Rc2Wl21OBF67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ï¼ˆPIGNNã®ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ãƒ©ã‚¹ï¼‰"
      ],
      "metadata": {
        "id": "ocDgeAXM6Ifi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_geometric.utils import softmax\n",
        "import os\n",
        "\n",
        "# ==========================================\n",
        "# 1. å‰å‡¦ç†é–¢æ•°ã®å®šç¾©\n",
        "# ==========================================\n",
        "def preprocess_batch(data, device):\n",
        "    data.x = data.x.float()\n",
        "    data.pos = data.pos.float()\n",
        "    data.vel = data.vel.float()\n",
        "    return data.to(device)\n",
        "\n",
        "# ==========================================\n",
        "# 2. ãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆå¯è¦–åŒ–å¯¾å¿œ & tauï¼‰\n",
        "# ==========================================\n",
        "class PIGNNLayer(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, tau=1.5):\n",
        "        super(PIGNNLayer, self).__init__(aggr='add')\n",
        "        self.tau = tau\n",
        "        self.lin = nn.Linear(in_channels, out_channels)\n",
        "        self.att = nn.Parameter(torch.Tensor(1, out_channels * 2))\n",
        "        nn.init.xavier_uniform_(self.att)\n",
        "        self._last_att = None  # å¯è¦–åŒ–ç”¨ã«ä¿å­˜ã™ã‚‹å¤‰æ•°\n",
        "\n",
        "    def forward(self, x, edge_index, pos, vel, return_attention=False):\n",
        "        h = self.lin(x)\n",
        "        # ãƒãƒ¼ãƒ ID(index 6)ã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã«æ¸¡ã™\n",
        "        out = self.propagate(edge_index, x=h, pos=pos, vel=vel, team=x[:, 6:7])\n",
        "\n",
        "        if return_attention:\n",
        "            # messageå†…ã§ä¿å­˜ã•ã‚ŒãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã‚’è¿”ã™\n",
        "            return out, (edge_index, self._last_att)\n",
        "        return out\n",
        "\n",
        "    def message(self, x_i, x_j, pos_i, pos_j, vel_i, vel_j, edge_index_i, team_i, team_j):\n",
        "        # æœªæ¥ä½ç½®äºˆæ¸¬ï¼ˆtau=0.2ï¼‰\n",
        "        pos_i_pred = pos_i + vel_i * self.tau\n",
        "        pos_j_pred = pos_j + vel_j * self.tau\n",
        "        dist_future = torch.norm(pos_i_pred - pos_j_pred, dim=-1, keepdim=True)\n",
        "        physics_bias = torch.exp(-dist_future / 2.0)\n",
        "\n",
        "        # ãƒãƒ¼ãƒ é–¢ä¿‚ãƒã‚¤ã‚¢ã‚¹\n",
        "        is_teammate = (team_i == team_j).float()\n",
        "        team_bias = torch.where(is_teammate > 0.5, 0.5, -0.5)\n",
        "\n",
        "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿è¨ˆç®—\n",
        "        alpha = torch.cat([x_i, x_j], dim=-1)\n",
        "        alpha = (alpha * self.att).sum(dim=-1, keepdim=True)\n",
        "        alpha = softmax(F.leaky_relu(alpha) + physics_bias + team_bias, edge_index_i)\n",
        "\n",
        "        # å¯è¦–åŒ–ç”¨ã«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã‚’è¨˜éŒ²\n",
        "        self._last_att = alpha\n",
        "        return alpha * x_j\n",
        "\n",
        "class PIGNNClassifier(nn.Module):\n",
        "    def __init__(self, hidden_channels=64):\n",
        "        super(PIGNNClassifier, self).__init__()\n",
        "        self.conv1 = PIGNNLayer(7, hidden_channels, tau=0.2)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels, tau=0.2)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data, return_attention=False):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        if return_attention:\n",
        "            # 1å±¤ç›®ï¼ˆç‰©ç†çš„ç‰¹å¾´ãŒæœ€ã‚‚å¼·ãå‡ºã‚‹å±¤ï¼‰ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’å–å¾—\n",
        "            x, (edge_idx_out, att_weights) = self.conv1(x, edge_index, pos, vel, return_attention=True)\n",
        "            x = F.elu(x)\n",
        "            x = self.conv2(x, edge_index, pos, vel)\n",
        "        else:\n",
        "            x = F.elu(self.conv1(x, edge_index, pos, vel))\n",
        "            x = self.conv2(x, edge_index, pos, vel)\n",
        "\n",
        "        x_pool = global_mean_pool(x, batch)\n",
        "        logits = F.log_softmax(self.lin(x_pool), dim=1)\n",
        "\n",
        "        if return_attention:\n",
        "            return logits, (edge_idx_out, att_weights)\n",
        "        return logits\n",
        "\n",
        "# ==========================================\n",
        "# 3. ç†è«–ä¿®æ­£ï¼šé›†å›£é‹å‹•å­¦çš„åˆ¶ç´„ï¼ˆL_physï¼‰\n",
        "# ==========================================\n",
        "def pignn_theoretical_loss(output, target, data, alpha=0.1):\n",
        "    weights = torch.tensor([1.0, 3.3], device=output.device)\n",
        "    loss_task = F.nll_loss(output, target, weight=weights)\n",
        "\n",
        "    probs = torch.exp(output)[:, 1]\n",
        "\n",
        "    # --- æ”»æ’ƒé¸æ‰‹ã®ã¿ã®æŽ¨é€²åŠ›ã‚’è¨ˆç®—ï¼ˆindex 6 ã‚’ä½¿ç”¨ï¼‰ ---\n",
        "    atk_mask = (data.x[:, 6] == 1.0)\n",
        "    all_vxs = data.vel[:, 0]\n",
        "\n",
        "    batch_size = output.size(0)\n",
        "    avg_vxs_atk = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        mask = (data.batch == i) & atk_mask\n",
        "        if mask.any():\n",
        "            avg_vxs_atk.append(torch.mean(all_vxs[mask]))\n",
        "        else:\n",
        "            avg_vxs_atk.append(torch.tensor(0.0, device=output.device))\n",
        "\n",
        "    avg_vxs_atk = torch.stack(avg_vxs_atk)\n",
        "\n",
        "    # ç‰©ç†æå¤±ï¼šSuccessç¢ºçŽ‡ Ã— ReLU(-æ”»æ’ƒå´ã®å¹³å‡vx)\n",
        "    loss_phys = torch.mean(probs * torch.relu(-avg_vxs_atk))\n",
        "\n",
        "    total_loss = loss_task + (alpha * loss_phys)\n",
        "    return total_loss, loss_task, loss_phys\n",
        "\n",
        "# ==========================================\n",
        "# 4. å­¦ç¿’ãƒ»è©•ä¾¡ãƒ«ãƒ¼ãƒ—\n",
        "# ==========================================\n",
        "def train_pignn_epoch_fixed(model, loader, optimizer, device, alpha_p):\n",
        "    model.train()\n",
        "    total_loss, total_phys = 0, 0\n",
        "    current_alpha = alpha_p\n",
        "\n",
        "    for data in loader:\n",
        "        data = preprocess_batch(data, device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss, _, l_phys = pignn_theoretical_loss(out, data.y.view(-1), data, alpha=current_alpha)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "        total_phys += l_phys.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(loader.dataset), total_phys / len(loader.dataset), current_alpha\n",
        "\n",
        "def test_pignn(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = preprocess_batch(data, device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == data.y.view(-1)).sum().item()\n",
        "    return correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "hIyIVn-LHCPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³ (CVãƒ«ãƒ¼ãƒ—) - å­¦ç¿’æ›²ç·šè¨˜éŒ²ç‰ˆ\n",
        "# ==========================================\n",
        "import matplotlib.pyplot as plt # è¿½åŠ \n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# è«–æ–‡åŒæ§˜ã€å›ºå®šå€¤ã§è©•ä¾¡ï¼ˆ0, 1.0, 10.0ãªã©ï¼‰\n",
        "FIXED_ALPHA = 50.0\n",
        "\n",
        "v16_load_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_v17_final.pt\"\n",
        "print(f\"CVç”¨ãƒžã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {v16_load_path}\")\n",
        "checkpoint = torch.load(v16_load_path, weights_only=False)\n",
        "all_data_list = checkpoint['all_data']\n",
        "\n",
        "# --- ã€ä¿®æ­£ã€‘æŽ¨æ¸¬ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã€åˆ»å°ã•ã‚ŒãŸIDã‚’ç›´æŽ¥å–å¾— ---\n",
        "match_ids = sorted(list(set([int(d.match_id.item()) for d in all_data_list])))\n",
        "print(f\"æ¤œå‡ºã•ã‚ŒãŸè©¦åˆID: {match_ids} (è¨ˆ {len(match_ids)} è©¦åˆ)\")\n",
        "\n",
        "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "EPOCHS_CV =100\n",
        "LR = 0.0005\n",
        "cv_final_reports = []\n",
        "best_overall_f1 = 0\n",
        "\n",
        "# --- ã€è¿½åŠ ã€‘å…¨CVã®å­¦ç¿’ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ç®± ---\n",
        "all_cv_history = []\n",
        "\n",
        "print(f\"PIGNN ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ (Alpha={FIXED_ALPHA} å›ºå®šãƒ¢ãƒ¼ãƒ‰)\\n\")\n",
        "\n",
        "for test_match in match_ids:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\" ðŸŒ€ Round: Match {test_match} ã‚’ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # 1. åˆ»å°ã•ã‚ŒãŸ match_id ã‚’ä¿¡ã˜ã¦åˆ‡ã‚Šåˆ†ã‘\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match]\n",
        "\n",
        "    # 2. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿1:1ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
        "    cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "\n",
        "    cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "    cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    cv_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    cv_optimizer = torch.optim.Adam(cv_model.parameters(), lr=LR)\n",
        "\n",
        "    # --- ã€è¿½åŠ ã€‘ã“ã®Roundã®ãƒ­ã‚°ã‚’ä¿æŒã™ã‚‹è¾žæ›¸ ---\n",
        "    round_history = {'total_loss': [], 'physics_loss': []}\n",
        "\n",
        "    # 3. å­¦ç¿’ãƒ«ãƒ¼ãƒ— (å›ºå®šAlphaç‰ˆ)\n",
        "    for epoch in range(1, EPOCHS_CV + 1):\n",
        "        loss, phys, _ = train_pignn_epoch_fixed(cv_model, cv_train_loader, cv_optimizer, device, FIXED_ALPHA)\n",
        "\n",
        "        # --- ã€è¿½åŠ ã€‘ãƒ­ã‚°ã‚’è¨˜éŒ² ---\n",
        "        round_history['total_loss'].append(loss)\n",
        "        round_history['physics_loss'].append(phys)\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f\"  Epoch {epoch:02d} | Loss: {loss:.4f} | Phys_L: {phys:.6f}\")\n",
        "\n",
        "    all_cv_history.append(round_history) # è¿½åŠ \n",
        "\n",
        "    # 4. è©•ä¾¡\n",
        "    cv_model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in cv_test_loader:\n",
        "            data = data.to(device)\n",
        "            out = cv_model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    # ã‚¹ã‚³ã‚¢é›†è¨ˆ\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    current_f1 = report['1']['f1-score']\n",
        "\n",
        "    cv_final_reports.append({\n",
        "        'match': test_match,\n",
        "        'recall': report['1']['recall'],\n",
        "        'precision': report['1']['precision'],\n",
        "        'f1': current_f1\n",
        "    })\n",
        "\n",
        "    # --- Î±åˆ¥ã«ãƒ•ã‚©ãƒ«ãƒ€ã‚’åˆ†ã‘ã¦æ•´ç†ä¿å­˜ ---\n",
        "    import os\n",
        "    base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "    alpha_dir = os.path.join(base_model_dir, f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}_take3\")\n",
        "    os.makedirs(alpha_dir, exist_ok=True)\n",
        "\n",
        "    model_filename = f'pignn_testmatch_{test_match}.pth'\n",
        "    model_path = os.path.join(alpha_dir, model_filename)\n",
        "    torch.save(cv_model.state_dict(), model_path)\n",
        "    print(f\" >> [Alpha {FIXED_ALPHA}] Match {test_match} weight saved.\")\n",
        "\n",
        "    if current_f1 > best_overall_f1:\n",
        "        best_overall_f1 = current_f1\n",
        "        best_model_path = os.path.join(alpha_dir, f'best_overall_alpha_{FIXED_ALPHA}.pth')\n",
        "        torch.save(cv_model.state_dict(), best_model_path)\n",
        "        print(f\"  [Alpha {FIXED_ALPHA}] New Best Model Saved: F1={best_overall_f1:.4f}\")\n",
        "\n",
        "    print(f\" >> Result: Recall={report['1']['recall']:.4f}, Precision={report['1']['precision']:.4f}, F1={current_f1:.4f}\")\n",
        "\n",
        "# 5. æœ€çµ‚é›†è¨ˆ\n",
        "print(f\"\\n\\n{'#'*60}\\n  Alpha={FIXED_ALPHA} CV æœ€çµ‚å¹³å‡çµæžœ\\n{'#'*60}\")\n",
        "avg_recall = np.mean([r['recall'] for r in cv_final_reports])\n",
        "avg_f1 = np.mean([r['f1'] for r in cv_final_reports])\n",
        "avg_precision = np.mean([r['precision'] for r in cv_final_reports])\n",
        "\n",
        "print(f\"\\n[OVERALL] Avg Success Recall:     {avg_recall:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success Precision: {avg_precision:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success F1-score:  {avg_f1:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. å­¦ç¿’æ›²ç·šã®æç”» (CVå¹³å‡) - ã€è¿½åŠ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€‘\n",
        "# ==========================================\n",
        "avg_total_loss = np.mean([h['total_loss'] for h in all_cv_history], axis=0)\n",
        "avg_phys_loss = np.mean([h['physics_loss'] for h in all_cv_history], axis=0)\n",
        "epochs_range = range(1, EPOCHS_CV + 1)\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Total Training Loss', color='tab:blue')\n",
        "ax1.plot(epochs_range, avg_total_loss, color='tab:blue', lw=2, label='Total Loss (Avg)')\n",
        "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
        "ax1.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel('Physics Loss ($L_{phys}$)', color='tab:red')\n",
        "ax2.plot(epochs_range, avg_phys_loss, color='tab:red', lw=2, linestyle=':', label='Physics Loss (Avg)')\n",
        "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "plt.title(f'PIGNN Learning Curve (CV Average, Alpha={FIXED_ALPHA})')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0uKC0kgrBrjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³ (CVãƒ«ãƒ¼ãƒ—) - å­¦ç¿’æ›²ç·šè¨˜éŒ²ç‰ˆ\n",
        "# ==========================================\n",
        "import matplotlib.pyplot as plt # è¿½åŠ \n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# è«–æ–‡åŒæ§˜ã€å›ºå®šå€¤ã§è©•ä¾¡ï¼ˆ0, 1.0, 10.0ãªã©ï¼‰\n",
        "FIXED_ALPHA = 0\n",
        "\n",
        "v16_load_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_v17_final.pt\"\n",
        "print(f\"CVç”¨ãƒžã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {v16_load_path}\")\n",
        "checkpoint = torch.load(v16_load_path, weights_only=False)\n",
        "all_data_list = checkpoint['all_data']\n",
        "\n",
        "# --- ã€ä¿®æ­£ã€‘æŽ¨æ¸¬ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã€åˆ»å°ã•ã‚ŒãŸIDã‚’ç›´æŽ¥å–å¾— ---\n",
        "match_ids = sorted(list(set([int(d.match_id.item()) for d in all_data_list])))\n",
        "print(f\"æ¤œå‡ºã•ã‚ŒãŸè©¦åˆID: {match_ids} (è¨ˆ {len(match_ids)} è©¦åˆ)\")\n",
        "\n",
        "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "EPOCHS_CV =100\n",
        "LR = 0.0005\n",
        "cv_final_reports = []\n",
        "best_overall_f1 = 0\n",
        "\n",
        "# --- ã€è¿½åŠ ã€‘å…¨CVã®å­¦ç¿’ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ç®± ---\n",
        "all_cv_history = []\n",
        "\n",
        "print(f\"PIGNN ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ (Alpha={FIXED_ALPHA} å›ºå®šãƒ¢ãƒ¼ãƒ‰)\\n\")\n",
        "\n",
        "for test_match in match_ids:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"  Round: Match {test_match} ã‚’ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # 1. åˆ»å°ã•ã‚ŒãŸ match_id ã‚’ä¿¡ã˜ã¦åˆ‡ã‚Šåˆ†ã‘\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match]\n",
        "\n",
        "    # 2. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿1:1ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
        "    cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "\n",
        "    cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "    cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    cv_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    cv_optimizer = torch.optim.Adam(cv_model.parameters(), lr=LR)\n",
        "\n",
        "    # --- ã€è¿½åŠ ã€‘ã“ã®Roundã®ãƒ­ã‚°ã‚’ä¿æŒã™ã‚‹è¾žæ›¸ ---\n",
        "    round_history = {'total_loss': [], 'physics_loss': []}\n",
        "\n",
        "    # 3. å­¦ç¿’ãƒ«ãƒ¼ãƒ— (å›ºå®šAlphaç‰ˆ)\n",
        "    for epoch in range(1, EPOCHS_CV + 1):\n",
        "        loss, phys, _ = train_pignn_epoch_fixed(cv_model, cv_train_loader, cv_optimizer, device, FIXED_ALPHA)\n",
        "\n",
        "        # --- ã€è¿½åŠ ã€‘ãƒ­ã‚°ã‚’è¨˜éŒ² ---\n",
        "        round_history['total_loss'].append(loss)\n",
        "        round_history['physics_loss'].append(phys)\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f\"  Epoch {epoch:02d} | Loss: {loss:.4f} | Phys_L: {phys:.6f}\")\n",
        "\n",
        "    all_cv_history.append(round_history) # è¿½åŠ \n",
        "\n",
        "    # 4. è©•ä¾¡\n",
        "    cv_model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in cv_test_loader:\n",
        "            data = data.to(device)\n",
        "            out = cv_model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    # ã‚¹ã‚³ã‚¢é›†è¨ˆ\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    current_f1 = report['1']['f1-score']\n",
        "\n",
        "    cv_final_reports.append({\n",
        "        'match': test_match,\n",
        "        'recall': report['1']['recall'],\n",
        "        'precision': report['1']['precision'],\n",
        "        'f1': current_f1\n",
        "    })\n",
        "\n",
        "    # --- Î±åˆ¥ã«ãƒ•ã‚©ãƒ«ãƒ€ã‚’åˆ†ã‘ã¦æ•´ç†ä¿å­˜ ---\n",
        "    import os\n",
        "    base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "    alpha_dir = os.path.join(base_model_dir, f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}_take3\")\n",
        "    os.makedirs(alpha_dir, exist_ok=True)\n",
        "\n",
        "    model_filename = f'pignn_testmatch_{test_match}.pth'\n",
        "    model_path = os.path.join(alpha_dir, model_filename)\n",
        "    torch.save(cv_model.state_dict(), model_path)\n",
        "    print(f\" >> [Alpha {FIXED_ALPHA}] Match {test_match} weight saved.\")\n",
        "\n",
        "    if current_f1 > best_overall_f1:\n",
        "        best_overall_f1 = current_f1\n",
        "        best_model_path = os.path.join(alpha_dir, f'best_overall_alpha_{FIXED_ALPHA}.pth')\n",
        "        torch.save(cv_model.state_dict(), best_model_path)\n",
        "        print(f\"  [Alpha {FIXED_ALPHA}] New Best Model Saved: F1={best_overall_f1:.4f}\")\n",
        "\n",
        "    print(f\" >> Result: Recall={report['1']['recall']:.4f}, Precision={report['1']['precision']:.4f}, F1={current_f1:.4f}\")\n",
        "\n",
        "# 5. æœ€çµ‚é›†è¨ˆ\n",
        "print(f\"\\n\\n{'#'*60}\\n  Alpha={FIXED_ALPHA} CV æœ€çµ‚å¹³å‡çµæžœ\\n{'#'*60}\")\n",
        "avg_recall = np.mean([r['recall'] for r in cv_final_reports])\n",
        "avg_f1 = np.mean([r['f1'] for r in cv_final_reports])\n",
        "avg_precision = np.mean([r['precision'] for r in cv_final_reports])\n",
        "\n",
        "print(f\"\\n[OVERALL] Avg Success Recall:     {avg_recall:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success Precision: {avg_precision:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success F1-score:  {avg_f1:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. å­¦ç¿’æ›²ç·šã®æç”» (CVå¹³å‡) - ã€è¿½åŠ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€‘\n",
        "# ==========================================\n",
        "avg_total_loss = np.mean([h['total_loss'] for h in all_cv_history], axis=0)\n",
        "avg_phys_loss = np.mean([h['physics_loss'] for h in all_cv_history], axis=0)\n",
        "epochs_range = range(1, EPOCHS_CV + 1)\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Total Training Loss', color='tab:blue')\n",
        "ax1.plot(epochs_range, avg_total_loss, color='tab:blue', lw=2, label='Total Loss (Avg)')\n",
        "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
        "ax1.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel('Physics Loss ($L_{phys}$)', color='tab:red')\n",
        "ax2.plot(epochs_range, avg_phys_loss, color='tab:red', lw=2, linestyle=':', label='Physics Loss (Avg)')\n",
        "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "plt.title(f'PIGNN Learning Curve (CV Average, Alpha={FIXED_ALPHA})')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ANiHTpO0OOmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³ (CVãƒ«ãƒ¼ãƒ—) - å­¦ç¿’æ›²ç·šè¨˜éŒ²ç‰ˆ\n",
        "# ==========================================\n",
        "import matplotlib.pyplot as plt # è¿½åŠ \n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# è«–æ–‡åŒæ§˜ã€å›ºå®šå€¤ã§è©•ä¾¡ï¼ˆ0, 1.0, 10.0ãªã©ï¼‰\n",
        "FIXED_ALPHA = 10.0\n",
        "\n",
        "v16_load_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_v17_final.pt\"\n",
        "print(f\"CVç”¨ãƒžã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {v16_load_path}\")\n",
        "checkpoint = torch.load(v16_load_path, weights_only=False)\n",
        "all_data_list = checkpoint['all_data']\n",
        "\n",
        "# --- ã€ä¿®æ­£ã€‘æŽ¨æ¸¬ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã€åˆ»å°ã•ã‚ŒãŸIDã‚’ç›´æŽ¥å–å¾— ---\n",
        "match_ids = sorted(list(set([int(d.match_id.item()) for d in all_data_list])))\n",
        "print(f\"æ¤œå‡ºã•ã‚ŒãŸè©¦åˆID: {match_ids} (è¨ˆ {len(match_ids)} è©¦åˆ)\")\n",
        "\n",
        "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "EPOCHS_CV =100\n",
        "LR = 0.0005\n",
        "cv_final_reports = []\n",
        "best_overall_f1 = 0\n",
        "\n",
        "# --- ã€è¿½åŠ ã€‘å…¨CVã®å­¦ç¿’ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ç®± ---\n",
        "all_cv_history = []\n",
        "\n",
        "print(f\"PIGNN ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ (Alpha={FIXED_ALPHA} å›ºå®šãƒ¢ãƒ¼ãƒ‰)\\n\")\n",
        "\n",
        "for test_match in match_ids:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\" ðŸŒ€ Round: Match {test_match} ã‚’ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # 1. åˆ»å°ã•ã‚ŒãŸ match_id ã‚’ä¿¡ã˜ã¦åˆ‡ã‚Šåˆ†ã‘\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match]\n",
        "\n",
        "    # 2. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿1:1ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
        "    cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "\n",
        "    cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "    cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    cv_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    cv_optimizer = torch.optim.Adam(cv_model.parameters(), lr=LR)\n",
        "\n",
        "    # --- ã€è¿½åŠ ã€‘ã“ã®Roundã®ãƒ­ã‚°ã‚’ä¿æŒã™ã‚‹è¾žæ›¸ ---\n",
        "    round_history = {'total_loss': [], 'physics_loss': []}\n",
        "\n",
        "    # 3. å­¦ç¿’ãƒ«ãƒ¼ãƒ— (å›ºå®šAlphaç‰ˆ)\n",
        "    for epoch in range(1, EPOCHS_CV + 1):\n",
        "        loss, phys, _ = train_pignn_epoch_fixed(cv_model, cv_train_loader, cv_optimizer, device, FIXED_ALPHA)\n",
        "\n",
        "        # --- ã€è¿½åŠ ã€‘ãƒ­ã‚°ã‚’è¨˜éŒ² ---\n",
        "        round_history['total_loss'].append(loss)\n",
        "        round_history['physics_loss'].append(phys)\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f\"  Epoch {epoch:02d} | Loss: {loss:.4f} | Phys_L: {phys:.6f}\")\n",
        "\n",
        "    all_cv_history.append(round_history) # è¿½åŠ \n",
        "\n",
        "    # 4. è©•ä¾¡\n",
        "    cv_model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in cv_test_loader:\n",
        "            data = data.to(device)\n",
        "            out = cv_model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    # ã‚¹ã‚³ã‚¢é›†è¨ˆ\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    current_f1 = report['1']['f1-score']\n",
        "\n",
        "    cv_final_reports.append({\n",
        "        'match': test_match,\n",
        "        'recall': report['1']['recall'],\n",
        "        'precision': report['1']['precision'],\n",
        "        'f1': current_f1\n",
        "    })\n",
        "\n",
        "    # --- Î±åˆ¥ã«ãƒ•ã‚©ãƒ«ãƒ€ã‚’åˆ†ã‘ã¦æ•´ç†ä¿å­˜ ---\n",
        "    import os\n",
        "    base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "    alpha_dir = os.path.join(base_model_dir, f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}_take3\")\n",
        "    os.makedirs(alpha_dir, exist_ok=True)\n",
        "\n",
        "    model_filename = f'pignn_testmatch_{test_match}.pth'\n",
        "    model_path = os.path.join(alpha_dir, model_filename)\n",
        "    torch.save(cv_model.state_dict(), model_path)\n",
        "    print(f\" >> [Alpha {FIXED_ALPHA}] Match {test_match} weight saved.\")\n",
        "\n",
        "    if current_f1 > best_overall_f1:\n",
        "        best_overall_f1 = current_f1\n",
        "        best_model_path = os.path.join(alpha_dir, f'best_overall_alpha_{FIXED_ALPHA}.pth')\n",
        "        torch.save(cv_model.state_dict(), best_model_path)\n",
        "        print(f\"  [Alpha {FIXED_ALPHA}] New Best Model Saved: F1={best_overall_f1:.4f}\")\n",
        "\n",
        "    print(f\" >> Result: Recall={report['1']['recall']:.4f}, Precision={report['1']['precision']:.4f}, F1={current_f1:.4f}\")\n",
        "\n",
        "# 5. æœ€çµ‚é›†è¨ˆ\n",
        "print(f\"\\n\\n{'#'*60}\\n  Alpha={FIXED_ALPHA} CV æœ€çµ‚å¹³å‡çµæžœ\\n{'#'*60}\")\n",
        "avg_recall = np.mean([r['recall'] for r in cv_final_reports])\n",
        "avg_f1 = np.mean([r['f1'] for r in cv_final_reports])\n",
        "avg_precision = np.mean([r['precision'] for r in cv_final_reports])\n",
        "\n",
        "print(f\"\\n[OVERALL] Avg Success Recall:     {avg_recall:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success Precision: {avg_precision:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success F1-score:  {avg_f1:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. å­¦ç¿’æ›²ç·šã®æç”» (CVå¹³å‡) - ã€è¿½åŠ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€‘\n",
        "# ==========================================\n",
        "avg_total_loss = np.mean([h['total_loss'] for h in all_cv_history], axis=0)\n",
        "avg_phys_loss = np.mean([h['physics_loss'] for h in all_cv_history], axis=0)\n",
        "epochs_range = range(1, EPOCHS_CV + 1)\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Total Training Loss', color='tab:blue')\n",
        "ax1.plot(epochs_range, avg_total_loss, color='tab:blue', lw=2, label='Total Loss (Avg)')\n",
        "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
        "ax1.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel('Physics Loss ($L_{phys}$)', color='tab:red')\n",
        "ax2.plot(epochs_range, avg_phys_loss, color='tab:red', lw=2, linestyle=':', label='Physics Loss (Avg)')\n",
        "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "plt.title(f'PIGNN Learning Curve (CV Average, Alpha={FIXED_ALPHA})')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tH1AqLxb5ZGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³ (CVãƒ«ãƒ¼ãƒ—) - å­¦ç¿’æ›²ç·šè¨˜éŒ²ç‰ˆ\n",
        "# ==========================================\n",
        "import matplotlib.pyplot as plt # è¿½åŠ \n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# è«–æ–‡åŒæ§˜ã€å›ºå®šå€¤ã§è©•ä¾¡ï¼ˆ0, 1.0, 10.0ãªã©ï¼‰\n",
        "FIXED_ALPHA = 100.0\n",
        "v16_load_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_v17_final.pt\"\n",
        "print(f\"CVç”¨ãƒžã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {v16_load_path}\")\n",
        "checkpoint = torch.load(v16_load_path, weights_only=False)\n",
        "all_data_list = checkpoint['all_data']\n",
        "\n",
        "# --- ã€ä¿®æ­£ã€‘æŽ¨æ¸¬ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã€åˆ»å°ã•ã‚ŒãŸIDã‚’ç›´æŽ¥å–å¾— ---\n",
        "match_ids = sorted(list(set([int(d.match_id.item()) for d in all_data_list])))\n",
        "print(f\"æ¤œå‡ºã•ã‚ŒãŸè©¦åˆID: {match_ids} (è¨ˆ {len(match_ids)} è©¦åˆ)\")\n",
        "\n",
        "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "EPOCHS_CV =100\n",
        "LR = 0.0005\n",
        "cv_final_reports = []\n",
        "best_overall_f1 = 0\n",
        "\n",
        "# --- ã€è¿½åŠ ã€‘å…¨CVã®å­¦ç¿’ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ç®± ---\n",
        "all_cv_history = []\n",
        "\n",
        "print(f\"PIGNN ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ (Alpha={FIXED_ALPHA} å›ºå®šãƒ¢ãƒ¼ãƒ‰)\\n\")\n",
        "\n",
        "for test_match in match_ids:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"  Round: Match {test_match} ã‚’ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # 1. åˆ»å°ã•ã‚ŒãŸ match_id ã‚’ä¿¡ã˜ã¦åˆ‡ã‚Šåˆ†ã‘\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match]\n",
        "\n",
        "    # 2. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿1:1ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
        "    cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "\n",
        "    cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "    cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    cv_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    cv_optimizer = torch.optim.Adam(cv_model.parameters(), lr=LR)\n",
        "\n",
        "    # --- ã€è¿½åŠ ã€‘ã“ã®Roundã®ãƒ­ã‚°ã‚’ä¿æŒã™ã‚‹è¾žæ›¸ ---\n",
        "    round_history = {'total_loss': [], 'physics_loss': []}\n",
        "\n",
        "    # 3. å­¦ç¿’ãƒ«ãƒ¼ãƒ— (å›ºå®šAlphaç‰ˆ)\n",
        "    for epoch in range(1, EPOCHS_CV + 1):\n",
        "        loss, phys, _ = train_pignn_epoch_fixed(cv_model, cv_train_loader, cv_optimizer, device, FIXED_ALPHA)\n",
        "\n",
        "        # --- ã€è¿½åŠ ã€‘ãƒ­ã‚°ã‚’è¨˜éŒ² ---\n",
        "        round_history['total_loss'].append(loss)\n",
        "        round_history['physics_loss'].append(phys)\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f\"  Epoch {epoch:02d} | Loss: {loss:.4f} | Phys_L: {phys:.6f}\")\n",
        "\n",
        "    all_cv_history.append(round_history) # è¿½åŠ \n",
        "\n",
        "    # 4. è©•ä¾¡\n",
        "    cv_model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in cv_test_loader:\n",
        "            data = data.to(device)\n",
        "            out = cv_model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    # ã‚¹ã‚³ã‚¢é›†è¨ˆ\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    current_f1 = report['1']['f1-score']\n",
        "\n",
        "    cv_final_reports.append({\n",
        "        'match': test_match,\n",
        "        'recall': report['1']['recall'],\n",
        "        'precision': report['1']['precision'],\n",
        "        'f1': current_f1\n",
        "    })\n",
        "\n",
        "    # --- Î±åˆ¥ã«ãƒ•ã‚©ãƒ«ãƒ€ã‚’åˆ†ã‘ã¦æ•´ç†ä¿å­˜ ---\n",
        "    import os\n",
        "    base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "    alpha_dir = os.path.join(base_model_dir, f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}_take3\")\n",
        "    os.makedirs(alpha_dir, exist_ok=True)\n",
        "\n",
        "    model_filename = f'pignn_testmatch_{test_match}.pth'\n",
        "    model_path = os.path.join(alpha_dir, model_filename)\n",
        "    torch.save(cv_model.state_dict(), model_path)\n",
        "    print(f\" >> [Alpha {FIXED_ALPHA}] Match {test_match} weight saved.\")\n",
        "\n",
        "    if current_f1 > best_overall_f1:\n",
        "        best_overall_f1 = current_f1\n",
        "        best_model_path = os.path.join(alpha_dir, f'best_overall_alpha_{FIXED_ALPHA}.pth')\n",
        "        torch.save(cv_model.state_dict(), best_model_path)\n",
        "        print(f\"  [Alpha {FIXED_ALPHA}] New Best Model Saved: F1={best_overall_f1:.4f}\")\n",
        "\n",
        "    print(f\" >> Result: Recall={report['1']['recall']:.4f}, Precision={report['1']['precision']:.4f}, F1={current_f1:.4f}\")\n",
        "\n",
        "# 5. æœ€çµ‚é›†è¨ˆ\n",
        "print(f\"\\n\\n{'#'*60}\\n  Alpha={FIXED_ALPHA} CV æœ€çµ‚å¹³å‡çµæžœ\\n{'#'*60}\")\n",
        "avg_recall = np.mean([r['recall'] for r in cv_final_reports])\n",
        "avg_f1 = np.mean([r['f1'] for r in cv_final_reports])\n",
        "avg_precision = np.mean([r['precision'] for r in cv_final_reports])\n",
        "\n",
        "print(f\"\\n[OVERALL] Avg Success Recall:     {avg_recall:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success Precision: {avg_precision:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success F1-score:  {avg_f1:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. å­¦ç¿’æ›²ç·šã®æç”» (CVå¹³å‡) - ã€è¿½åŠ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€‘\n",
        "# ==========================================\n",
        "avg_total_loss = np.mean([h['total_loss'] for h in all_cv_history], axis=0)\n",
        "avg_phys_loss = np.mean([h['physics_loss'] for h in all_cv_history], axis=0)\n",
        "epochs_range = range(1, EPOCHS_CV + 1)\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Total Training Loss', color='tab:blue')\n",
        "ax1.plot(epochs_range, avg_total_loss, color='tab:blue', lw=2, label='Total Loss (Avg)')\n",
        "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
        "ax1.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel('Physics Loss ($L_{phys}$)', color='tab:red')\n",
        "ax2.plot(epochs_range, avg_phys_loss, color='tab:red', lw=2, linestyle=':', label='Physics Loss (Avg)')\n",
        "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "plt.title(f'PIGNN Learning Curve (CV Average, Alpha={FIXED_ALPHA})')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QspcV5OXnWt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³ (CVãƒ«ãƒ¼ãƒ—) - å­¦ç¿’æ›²ç·šè¨˜éŒ²ç‰ˆ\n",
        "# ==========================================\n",
        "import matplotlib.pyplot as plt # è¿½åŠ \n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# è«–æ–‡åŒæ§˜ã€å›ºå®šå€¤ã§è©•ä¾¡ï¼ˆ0, 1.0, 10.0ãªã©ï¼‰\n",
        "FIXED_ALPHA = 200.0\n",
        "v16_load_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_v17_final.pt\"\n",
        "print(f\"CVç”¨ãƒžã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {v16_load_path}\")\n",
        "checkpoint = torch.load(v16_load_path, weights_only=False)\n",
        "all_data_list = checkpoint['all_data']\n",
        "\n",
        "# --- ã€ä¿®æ­£ã€‘æŽ¨æ¸¬ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã€åˆ»å°ã•ã‚ŒãŸIDã‚’ç›´æŽ¥å–å¾— ---\n",
        "match_ids = sorted(list(set([int(d.match_id.item()) for d in all_data_list])))\n",
        "print(f\"æ¤œå‡ºã•ã‚ŒãŸè©¦åˆID: {match_ids} (è¨ˆ {len(match_ids)} è©¦åˆ)\")\n",
        "\n",
        "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "EPOCHS_CV =100\n",
        "LR = 0.0005\n",
        "cv_final_reports = []\n",
        "best_overall_f1 = 0\n",
        "\n",
        "# --- ã€è¿½åŠ ã€‘å…¨CVã®å­¦ç¿’ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ç®± ---\n",
        "all_cv_history = []\n",
        "\n",
        "print(f\"PIGNN ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ (Alpha={FIXED_ALPHA} å›ºå®šãƒ¢ãƒ¼ãƒ‰)\\n\")\n",
        "\n",
        "for test_match in match_ids:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"  Round: Match {test_match} ã‚’ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # 1. åˆ»å°ã•ã‚ŒãŸ match_id ã‚’ä¿¡ã˜ã¦åˆ‡ã‚Šåˆ†ã‘\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match]\n",
        "\n",
        "    # 2. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿1:1ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
        "    cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "\n",
        "    cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "    cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    cv_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    cv_optimizer = torch.optim.Adam(cv_model.parameters(), lr=LR)\n",
        "\n",
        "    # --- ã€è¿½åŠ ã€‘ã“ã®Roundã®ãƒ­ã‚°ã‚’ä¿æŒã™ã‚‹è¾žæ›¸ ---\n",
        "    round_history = {'total_loss': [], 'physics_loss': []}\n",
        "\n",
        "    # 3. å­¦ç¿’ãƒ«ãƒ¼ãƒ— (å›ºå®šAlphaç‰ˆ)\n",
        "    for epoch in range(1, EPOCHS_CV + 1):\n",
        "        loss, phys, _ = train_pignn_epoch_fixed(cv_model, cv_train_loader, cv_optimizer, device, FIXED_ALPHA)\n",
        "\n",
        "        # --- ã€è¿½åŠ ã€‘ãƒ­ã‚°ã‚’è¨˜éŒ² ---\n",
        "        round_history['total_loss'].append(loss)\n",
        "        round_history['physics_loss'].append(phys)\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f\"  Epoch {epoch:02d} | Loss: {loss:.4f} | Phys_L: {phys:.6f}\")\n",
        "\n",
        "    all_cv_history.append(round_history) # è¿½åŠ \n",
        "\n",
        "    # 4. è©•ä¾¡\n",
        "    cv_model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in cv_test_loader:\n",
        "            data = data.to(device)\n",
        "            out = cv_model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    # ã‚¹ã‚³ã‚¢é›†è¨ˆ\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    current_f1 = report['1']['f1-score']\n",
        "\n",
        "    cv_final_reports.append({\n",
        "        'match': test_match,\n",
        "        'recall': report['1']['recall'],\n",
        "        'precision': report['1']['precision'],\n",
        "        'f1': current_f1\n",
        "    })\n",
        "\n",
        "    # --- Î±åˆ¥ã«ãƒ•ã‚©ãƒ«ãƒ€ã‚’åˆ†ã‘ã¦æ•´ç†ä¿å­˜ ---\n",
        "    import os\n",
        "    base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "    alpha_dir = os.path.join(base_model_dir, f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}_take3\")\n",
        "    os.makedirs(alpha_dir, exist_ok=True)\n",
        "\n",
        "    model_filename = f'pignn_testmatch_{test_match}.pth'\n",
        "    model_path = os.path.join(alpha_dir, model_filename)\n",
        "    torch.save(cv_model.state_dict(), model_path)\n",
        "    print(f\" >> [Alpha {FIXED_ALPHA}] Match {test_match} weight saved.\")\n",
        "\n",
        "    if current_f1 > best_overall_f1:\n",
        "        best_overall_f1 = current_f1\n",
        "        best_model_path = os.path.join(alpha_dir, f'best_overall_alpha_{FIXED_ALPHA}.pth')\n",
        "        torch.save(cv_model.state_dict(), best_model_path)\n",
        "        print(f\"  [Alpha {FIXED_ALPHA}] New Best Model Saved: F1={best_overall_f1:.4f}\")\n",
        "\n",
        "    print(f\" >> Result: Recall={report['1']['recall']:.4f}, Precision={report['1']['precision']:.4f}, F1={current_f1:.4f}\")\n",
        "\n",
        "# 5. æœ€çµ‚é›†è¨ˆ\n",
        "print(f\"\\n\\n{'#'*60}\\n  Alpha={FIXED_ALPHA} CV æœ€çµ‚å¹³å‡çµæžœ\\n{'#'*60}\")\n",
        "avg_recall = np.mean([r['recall'] for r in cv_final_reports])\n",
        "avg_f1 = np.mean([r['f1'] for r in cv_final_reports])\n",
        "avg_precision = np.mean([r['precision'] for r in cv_final_reports])\n",
        "\n",
        "print(f\"\\n[OVERALL] Avg Success Recall:     {avg_recall:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success Precision: {avg_precision:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success F1-score:  {avg_f1:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. å­¦ç¿’æ›²ç·šã®æç”» (CVå¹³å‡) - ã€è¿½åŠ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€‘\n",
        "# ==========================================\n",
        "avg_total_loss = np.mean([h['total_loss'] for h in all_cv_history], axis=0)\n",
        "avg_phys_loss = np.mean([h['physics_loss'] for h in all_cv_history], axis=0)\n",
        "epochs_range = range(1, EPOCHS_CV + 1)\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Total Training Loss', color='tab:blue')\n",
        "ax1.plot(epochs_range, avg_total_loss, color='tab:blue', lw=2, label='Total Loss (Avg)')\n",
        "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
        "ax1.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel('Physics Loss ($L_{phys}$)', color='tab:red')\n",
        "ax2.plot(epochs_range, avg_phys_loss, color='tab:red', lw=2, linestyle=':', label='Physics Loss (Avg)')\n",
        "ax2.tick_params(axis='y', labelcolor='tab:red')\n",
        "\n",
        "plt.title(f'PIGNN Learning Curve (CV Average, Alpha={FIXED_ALPHA})')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FbT2y0hMpId9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# ==========================================\n",
        "# è¨­å®šï¼šæ¤œè¨¼ã—ãŸã„ Alpha ã‚’æŒ‡å®š\n",
        "# ==========================================\n",
        "FIXED_ALPHA = 0  # æ¯”è¼ƒã®ãŸã‚ã«ã“ã“ã‚’ 0 ã‚„ 1.0 ã«åˆ‡ã‚Šæ›¿ãˆã¦å®Ÿè¡Œ\n",
        "base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "# _take2 ã‚’å«ã‚ã‚‹å ´åˆ\n",
        "alpha_folder = f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}_take3\"\n",
        "model_load_dir = os.path.join(base_model_dir, alpha_folder)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "success_team_vxs = []\n",
        "\n",
        "print(f\" Alpha={FIXED_ALPHA} ã®å…¨è©¦åˆãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆè©•ä¾¡ä¸­...\")\n",
        "\n",
        "# --- CVã®çµæžœã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€å„è©¦åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚¹ãƒˆ ---\n",
        "# match_ids ã¯ CVå®Ÿè¡Œæ™‚ã¨åŒã˜ [1, 2, 3, 4, 5, 6, 7]\n",
        "for test_match in match_ids:\n",
        "    # 1. ãã®è©¦åˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # 2. ãã®è©¦åˆã§è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "    model_path = os.path.join(model_load_dir, f'pignn_testmatch_{test_match}.pth')\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\" Skip: {model_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
        "        continue\n",
        "\n",
        "    model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # 3. äºˆæ¸¬ã¨ç‰©ç†æƒ…å ±ã®æŠ½å‡º\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            # å„ã‚·ãƒ¼ãƒ³(ã‚°ãƒ©ãƒ•)ã”ã¨ã®ç‰©ç†é‡æŠ½å‡º\n",
        "            for i in range(out.size(0)):\n",
        "                mask = (data.batch == i)\n",
        "                # è«–æ–‡(cite: 12)ã®Permutation Importanceã§æœ€é‡è¦è¦–ã•ã‚ŒãŸvxã‚’è¨ˆç®—\n",
        "                avg_vx = torch.mean(data.vel[mask, 0]).item()\n",
        "\n",
        "                if pred[i] == 1: # AIãŒæˆåŠŸ(1)ã¨äºˆæ¸¬ã—ãŸæ™‚ã®ã¿\n",
        "                    success_team_vxs.append(avg_vx)\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# ==========================================\n",
        "# 4. ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤ºã‚»ã‚¯ã‚·ãƒ§ãƒ³\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"  PIGNN æœ€çµ‚ã‚¯ãƒ©ã‚·ãƒ•ã‚£ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒãƒ¼ãƒˆ (Alpha={FIXED_ALPHA})\")\n",
        "print(\"=\"*60)\n",
        "# è«–æ–‡(cite: 115)ã®ã€ŒnaÃ¯ve baselineã€ã¨ã®æ¯”è¼ƒã‚’å¿µé ­ã«ç½®ã„ãŸå‡ºåŠ›\n",
        "print(classification_report(all_labels, all_preds, target_names=['Failure', 'Success'], zero_division=0))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"  ç‰©ç†çš„æ•´åˆæ€§ æ¤œè¨¼ (vx = Byline to Byline Speed)\")\n",
        "print(\"=\"*60)\n",
        "if len(success_team_vxs) > 0:\n",
        "    avg_team_vx = np.mean(success_team_vxs)\n",
        "    # è«–æ–‡(cite: 196)ã§ã€Œhighest impactã€ã¨ã•ã‚ŒãŸvxã®æ–¹å‘æ€§ã‚’ç¢ºèª\n",
        "    positive_ratio = np.sum(np.array(success_team_vxs) > 0) / len(success_team_vxs)\n",
        "\n",
        "    print(f\"æˆåŠŸäºˆæ¸¬ã‚·ãƒ¼ãƒ³ã®å¹³å‡ vx: {avg_team_vx:.4f} m/s\")\n",
        "    print(f\"å³å‘ã(æ­£æ–¹å‘)ã¸ã®æŽ¨é€²åŠ›å‰²åˆ: {positive_ratio*100:.1f} %\")\n",
        "\n",
        "    # è«–æ–‡(cite: 6, 33)ã®ã€Œhigh speed attackã€ã®å®šç¾©ã«åŸºã¥ãè©•ä¾¡\n",
        "    if positive_ratio > 0.65:\n",
        "        print(\">> åˆ¤å®š: ç‰©ç†çš„å¦¥å½“ã€‚ãƒ¢ãƒ‡ãƒ«ã¯è«–æ–‡ã®å®šç¾©é€šã‚Šã€Žå‰æ–¹ã¸ã®é€Ÿåº¦ã€ã‚’é‡è¦–ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "    else:\n",
        "        print(\">> åˆ¤å®š: ç‰©ç†çš„ä¹–é›¢ã‚ã‚Šã€‚æˆ¦è¡“çš„ç‰¹å¾´ã‚ˆã‚Šã‚‚ãƒŽã‚¤ã‚ºã‚’å­¦ç¿’ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\"Successã¨äºˆæ¸¬ã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "\n",
        "# 5. æ··åŒè¡Œåˆ—ã®æç”»\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens' if FIXED_ALPHA > 0 else 'Blues',\n",
        "            xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title(f'Confusion Matrix (Alpha={FIXED_ALPHA})')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YobBTnxNrLh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# ==========================================\n",
        "# è¨­å®šï¼šæ¤œè¨¼ã—ãŸã„ Alpha ã‚’æŒ‡å®š\n",
        "# ==========================================\n",
        "FIXED_ALPHA = 50.0  # æ¯”è¼ƒã®ãŸã‚ã«ã“ã“ã‚’ 0 ã‚„ 1.0 ã«åˆ‡ã‚Šæ›¿ãˆã¦å®Ÿè¡Œ\n",
        "base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "# _take2 ã‚’å«ã‚ã‚‹å ´åˆ\n",
        "alpha_folder = f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}_take3\"\n",
        "model_load_dir = os.path.join(base_model_dir, alpha_folder)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "success_team_vxs = []\n",
        "\n",
        "print(f\" Alpha={FIXED_ALPHA} ã®å…¨è©¦åˆãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆè©•ä¾¡ä¸­...\")\n",
        "\n",
        "# --- CVã®çµæžœã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€å„è©¦åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚¹ãƒˆ ---\n",
        "# match_ids ã¯ CVå®Ÿè¡Œæ™‚ã¨åŒã˜ [1, 2, 3, 4, 5, 6, 7]\n",
        "for test_match in match_ids:\n",
        "    # 1. ãã®è©¦åˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # 2. ãã®è©¦åˆã§è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "    model_path = os.path.join(model_load_dir, f'pignn_testmatch_{test_match}.pth')\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\" Skip: {model_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
        "        continue\n",
        "\n",
        "    model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # 3. äºˆæ¸¬ã¨ç‰©ç†æƒ…å ±ã®æŠ½å‡º\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            # å„ã‚·ãƒ¼ãƒ³(ã‚°ãƒ©ãƒ•)ã”ã¨ã®ç‰©ç†é‡æŠ½å‡º\n",
        "            for i in range(out.size(0)):\n",
        "                mask = (data.batch == i)\n",
        "                # è«–æ–‡(cite: 12)ã®Permutation Importanceã§æœ€é‡è¦è¦–ã•ã‚ŒãŸvxã‚’è¨ˆç®—\n",
        "                avg_vx = torch.mean(data.vel[mask, 0]).item()\n",
        "\n",
        "                if pred[i] == 1: # AIãŒæˆåŠŸ(1)ã¨äºˆæ¸¬ã—ãŸæ™‚ã®ã¿\n",
        "                    success_team_vxs.append(avg_vx)\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# ==========================================\n",
        "# 4. ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤ºã‚»ã‚¯ã‚·ãƒ§ãƒ³\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"  PIGNN æœ€çµ‚ã‚¯ãƒ©ã‚·ãƒ•ã‚£ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒãƒ¼ãƒˆ (Alpha={FIXED_ALPHA})\")\n",
        "print(\"=\"*60)\n",
        "# è«–æ–‡(cite: 115)ã®ã€ŒnaÃ¯ve baselineã€ã¨ã®æ¯”è¼ƒã‚’å¿µé ­ã«ç½®ã„ãŸå‡ºåŠ›\n",
        "print(classification_report(all_labels, all_preds, target_names=['Failure', 'Success'], zero_division=0))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"  ç‰©ç†çš„æ•´åˆæ€§ æ¤œè¨¼ (vx = Byline to Byline Speed)\")\n",
        "print(\"=\"*60)\n",
        "if len(success_team_vxs) > 0:\n",
        "    avg_team_vx = np.mean(success_team_vxs)\n",
        "    # è«–æ–‡(cite: 196)ã§ã€Œhighest impactã€ã¨ã•ã‚ŒãŸvxã®æ–¹å‘æ€§ã‚’ç¢ºèª\n",
        "    positive_ratio = np.sum(np.array(success_team_vxs) > 0) / len(success_team_vxs)\n",
        "\n",
        "    print(f\"æˆåŠŸäºˆæ¸¬ã‚·ãƒ¼ãƒ³ã®å¹³å‡ vx: {avg_team_vx:.4f} m/s\")\n",
        "    print(f\"å³å‘ã(æ­£æ–¹å‘)ã¸ã®æŽ¨é€²åŠ›å‰²åˆ: {positive_ratio*100:.1f} %\")\n",
        "\n",
        "    # è«–æ–‡(cite: 6, 33)ã®ã€Œhigh speed attackã€ã®å®šç¾©ã«åŸºã¥ãè©•ä¾¡\n",
        "    if positive_ratio > 0.65:\n",
        "        print(\">> åˆ¤å®š: ç‰©ç†çš„å¦¥å½“ã€‚ãƒ¢ãƒ‡ãƒ«ã¯è«–æ–‡ã®å®šç¾©é€šã‚Šã€Žå‰æ–¹ã¸ã®é€Ÿåº¦ã€ã‚’é‡è¦–ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "    else:\n",
        "        print(\">> åˆ¤å®š: ç‰©ç†çš„ä¹–é›¢ã‚ã‚Šã€‚æˆ¦è¡“çš„ç‰¹å¾´ã‚ˆã‚Šã‚‚ãƒŽã‚¤ã‚ºã‚’å­¦ç¿’ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\"Successã¨äºˆæ¸¬ã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "\n",
        "# 5. æ··åŒè¡Œåˆ—ã®æç”»\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens' if FIXED_ALPHA > 0 else 'Blues',\n",
        "            xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title(f'Confusion Matrix (Alpha={FIXED_ALPHA})')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "83Y6OBvdrfJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# ==========================================\n",
        "# è¨­å®šï¼šæ¤œè¨¼ã—ãŸã„ Alpha ã‚’æŒ‡å®š\n",
        "# ==========================================\n",
        "FIXED_ALPHA = 10.0  # æ¯”è¼ƒã®ãŸã‚ã«ã“ã“ã‚’ 0 ã‚„ 1.0 ã«åˆ‡ã‚Šæ›¿ãˆã¦å®Ÿè¡Œ\n",
        "base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "# _take2 ã‚’å«ã‚ã‚‹å ´åˆ\n",
        "alpha_folder = f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}_take3\"\n",
        "model_load_dir = os.path.join(base_model_dir, alpha_folder)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "success_team_vxs = []\n",
        "\n",
        "print(f\" Alpha={FIXED_ALPHA} ã®å…¨è©¦åˆãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆè©•ä¾¡ä¸­...\")\n",
        "\n",
        "# --- CVã®çµæžœã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€å„è©¦åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚¹ãƒˆ ---\n",
        "# match_ids ã¯ CVå®Ÿè¡Œæ™‚ã¨åŒã˜ [1, 2, 3, 4, 5, 6, 7]\n",
        "for test_match in match_ids:\n",
        "    # 1. ãã®è©¦åˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # 2. ãã®è©¦åˆã§è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "    model_path = os.path.join(model_load_dir, f'pignn_testmatch_{test_match}.pth')\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\" Skip: {model_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
        "        continue\n",
        "\n",
        "    model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # 3. äºˆæ¸¬ã¨ç‰©ç†æƒ…å ±ã®æŠ½å‡º\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            # å„ã‚·ãƒ¼ãƒ³(ã‚°ãƒ©ãƒ•)ã”ã¨ã®ç‰©ç†é‡æŠ½å‡º\n",
        "            for i in range(out.size(0)):\n",
        "                mask = (data.batch == i)\n",
        "                # è«–æ–‡(cite: 12)ã®Permutation Importanceã§æœ€é‡è¦è¦–ã•ã‚ŒãŸvxã‚’è¨ˆç®—\n",
        "                avg_vx = torch.mean(data.vel[mask, 0]).item()\n",
        "\n",
        "                if pred[i] == 1: # AIãŒæˆåŠŸ(1)ã¨äºˆæ¸¬ã—ãŸæ™‚ã®ã¿\n",
        "                    success_team_vxs.append(avg_vx)\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# ==========================================\n",
        "# 4. ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤ºã‚»ã‚¯ã‚·ãƒ§ãƒ³\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"  PIGNN æœ€çµ‚ã‚¯ãƒ©ã‚·ãƒ•ã‚£ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒãƒ¼ãƒˆ (Alpha={FIXED_ALPHA})\")\n",
        "print(\"=\"*60)\n",
        "# è«–æ–‡(cite: 115)ã®ã€ŒnaÃ¯ve baselineã€ã¨ã®æ¯”è¼ƒã‚’å¿µé ­ã«ç½®ã„ãŸå‡ºåŠ›\n",
        "print(classification_report(all_labels, all_preds, target_names=['Failure', 'Success'], zero_division=0))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"  ç‰©ç†çš„æ•´åˆæ€§ æ¤œè¨¼ (vx = Byline to Byline Speed)\")\n",
        "print(\"=\"*60)\n",
        "if len(success_team_vxs) > 0:\n",
        "    avg_team_vx = np.mean(success_team_vxs)\n",
        "    # è«–æ–‡(cite: 196)ã§ã€Œhighest impactã€ã¨ã•ã‚ŒãŸvxã®æ–¹å‘æ€§ã‚’ç¢ºèª\n",
        "    positive_ratio = np.sum(np.array(success_team_vxs) > 0) / len(success_team_vxs)\n",
        "\n",
        "    print(f\"æˆåŠŸäºˆæ¸¬ã‚·ãƒ¼ãƒ³ã®å¹³å‡ vx: {avg_team_vx:.4f} m/s\")\n",
        "    print(f\"å³å‘ã(æ­£æ–¹å‘)ã¸ã®æŽ¨é€²åŠ›å‰²åˆ: {positive_ratio*100:.1f} %\")\n",
        "\n",
        "    # è«–æ–‡(cite: 6, 33)ã®ã€Œhigh speed attackã€ã®å®šç¾©ã«åŸºã¥ãè©•ä¾¡\n",
        "    if positive_ratio > 0.65:\n",
        "        print(\">> åˆ¤å®š: ç‰©ç†çš„å¦¥å½“ã€‚ãƒ¢ãƒ‡ãƒ«ã¯è«–æ–‡ã®å®šç¾©é€šã‚Šã€Žå‰æ–¹ã¸ã®é€Ÿåº¦ã€ã‚’é‡è¦–ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "    else:\n",
        "        print(\">> åˆ¤å®š: ç‰©ç†çš„ä¹–é›¢ã‚ã‚Šã€‚æˆ¦è¡“çš„ç‰¹å¾´ã‚ˆã‚Šã‚‚ãƒŽã‚¤ã‚ºã‚’å­¦ç¿’ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\"Successã¨äºˆæ¸¬ã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "\n",
        "# 5. æ··åŒè¡Œåˆ—ã®æç”»\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens' if FIXED_ALPHA > 0 else 'Blues',\n",
        "            xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title(f'Confusion Matrix (Alpha={FIXED_ALPHA})')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Rv5gFJEz5R0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# ==========================================\n",
        "# è¨­å®šï¼šæ¤œè¨¼ã—ãŸã„ Alpha ã‚’æŒ‡å®š\n",
        "# ==========================================\n",
        "FIXED_ALPHA = 100.0  # æ¯”è¼ƒã®ãŸã‚ã«ã“ã“ã‚’ 0 ã‚„ 1.0 ã«åˆ‡ã‚Šæ›¿ãˆã¦å®Ÿè¡Œ\n",
        "base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "# _take2 ã‚’å«ã‚ã‚‹å ´åˆ\n",
        "alpha_folder = f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}_take3\"\n",
        "model_load_dir = os.path.join(base_model_dir, alpha_folder)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "success_team_vxs = []\n",
        "\n",
        "print(f\" Alpha={FIXED_ALPHA} ã®å…¨è©¦åˆãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆè©•ä¾¡ä¸­...\")\n",
        "\n",
        "# --- CVã®çµæžœã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€å„è©¦åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚¹ãƒˆ ---\n",
        "# match_ids ã¯ CVå®Ÿè¡Œæ™‚ã¨åŒã˜ [1, 2, 3, 4, 5, 6, 7]\n",
        "for test_match in match_ids:\n",
        "    # 1. ãã®è©¦åˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # 2. ãã®è©¦åˆã§è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "    model_path = os.path.join(model_load_dir, f'pignn_testmatch_{test_match}.pth')\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\" Skip: {model_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
        "        continue\n",
        "\n",
        "    model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # 3. äºˆæ¸¬ã¨ç‰©ç†æƒ…å ±ã®æŠ½å‡º\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            # å„ã‚·ãƒ¼ãƒ³(ã‚°ãƒ©ãƒ•)ã”ã¨ã®ç‰©ç†é‡æŠ½å‡º\n",
        "            for i in range(out.size(0)):\n",
        "                mask = (data.batch == i)\n",
        "                # è«–æ–‡(cite: 12)ã®Permutation Importanceã§æœ€é‡è¦è¦–ã•ã‚ŒãŸvxã‚’è¨ˆç®—\n",
        "                avg_vx = torch.mean(data.vel[mask, 0]).item()\n",
        "\n",
        "                if pred[i] == 1: # AIãŒæˆåŠŸ(1)ã¨äºˆæ¸¬ã—ãŸæ™‚ã®ã¿\n",
        "                    success_team_vxs.append(avg_vx)\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# ==========================================\n",
        "# 4. ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤ºã‚»ã‚¯ã‚·ãƒ§ãƒ³\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"  PIGNN æœ€çµ‚ã‚¯ãƒ©ã‚·ãƒ•ã‚£ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒãƒ¼ãƒˆ (Alpha={FIXED_ALPHA})\")\n",
        "print(\"=\"*60)\n",
        "# è«–æ–‡(cite: 115)ã®ã€ŒnaÃ¯ve baselineã€ã¨ã®æ¯”è¼ƒã‚’å¿µé ­ã«ç½®ã„ãŸå‡ºåŠ›\n",
        "print(classification_report(all_labels, all_preds, target_names=['Failure', 'Success'], zero_division=0))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"  ç‰©ç†çš„æ•´åˆæ€§ æ¤œè¨¼ (vx = Byline to Byline Speed)\")\n",
        "print(\"=\"*60)\n",
        "if len(success_team_vxs) > 0:\n",
        "    avg_team_vx = np.mean(success_team_vxs)\n",
        "    # è«–æ–‡(cite: 196)ã§ã€Œhighest impactã€ã¨ã•ã‚ŒãŸvxã®æ–¹å‘æ€§ã‚’ç¢ºèª\n",
        "    positive_ratio = np.sum(np.array(success_team_vxs) > 0) / len(success_team_vxs)\n",
        "\n",
        "    print(f\"æˆåŠŸäºˆæ¸¬ã‚·ãƒ¼ãƒ³ã®å¹³å‡ vx: {avg_team_vx:.4f} m/s\")\n",
        "    print(f\"å³å‘ã(æ­£æ–¹å‘)ã¸ã®æŽ¨é€²åŠ›å‰²åˆ: {positive_ratio*100:.1f} %\")\n",
        "\n",
        "    # è«–æ–‡(cite: 6, 33)ã®ã€Œhigh speed attackã€ã®å®šç¾©ã«åŸºã¥ãè©•ä¾¡\n",
        "    if positive_ratio > 0.65:\n",
        "        print(\">> åˆ¤å®š: ç‰©ç†çš„å¦¥å½“ã€‚ãƒ¢ãƒ‡ãƒ«ã¯è«–æ–‡ã®å®šç¾©é€šã‚Šã€Žå‰æ–¹ã¸ã®é€Ÿåº¦ã€ã‚’é‡è¦–ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "    else:\n",
        "        print(\">> åˆ¤å®š: ç‰©ç†çš„ä¹–é›¢ã‚ã‚Šã€‚æˆ¦è¡“çš„ç‰¹å¾´ã‚ˆã‚Šã‚‚ãƒŽã‚¤ã‚ºã‚’å­¦ç¿’ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\"Successã¨äºˆæ¸¬ã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "\n",
        "# 5. æ··åŒè¡Œåˆ—ã®æç”»\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens' if FIXED_ALPHA > 0 else 'Blues',\n",
        "            xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title(f'Confusion Matrix (Alpha={FIXED_ALPHA})')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H7TQXWqSo9m8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# ==========================================\n",
        "# è¨­å®šï¼šæ¤œè¨¼ã—ãŸã„ Alpha ã‚’æŒ‡å®š\n",
        "# ==========================================\n",
        "FIXED_ALPHA = 200.0  # æ¯”è¼ƒã®ãŸã‚ã«ã“ã“ã‚’ 0 ã‚„ 1.0 ã«åˆ‡ã‚Šæ›¿ãˆã¦å®Ÿè¡Œ\n",
        "base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "# _take2 ã‚’å«ã‚ã‚‹å ´åˆ\n",
        "alpha_folder = f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}_take3\"\n",
        "model_load_dir = os.path.join(base_model_dir, alpha_folder)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "success_team_vxs = []\n",
        "\n",
        "print(f\" Alpha={FIXED_ALPHA} ã®å…¨è©¦åˆãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆè©•ä¾¡ä¸­...\")\n",
        "\n",
        "# --- CVã®çµæžœã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€å„è©¦åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚¹ãƒˆ ---\n",
        "# match_ids ã¯ CVå®Ÿè¡Œæ™‚ã¨åŒã˜ [1, 2, 3, 4, 5, 6, 7]\n",
        "for test_match in match_ids:\n",
        "    # 1. ãã®è©¦åˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # 2. ãã®è©¦åˆã§è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "    model_path = os.path.join(model_load_dir, f'pignn_testmatch_{test_match}.pth')\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\" Skip: {model_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
        "        continue\n",
        "\n",
        "    model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # 3. äºˆæ¸¬ã¨ç‰©ç†æƒ…å ±ã®æŠ½å‡º\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            # å„ã‚·ãƒ¼ãƒ³(ã‚°ãƒ©ãƒ•)ã”ã¨ã®ç‰©ç†é‡æŠ½å‡º\n",
        "            for i in range(out.size(0)):\n",
        "                mask = (data.batch == i)\n",
        "                # è«–æ–‡(cite: 12)ã®Permutation Importanceã§æœ€é‡è¦è¦–ã•ã‚ŒãŸvxã‚’è¨ˆç®—\n",
        "                avg_vx = torch.mean(data.vel[mask, 0]).item()\n",
        "\n",
        "                if pred[i] == 1: # AIãŒæˆåŠŸ(1)ã¨äºˆæ¸¬ã—ãŸæ™‚ã®ã¿\n",
        "                    success_team_vxs.append(avg_vx)\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# ==========================================\n",
        "# 4. ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤ºã‚»ã‚¯ã‚·ãƒ§ãƒ³\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"  PIGNN æœ€çµ‚ã‚¯ãƒ©ã‚·ãƒ•ã‚£ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒãƒ¼ãƒˆ (Alpha={FIXED_ALPHA})\")\n",
        "print(\"=\"*60)\n",
        "# è«–æ–‡(cite: 115)ã®ã€ŒnaÃ¯ve baselineã€ã¨ã®æ¯”è¼ƒã‚’å¿µé ­ã«ç½®ã„ãŸå‡ºåŠ›\n",
        "print(classification_report(all_labels, all_preds, target_names=['Failure', 'Success'], zero_division=0))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"  ç‰©ç†çš„æ•´åˆæ€§ æ¤œè¨¼ (vx = Byline to Byline Speed)\")\n",
        "print(\"=\"*60)\n",
        "if len(success_team_vxs) > 0:\n",
        "    avg_team_vx = np.mean(success_team_vxs)\n",
        "    # è«–æ–‡(cite: 196)ã§ã€Œhighest impactã€ã¨ã•ã‚ŒãŸvxã®æ–¹å‘æ€§ã‚’ç¢ºèª\n",
        "    positive_ratio = np.sum(np.array(success_team_vxs) > 0) / len(success_team_vxs)\n",
        "\n",
        "    print(f\"æˆåŠŸäºˆæ¸¬ã‚·ãƒ¼ãƒ³ã®å¹³å‡ vx: {avg_team_vx:.4f} m/s\")\n",
        "    print(f\"å³å‘ã(æ­£æ–¹å‘)ã¸ã®æŽ¨é€²åŠ›å‰²åˆ: {positive_ratio*100:.1f} %\")\n",
        "\n",
        "    # è«–æ–‡(cite: 6, 33)ã®ã€Œhigh speed attackã€ã®å®šç¾©ã«åŸºã¥ãè©•ä¾¡\n",
        "    if positive_ratio > 0.65:\n",
        "        print(\">> åˆ¤å®š: ç‰©ç†çš„å¦¥å½“ã€‚ãƒ¢ãƒ‡ãƒ«ã¯è«–æ–‡ã®å®šç¾©é€šã‚Šã€Žå‰æ–¹ã¸ã®é€Ÿåº¦ã€ã‚’é‡è¦–ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "    else:\n",
        "        print(\">> åˆ¤å®š: ç‰©ç†çš„ä¹–é›¢ã‚ã‚Šã€‚æˆ¦è¡“çš„ç‰¹å¾´ã‚ˆã‚Šã‚‚ãƒŽã‚¤ã‚ºã‚’å­¦ç¿’ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\"Successã¨äºˆæ¸¬ã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "\n",
        "# 5. æ··åŒè¡Œåˆ—ã®æç”»\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens' if FIXED_ALPHA > 0 else 'Blues',\n",
        "            xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title(f'Confusion Matrix (Alpha={FIXED_ALPHA})')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yhMBNX3kp0Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "def run_leakage_diagnostic(model, loader, device):\n",
        "    model.eval()\n",
        "    # ç‰¹å¾´é‡ãƒ©ãƒ™ãƒ«: 0:x, 1:y, 2:vx, 3:vy, 4:dist_goal, 5:dist_ball, 6:team_id\n",
        "    feature_names = [\"x\", \"y\", \"vx\", \"vy\", \"dist_goal\", \"dist_ball\", \"team_id\"]\n",
        "\n",
        "    print(f\"{'Removed Feature':<15} | {'Test Acc':<10} | {'Recall (S)':<10}\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆå…¨ç‰¹å¾´é‡ã‚ã‚Šï¼‰\n",
        "    base_acc = test_pignn(model, loader, device)\n",
        "    print(f\"{'None (Baseline)':<15} | {base_acc:.4f}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(7):\n",
        "            correct = 0\n",
        "            tp = 0 # True Positive\n",
        "            fn = 0 # False Negative\n",
        "\n",
        "            for data in loader:\n",
        "                data = preprocess_batch(data, device)\n",
        "\n",
        "                # ç‰¹å®šã®ç‰¹å¾´é‡ã‚’ã‚¼ãƒ­ã«ç½®ãæ›ãˆã‚‹\n",
        "                x_shuffled = data.x.clone()\n",
        "                x_shuffled[:, i] = 0.0\n",
        "\n",
        "                # æŽ¨è«–\n",
        "                out = model(Data(x=x_shuffled, edge_index=data.edge_index,\n",
        "                                 batch=data.batch, pos=data.pos, vel=data.vel))\n",
        "                pred = out.argmax(dim=1)\n",
        "\n",
        "                # ç²¾åº¦è¨ˆç®—\n",
        "                y_true = data.y.view(-1)\n",
        "                correct += (pred == y_true).sum().item()\n",
        "\n",
        "                # Recall (Success) è¨ˆç®—\n",
        "                tp += ((pred == 1) & (y_true == 1)).sum().item()\n",
        "                fn += ((pred == 0) & (y_true == 1)).sum().item()\n",
        "\n",
        "            acc = correct / len(loader.dataset)\n",
        "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            print(f\"{feature_names[i]:<15} | {acc:.4f}     | {recall:.4f}\")\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "run_leakage_diagnostic(model, test_loader, device)"
      ],
      "metadata": {
        "id": "4iQw8_UUA99Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãŸã ã®MLP"
      ],
      "metadata": {
        "id": "Dhd4oLJUFBWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# 1. MLPãƒ¢ãƒ‡ãƒ«ã®å®šç¾© (PIGNNã¨å…¥åŠ›ã‚’å®Œå…¨ã«æƒãˆã‚‹)\n",
        "# ==========================================\n",
        "class SimpleMLPClassifier(nn.Module):\n",
        "    def __init__(self, in_channels=7, hidden_channels=64): # ã‚ãªãŸã®ãƒ‡ãƒ¼ã‚¿(7æ¬¡å…ƒ)ã«åˆã‚ã›ã‚‹\n",
        "        super(SimpleMLPClassifier, self).__init__()\n",
        "        # ã‚°ãƒ©ãƒ•ç•³ã¿è¾¼ã¿ã‚’è¡Œã‚ãšã€å…¨çµåˆå±¤ã®ã¿ã§åˆ¤å®š\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_channels, hidden_channels),\n",
        "            nn.BatchNorm1d(hidden_channels), # å‹¾é…æ¶ˆå¤±ã‚’é˜²ãŽå­¦ç¿’ã‚’å®‰å®šåŒ–\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_channels, hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_channels, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        # é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ:\n",
        "        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ï¼ˆè¿‘æŽ¥é¸æ‰‹ã®ç›¸äº’ä½œç”¨ç†è§£ï¼‰ã‚’ãƒã‚¤ãƒ‘ã‚¹ã—ã€\n",
        "        # å…¨é¸æ‰‹ã®å¹³å‡çš„ãªçµ±è¨ˆé‡ã ã‘ã§äºˆæ¸¬ã‚’è¡Œã†\n",
        "        x = global_mean_pool(data.x, data.batch) # ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’ç„¡è¦–ã—ãŸå¹³å‡åŒ–\n",
        "        return F.log_softmax(self.mlp(x), dim=1)\n",
        "\n",
        "# ==========================================\n",
        "# 2. MLPç”¨ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œé–¢æ•°\n",
        "# ==========================================\n",
        "def run_mlp_cv(all_data_list, match_ids, device):\n",
        "    FIXED_ALPHA = \"MLP\"\n",
        "    # ä¿å­˜å…ˆã‚’ç‹¬ç«‹ã•ã›ã‚‹\n",
        "    model_save_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/MLP_Baseline_take3\"\n",
        "    os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "    cv_final_reports = []\n",
        "    print(f\" MLP Baseline CVé–‹å§‹ (Input: 7 channels)\\n\")\n",
        "\n",
        "    for test_match in match_ids:\n",
        "        print(f\" Round: Match {test_match} (MLP)\")\n",
        "\n",
        "        # PIGNNã¨å…¨ãåŒã˜ãƒ«ãƒ¼ãƒ«ã§åˆ‡ã‚Šåˆ†ã‘\n",
        "        test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "        train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match]\n",
        "\n",
        "        # è«–æ–‡[cite: 105]åŒæ§˜ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿1:1ã«ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
        "        cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "\n",
        "        #cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "        cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True, drop_last=True)\n",
        "        cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ– (7æ¬¡å…ƒ)\n",
        "        model = SimpleMLPClassifier(in_channels=7).to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "        # 30ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ (ç‰©ç†æå¤±ãªã—ã®æ¨™æº–çš„ãªå­¦ç¿’)\n",
        "        model.train()\n",
        "        for epoch in range(1, 31):\n",
        "            for d in cv_train_loader:\n",
        "                d = d.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                out = model(d)\n",
        "                # è«–æ–‡åŒæ§˜[cite: 115]ã®50/50ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«è¿‘ã„æ¡ä»¶ã§NLLLossã‚’ä½¿ç”¨\n",
        "                loss = F.nll_loss(out, d.y.view(-1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # è©•ä¾¡ (å®Ÿæˆ¦ã®ä¸å‡è¡¡æ¯”çŽ‡ã®ã¾ã¾ãƒ†ã‚¹ãƒˆ)\n",
        "        model.eval()\n",
        "        y_true, y_pred = [], []\n",
        "        with torch.no_grad():\n",
        "            for d in cv_test_loader:\n",
        "                d = d.to(device)\n",
        "                out = model(d)\n",
        "                y_true.extend(d.y.view(-1).cpu().numpy())\n",
        "                y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ\n",
        "        report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "        cv_final_reports.append({\n",
        "            'match': test_match,\n",
        "            'recall': report['1']['recall'],\n",
        "            'precision': report['1']['precision'],\n",
        "            'f1': report['1']['f1-score']\n",
        "        })\n",
        "\n",
        "        # ä¿å­˜ (å¾Œã§å¯è¦–åŒ–æ¯”è¼ƒã«ä½¿ã†)\n",
        "        torch.save(model.state_dict(), os.path.join(model_save_dir, f'mlp_match_{test_match}.pth'))\n",
        "        print(f\" >> Result: Precision={report['1']['precision']:.4f}, F1={report['1']['f1-score']:.4f}\")\n",
        "\n",
        "    # å…¨ä½“ã®å¹³å‡ã‚’è¨ˆç®—\n",
        "    avg_precision = np.mean([r['precision'] for r in cv_final_reports])\n",
        "    avg_f1 = np.mean([r['f1'] for r in cv_final_reports])\n",
        "\n",
        "    print(f\"\\n MLP CV Final Results\")\n",
        "    print(f\"Avg Precision: {avg_precision:.4f}\")\n",
        "    print(f\"Avg F1-score:  {avg_f1:.4f}\")\n",
        "\n",
        "    return cv_final_reports\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "mlp_results = run_mlp_cv(all_data_list, match_ids, device)"
      ],
      "metadata": {
        "id": "cCob8uypFD02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================\n",
        "# 1. è¨­å®šï¼šMLPãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
        "# ==========================================\n",
        "model_save_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/MLP_Baseline_take3\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "all_preds_mlp = []\n",
        "all_labels_mlp = []\n",
        "success_vxs_mlp = []\n",
        "\n",
        "print(f\" MLP Baseline æœ€çµ‚çµ±åˆè©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "\n",
        "# CVã§ä¿å­˜ã—ãŸå…¨è©¦åˆã®MLPãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦è©•ä¾¡\n",
        "for test_match in match_ids:\n",
        "    # ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™\n",
        "    model = SimpleMLPClassifier(in_channels=7).to(device)\n",
        "    model_path = os.path.join(model_save_dir, f'mlp_match_{test_match}.pth')\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        continue\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            # ç‰©ç†çš„å¦¥å½“æ€§ã®è¨ˆç®—\n",
        "            for i in range(out.size(0)):\n",
        "                mask = (data.batch == i)\n",
        "                # å…¨ãƒŽãƒ¼ãƒ‰ã®å¹³å‡vxï¼ˆé›†å›£ã®æŽ¨é€²åŠ›ï¼‰\n",
        "                avg_vx = torch.mean(data.vel[mask, 0]).item()\n",
        "\n",
        "                if pred[i] == 1: # Successã¨äºˆæ¸¬ã—ãŸæ™‚ã®ã¿è¨˜éŒ²\n",
        "                    success_vxs_mlp.append(avg_vx)\n",
        "\n",
        "            all_preds_mlp.extend(pred.cpu().numpy())\n",
        "            all_labels_mlp.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# ==========================================\n",
        "# 2. ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"  MLP Baseline æœ€çµ‚ã‚¯ãƒ©ã‚·ãƒ•ã‚£ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒãƒ¼ãƒˆ\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(all_labels_mlp, all_preds_mlp, target_names=['Fail', 'Success'], zero_division=0))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"  ç‰©ç†çš„æ•´åˆæ€§ æ¤œè¨¼ (MLP vs ç‰©ç†æ³•å‰‡)\")\n",
        "print(\"=\"*60)\n",
        "if len(success_vxs_mlp) > 0:\n",
        "    avg_vx_mlp = np.mean(success_vxs_mlp)\n",
        "    pos_ratio_mlp = np.sum(np.array(success_vxs_mlp) > 0) / len(success_vxs_mlp)\n",
        "\n",
        "    print(f\"MLPãŒã€ŽæˆåŠŸã€ã¨äºˆæ¸¬ã—ãŸã‚·ãƒ¼ãƒ³ã®å¹³å‡ vx: {avg_vx_mlp:.4f} m/s\")\n",
        "    print(f\"å³å‘ã(æ”»æ’ƒæ–¹å‘)ã¸ã®æŽ¨é€²åŠ›å‰²åˆ: {pos_ratio_mlp*100:.1f} %\")\n",
        "\n",
        "    # è€ƒå¯Ÿç”¨ã‚³ãƒ¡ãƒ³ãƒˆ\n",
        "    if pos_ratio_mlp > 0.80:\n",
        "        print(\">> è€ƒå¯Ÿ: MLPã¯æ¥µã‚ã¦é«˜ã„ç¢ºçŽ‡ã§ã€Žå³ã¸ã®é€Ÿåº¦ã€ã‚’æˆåŠŸã®æ ¹æ‹ ã¨ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "        print(\">> ã“ã‚Œã¯ç©ºé–“æ§‹é€ ã‚’ç„¡è¦–ã—ã€å˜ç´”ãªç‰©ç†é‡ã®ã¿ã«ä¾å­˜ã—ã¦ã„ã‚‹å¯èƒ½æ€§ã‚’ç¤ºå”†ã—ã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\"Successäºˆæ¸¬ãªã—\")\n",
        "\n",
        "# æ··åŒè¡Œåˆ—ã®è¡¨ç¤º\n",
        "cm = confusion_matrix(all_labels_mlp, all_preds_mlp)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title('Confusion Matrix (MLP Baseline)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XokrwR8VFWuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PFI"
      ],
      "metadata": {
        "id": "RPuh0ClPxg2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# ==========================================\n",
        "# 1. ç‰¹å¾´é‡åã¨é‡è¦åº¦ç®—å‡ºé–¢æ•°ã®å®šç¾©\n",
        "# ==========================================\n",
        "# (ã“ã“ã¯å¤‰æ›´ãªã—ã§ã™ãŒã€ãã®ã¾ã¾å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«å«ã‚ã¦ã„ã¾ã™)\n",
        "feature_names = [\n",
        "    'Position_X',        # Index 0\n",
        "    'Position_Y',        # Index 1\n",
        "    'Velocity_X',        # Index 2\n",
        "    'Velocity_Y',        # Index 3\n",
        "    'Distance_to_Goal',  # Index 4\n",
        "    'Distance_to_Ball',  # Index 5\n",
        "    'Team_Flag'          # Index 6\n",
        "]\n",
        "\n",
        "def calculate_pfi_refined(model, loader, device, feature_names, model_type=\"GNN\"):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    # --- Step 1: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®F1ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— ---\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    baseline_f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    print(f\"[{model_type}] Baseline F1: {baseline_f1:.4f}\")\n",
        "\n",
        "    importance_scores = {}\n",
        "\n",
        "    # --- Step 2: å„ç‰¹å¾´é‡ã‚’é †ç•ªã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦å½±éŸ¿ã‚’æ¸¬å®š ---\n",
        "    for i, f_name in enumerate(feature_names):\n",
        "        shuffled_f1_list = []\n",
        "        for seed in range(3):\n",
        "            y_true_s, y_pred_s = [], []\n",
        "            with torch.no_grad():\n",
        "                for data in loader:\n",
        "                    data_s = copy.deepcopy(data).to(device)\n",
        "                    # ç‰¹å®šã®ç‰¹å¾´é‡åˆ—ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
        "                    perm = torch.randperm(data_s.x.size(0))\n",
        "                    data_s.x[:, i] = data_s.x[perm, i]\n",
        "\n",
        "                    out_s = model(data_s)\n",
        "                    y_true_s.extend(data_s.y.view(-1).cpu().numpy())\n",
        "                    y_pred_s.extend(out_s.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "            shuffled_f1 = f1_score(y_true_s, y_pred_s, zero_division=0)\n",
        "            shuffled_f1_list.append(shuffled_f1)\n",
        "\n",
        "        importance_scores[f_name] = baseline_f1 - np.mean(shuffled_f1_list)\n",
        "        print(f\"  > Done: {f_name}\")\n",
        "\n",
        "    return importance_scores\n",
        "\n",
        "# ==========================================\n",
        "# 2. ãƒ­ãƒ¼ãƒ‰ã¨å®Ÿè¡Œï¼ˆAlpha=10.0 ã«ä¿®æ­£ï¼‰\n",
        "# ==========================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«å™¨ã®æº–å‚™\n",
        "pignn_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "mlp_model = SimpleMLPClassifier(in_channels=7).to(device)\n",
        "\n",
        "# --- A. Alpha=100.0 (â˜…ææ¡ˆæ‰‹æ³•ãƒ»Champion Model) ---\n",
        "path_10 = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/alpha_100_0_take3/pignn_testmatch_1.pth\"\n",
        "if os.path.exists(path_10):\n",
        "    pignn_model.load_state_dict(torch.load(path_10, map_location=device))\n",
        "    print(\"\\n Calculating PFI for PIGNN (Alpha=100.0)...\")\n",
        "    pfi_10 = calculate_pfi_refined(pignn_model, cv_test_loader, device, feature_names, \"Alpha100\")\n",
        "else:\n",
        "    print(f\" Path not found: {path_10}\")\n",
        "    pfi_10 = {f: 0 for f in feature_names} # ã‚¨ãƒ©ãƒ¼å›žé¿ç”¨ãƒ€ãƒŸãƒ¼\n",
        "\n",
        "# --- B. Alpha=0.0 (ç‰©ç†ãªã—GNN) ---\n",
        "path_0 = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/alpha_0_take3/pignn_testmatch_1.pth\"\n",
        "if os.path.exists(path_0):\n",
        "    pignn_model.load_state_dict(torch.load(path_0, map_location=device))\n",
        "    print(\"\\n Calculating PFI for PIGNN (Alpha=0)...\")\n",
        "    pfi_0 = calculate_pfi_refined(pignn_model, cv_test_loader, device, feature_names, \"Alpha0\")\n",
        "else:\n",
        "    pfi_0 = {f: 0 for f in feature_names}\n",
        "\n",
        "# --- C. MLP (ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³) ---\n",
        "path_mlp = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/MLP_Baseline_take3/mlp_match_1.pth\"\n",
        "if os.path.exists(path_mlp):\n",
        "    mlp_model.load_state_dict(torch.load(path_mlp, map_location=device))\n",
        "    print(\"\\n Calculating PFI for MLP...\")\n",
        "    pfi_mlp = calculate_pfi_refined(mlp_model, cv_test_loader, device, feature_names, \"MLP\")\n",
        "else:\n",
        "    pfi_mlp = {f: 0 for f in feature_names}\n",
        "\n",
        "# ==========================================\n",
        "# 3. ã‚°ãƒ©ãƒ•æç”»\n",
        "# ==========================================\n",
        "df_pfi = pd.DataFrame({\n",
        "    'PIGNN (Î±=100.0)': pfi_10,  # â˜…ã“ã“ãŒä¸»å½¹\n",
        "    'PIGNN (Î±=0.0)': pfi_0,\n",
        "    'MLP (Baseline)': pfi_mlp\n",
        "})\n",
        "\n",
        "# è‰²ã®è¨­å®šï¼ˆ10=é’, 0=æ°´è‰², MLP=ã‚°ãƒ¬ãƒ¼ï¼‰\n",
        "colors = ['royalblue', 'skyblue', 'silver']\n",
        "\n",
        "ax = df_pfi.plot(kind='barh', figsize=(12, 8), width=0.8, color=colors)\n",
        "plt.axvline(0, color='black', linewidth=0.8)\n",
        "\n",
        "# ã‚¿ã‚¤ãƒˆãƒ«ã‚‚ä¿®æ­£\n",
        "plt.title('Feature Importance Comparison: PIGNN(Î±=100.0) vs Others', fontsize=16)\n",
        "plt.xlabel('Drop in F1-score (Significance)', fontsize=12)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.legend(loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q4OeLAlowlDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# ==========================================\n",
        "# 1. ç‰¹å¾´é‡åã¨é‡è¦åº¦ç®—å‡ºé–¢æ•°ã®å®šç¾©\n",
        "# ==========================================\n",
        "# (ã“ã“ã¯å¤‰æ›´ãªã—ã§ã™ãŒã€ãã®ã¾ã¾å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«å«ã‚ã¦ã„ã¾ã™)\n",
        "feature_names = [\n",
        "    'Position_X',        # Index 0\n",
        "    'Position_Y',        # Index 1\n",
        "    'Velocity_X',        # Index 2\n",
        "    'Velocity_Y',        # Index 3\n",
        "    'Distance_to_Goal',  # Index 4\n",
        "    'Distance_to_Ball',  # Index 5\n",
        "    'Team_Flag'          # Index 6\n",
        "]\n",
        "\n",
        "def calculate_pfi_refined(model, loader, device, feature_names, model_type=\"GNN\"):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    # --- Step 1: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®F1ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— ---\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    baseline_f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    print(f\"[{model_type}] Baseline F1: {baseline_f1:.4f}\")\n",
        "\n",
        "    importance_scores = {}\n",
        "\n",
        "    # --- Step 2: å„ç‰¹å¾´é‡ã‚’é †ç•ªã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦å½±éŸ¿ã‚’æ¸¬å®š ---\n",
        "    for i, f_name in enumerate(feature_names):\n",
        "        shuffled_f1_list = []\n",
        "        for seed in range(3):\n",
        "            y_true_s, y_pred_s = [], []\n",
        "            with torch.no_grad():\n",
        "                for data in loader:\n",
        "                    data_s = copy.deepcopy(data).to(device)\n",
        "                    # ç‰¹å®šã®ç‰¹å¾´é‡åˆ—ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
        "                    perm = torch.randperm(data_s.x.size(0))\n",
        "                    data_s.x[:, i] = data_s.x[perm, i]\n",
        "\n",
        "                    out_s = model(data_s)\n",
        "                    y_true_s.extend(data_s.y.view(-1).cpu().numpy())\n",
        "                    y_pred_s.extend(out_s.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "            shuffled_f1 = f1_score(y_true_s, y_pred_s, zero_division=0)\n",
        "            shuffled_f1_list.append(shuffled_f1)\n",
        "\n",
        "        importance_scores[f_name] = baseline_f1 - np.mean(shuffled_f1_list)\n",
        "        print(f\"  > Done: {f_name}\")\n",
        "\n",
        "    return importance_scores\n",
        "\n",
        "# ==========================================\n",
        "# 2. ãƒ­ãƒ¼ãƒ‰ã¨å®Ÿè¡Œï¼ˆAlpha=10.0 ã«ä¿®æ­£ï¼‰\n",
        "# ==========================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«å™¨ã®æº–å‚™\n",
        "pignn_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "mlp_model = SimpleMLPClassifier(in_channels=7).to(device)\n",
        "\n",
        "# --- A. Alpha=100.0 (â˜…ææ¡ˆæ‰‹æ³•ãƒ»Champion Model) ---\n",
        "path_10 = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/alpha_50_0_take3/pignn_testmatch_1.pth\"\n",
        "if os.path.exists(path_10):\n",
        "    pignn_model.load_state_dict(torch.load(path_10, map_location=device))\n",
        "    print(\"\\n Calculating PFI for PIGNN (Alpha=50.0)...\")\n",
        "    pfi_10 = calculate_pfi_refined(pignn_model, cv_test_loader, device, feature_names, \"Alpha50\")\n",
        "else:\n",
        "    print(f\" Path not found: {path_10}\")\n",
        "    pfi_10 = {f: 0 for f in feature_names} # ã‚¨ãƒ©ãƒ¼å›žé¿ç”¨ãƒ€ãƒŸãƒ¼\n",
        "\n",
        "# --- B. Alpha=0.0 (ç‰©ç†ãªã—GNN) ---\n",
        "path_0 = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/alpha_0_take3/pignn_testmatch_1.pth\"\n",
        "if os.path.exists(path_0):\n",
        "    pignn_model.load_state_dict(torch.load(path_0, map_location=device))\n",
        "    print(\"\\n Calculating PFI for PIGNN (Alpha=0)...\")\n",
        "    pfi_0 = calculate_pfi_refined(pignn_model, cv_test_loader, device, feature_names, \"Alpha0\")\n",
        "else:\n",
        "    pfi_0 = {f: 0 for f in feature_names}\n",
        "\n",
        "# --- C. MLP (ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³) ---\n",
        "path_mlp = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/MLP_Baseline_take3/mlp_match_1.pth\"\n",
        "if os.path.exists(path_mlp):\n",
        "    mlp_model.load_state_dict(torch.load(path_mlp, map_location=device))\n",
        "    print(\"\\n Calculating PFI for MLP...\")\n",
        "    pfi_mlp = calculate_pfi_refined(mlp_model, cv_test_loader, device, feature_names, \"MLP\")\n",
        "else:\n",
        "    pfi_mlp = {f: 0 for f in feature_names}\n",
        "\n",
        "# ==========================================\n",
        "# 3. ã‚°ãƒ©ãƒ•æç”»\n",
        "# ==========================================\n",
        "df_pfi = pd.DataFrame({\n",
        "    'PIGNN (Î±=50.0)': pfi_10,  # â˜…ã“ã“ãŒä¸»å½¹\n",
        "    'PIGNN (Î±=0.0)': pfi_0,\n",
        "    'MLP (Baseline)': pfi_mlp\n",
        "})\n",
        "\n",
        "# è‰²ã®è¨­å®šï¼ˆ10=é’, 0=æ°´è‰², MLP=ã‚°ãƒ¬ãƒ¼ï¼‰\n",
        "colors = ['royalblue', 'skyblue', 'silver']\n",
        "\n",
        "ax = df_pfi.plot(kind='barh', figsize=(12, 8), width=0.8, color=colors)\n",
        "plt.axvline(0, color='black', linewidth=0.8)\n",
        "\n",
        "# ã‚¿ã‚¤ãƒˆãƒ«ã‚‚ä¿®æ­£\n",
        "plt.title('Feature Importance Comparison: PIGNN(Î±=50.0) vs Others', fontsize=16)\n",
        "plt.xlabel('Drop in F1-score (Significance)', fontsize=12)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.legend(loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ptkc3Oc3tonC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMLPClassifier(nn.Module):\n",
        "    def __init__(self, in_channels=7, hidden_channels=64): # ã‚ãªãŸã®ãƒ‡ãƒ¼ã‚¿(7æ¬¡å…ƒ)ã«åˆã‚ã›ã‚‹\n",
        "        super(SimpleMLPClassifier, self).__init__()\n",
        "        # ã‚°ãƒ©ãƒ•ç•³ã¿è¾¼ã¿ã‚’è¡Œã‚ãšã€å…¨çµåˆå±¤ã®ã¿ã§åˆ¤å®š\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_channels, hidden_channels),\n",
        "            nn.BatchNorm1d(hidden_channels), # å‹¾é…æ¶ˆå¤±ã‚’é˜²ãŽå­¦ç¿’ã‚’å®‰å®šåŒ–\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_channels, hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_channels, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        # é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ:\n",
        "        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ï¼ˆè¿‘æŽ¥é¸æ‰‹ã®ç›¸äº’ä½œç”¨ç†è§£ï¼‰ã‚’ãƒã‚¤ãƒ‘ã‚¹ã—ã€\n",
        "        # å…¨é¸æ‰‹ã®å¹³å‡çš„ãªçµ±è¨ˆé‡ã ã‘ã§äºˆæ¸¬ã‚’è¡Œã†\n",
        "        x = global_mean_pool(data.x, data.batch) # ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’ç„¡è¦–ã—ãŸå¹³å‡åŒ–\n",
        "        return F.log_softmax(self.mlp(x), dim=1)"
      ],
      "metadata": {
        "id": "C8Lg0IH8W5Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "import copy\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ==========================================\n",
        "# æ§‹é€ ç ´å£Šãƒ†ã‚¹ãƒˆé–¢æ•° (Position Scramble)\n",
        "# ==========================================\n",
        "def test_with_position_noise(model, loader, device, model_name=\"Model\"):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            # dataè‡ªä½“ã‚’å£Šã•ãªã„ã‚ˆã†ã€cloneã‚’ä½¿ç”¨\n",
        "            x_shuffled = data.x.clone()\n",
        "\n",
        "            # --- å®Ÿé¨“ã®ã‚­ãƒ¢: åº§æ¨™(index 0, 1)ã®ã¿ã‚’-1~1ã®ç¯„å›²ã§ãƒ©ãƒ³ãƒ€ãƒ åŒ– ---\n",
        "            num_nodes = data.x.size(0)\n",
        "            noise_x = (torch.rand(num_nodes) * 2.0 - 1.0).to(device) # -1 ~ 1\n",
        "            noise_y = (torch.rand(num_nodes) * 2.0 - 1.0).to(device) # -1 ~ 1\n",
        "\n",
        "            x_shuffled[:, 0] = noise_x\n",
        "            x_shuffled[:, 1] = noise_y\n",
        "\n",
        "            # GNNç”¨ã® pos å±žæ€§ã‚‚åŒæ§˜ã«ç ´å£Šï¼ˆã‚‚ã—ãƒ¢ãƒ‡ãƒ«å†…ã§ä½¿ã£ã¦ã„ã‚‹å ´åˆï¼‰\n",
        "            data_copy = data.clone()\n",
        "            data_copy.x = x_shuffled\n",
        "            if hasattr(data_copy, 'pos'):\n",
        "                data_copy.pos[:, 0] = noise_x\n",
        "                data_copy.pos[:, 1] = noise_y\n",
        "\n",
        "            # æŽ¨è«–\n",
        "            out = model(data_copy)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    print(f\" [{model_name}] Position-Scramble F1 Score: {f1:.4f}\")\n",
        "    return f1\n",
        "\n",
        "# ==========================================\n",
        "# å®Ÿè¡Œãƒ‘ãƒ¼ãƒˆ\n",
        "# ==========================================\n",
        "print(\" æ§‹é€ ç ´å£Š (Position Scramble) ãƒ†ã‚¹ãƒˆé–‹å§‹ âš”ï¸\")\n",
        "print(\">> æ¡ä»¶: é€Ÿåº¦(Velocity)ã¯ãã®ã¾ã¾ã€‚ä½ç½®(Position)ã®ã¿ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã€‚\\n\")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ‘ã‚¹ã¯é©å®œèª¿æ•´ã—ã¦ãã ã•ã„ï¼‰\n",
        "# PIGNN (Alpha=100)\n",
        "pignn_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "path_pignn = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/alpha_100_0_take3/pignn_testmatch_1.pth\"\n",
        "\n",
        "# MLP (Baseline)\n",
        "mlp_model = SimpleMLPClassifier(in_channels=7).to(device)\n",
        "path_mlp = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/MLP_Baseline_take3/mlp_match_1.pth\"\n",
        "\n",
        "if os.path.exists(path_pignn) and os.path.exists(path_mlp):\n",
        "    pignn_model.load_state_dict(torch.load(path_pignn, map_location=device))\n",
        "    mlp_model.load_state_dict(torch.load(path_mlp, map_location=device))\n",
        "\n",
        "    # 1. PIGNNã®ãƒ†ã‚¹ãƒˆ\n",
        "    f1_pignn = test_with_position_noise(pignn_model, cv_test_loader, device, \"PIGNN (Î±=100)\")\n",
        "\n",
        "    # 2. MLPã®ãƒ†ã‚¹ãƒˆ\n",
        "    f1_mlp = test_with_position_noise(mlp_model, cv_test_loader, device, \"MLP (Baseline)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\" çµè«–åˆ¤å®š\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    # æœŸå¾…ã•ã‚Œã‚‹ã‚·ãƒŠãƒªã‚ª\n",
        "    # MLP: ä½ç½®ãŒå¤‰ã‚ã£ã¦ã‚‚ã€Œå¹³å‡é€Ÿåº¦ã€ãŒå¤‰ã‚ã‚‰ãªã‘ã‚Œã°ã€F1ã¯ã‚ã¾ã‚Šè½ã¡ãªã„ã¯ãšã€‚\n",
        "    # PIGNN: ä½ç½®ãŒå¤‰ã‚ã‚‹ã¨ã€Œç›¸å¯¾é–¢ä¿‚ã€ãŒå£Šã‚Œã‚‹ã®ã§ã€F1ãŒã‚¬ã‚¿è½ã¡ã™ã‚‹ã¯ãšã€‚\n",
        "\n",
        "    if f1_mlp > f1_pignn + 0.05:\n",
        "        print(f\" ä»®èª¬å®Ÿè¨¼æˆåŠŸï¼\")\n",
        "        print(f\">> MLP (F1={f1_mlp:.4f}) ã¯ä½ç½®æƒ…å ±ãŒãƒ‡ã‚¿ãƒ©ãƒ¡ã§ã‚‚é«˜ã‚¹ã‚³ã‚¢ã‚’ç¶­æŒã—ã¾ã—ãŸã€‚\")\n",
        "        print(\"   â†’ ã¤ã¾ã‚Šã€MLPã¯ã€Žé…ç½®ã€ã‚’è¦‹ã¦ãŠã‚‰ãšã€é€Ÿåº¦è¨ˆã¨ã—ã¦æ©Ÿèƒ½ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "        print(f\">> PIGNN (F1={f1_pignn:.4f}) ã¯ã‚¹ã‚³ã‚¢ãŒä½Žä¸‹ã—ã¾ã—ãŸã€‚\")\n",
        "        print(\"   â†’ ã¤ã¾ã‚Šã€PIGNNã¯æ­£ã—ãã€Žé…ç½®ï¼ˆæ§‹é€ ï¼‰ã€ã‚’åˆ¤æ–­æ ¹æ‹ ã«ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "    else:\n",
        "        print(\" äºˆæƒ³å¤–ã®çµæžœã§ã™ã€‚æ•°å€¤ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "else:\n",
        "    print(\" ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")"
      ],
      "metadata": {
        "id": "DsTo-sl2yR_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_with_goal_dist_noise(model, loader, device, model_name=\"Model\"):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            x_shuffled = data.x.clone()\n",
        "\n",
        "            # --- å®Ÿé¨“ã®ã‚­ãƒ¢: ã‚´ãƒ¼ãƒ«è·é›¢(index 4)ã‚’æ­£è¦åŒ–ç¯„å›²å†…ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ« ---\n",
        "            num_nodes = data.x.size(0)\n",
        "\n",
        "            # åº§æ¨™ç­‰ãŒ -1~1 ã§æ­£è¦åŒ–ã•ã‚Œã¦ã„ã‚‹å ´åˆã€ãã‚Œã«åˆã‚ã›ã‚‹\n",
        "            # ã‚‚ã—å…ƒã®ãƒ‡ãƒ¼ã‚¿ãŒ 0~1 ãªã‚‰ã€noise_dist = torch.rand(num_nodes)\n",
        "            noise_dist = (torch.rand(num_nodes) * 2.0 - 1.0).to(device)\n",
        "\n",
        "            x_shuffled[:, 4] = noise_dist\n",
        "\n",
        "            # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼ã¨æŽ¨è«–ï¼ˆä¿®æ­£ãªã—ï¼‰\n",
        "            data_copy = data.clone()\n",
        "            data_copy.x = x_shuffled\n",
        "            out = model(data_copy)\n",
        "\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    print(f\" [{model_name}] Goal-Dist Scramble F1 Score: {f1:.4f}\")\n",
        "    return f1\n",
        "\n",
        "print(\" ç©ºé–“æ–‡è„ˆç ´å£Š (Goal-Distance Scramble) ãƒ†ã‚¹ãƒˆé–‹å§‹ âš”ï¸\")\n",
        "print(\">> æ¡ä»¶: é€Ÿåº¦ã‚„ä½ç½®ã¯ãã®ã¾ã¾ã€‚ã‚´ãƒ¼ãƒ«ã¸ã®è·é›¢(dist_goal)ã®ã¿ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã€‚\\n\")\n",
        "\n",
        "# PIGNNã®ãƒ†ã‚¹ãƒˆ\n",
        "f1_pignn_dist = test_with_goal_dist_noise(pignn_model, cv_test_loader, device, \"PIGNN (Î±=100)\")\n",
        "\n",
        "# MLPã®ãƒ†ã‚¹ãƒˆ\n",
        "f1_mlp_dist = test_with_goal_dist_noise(mlp_model, cv_test_loader, device, \"MLP (Baseline)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\" è§£é‡ˆæ€§ã«é–¢ã™ã‚‹çµè«–åˆ¤å®š\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "if f1_pignn_dist < f1_mlp_dist:\n",
        "    print(f\" PIGNNã®æ–¹ãŒãƒ€ãƒ¡ãƒ¼ã‚¸ãŒå¤§ãã„ï¼ˆF1={f1_pignn_dist:.4f}ï¼‰\")\n",
        "    print(\">> PIGNNã¯ã€Žä»Šã©ã“ã§åŠ é€Ÿã—ã¦ã„ã‚‹ã‹ï¼ˆã‚´ãƒ¼ãƒ«ã¨ã®ä½ç½®é–¢ä¿‚ï¼‰ã€ã‚’è«–ç†çš„ã«é‡è¦–ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(f\" PIGNNãŒè€ãˆã¦ã„ã‚‹ï¼ˆF1={f1_pignn_dist:.4f}ï¼‰\")\n",
        "    print(\">> PIGNNã«ã¨ã£ã¦ã‚´ãƒ¼ãƒ«è·é›¢ã¯è£œåŠ©çš„ãªã‚‚ã®ã§ã€ã‚„ã¯ã‚Šã€Žé€Ÿåº¦($v_x$)ã€ã“ããŒçµ¶å¯¾çš„ãªåˆ¤æ–­åŸºæº–ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")"
      ],
      "metadata": {
        "id": "O4ckzAAdw-Uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã®å¯è¦–åŒ–"
      ],
      "metadata": {
        "id": "nNhUKPcP6W4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "import os\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# ==========================================\n",
        "# 1. å¯è¦–åŒ– & æ•°å€¤æŠ½å‡ºãƒ¡ã‚¤ãƒ³é–¢æ•°\n",
        "# ==========================================\n",
        "def visualize_pignn_tactical_analysis(model, data_item, device, title=\"PIGNN Tactical Analysis\"):\n",
        "    model.eval()\n",
        "    data_item = data_item.to(device)\n",
        "\n",
        "    # --- æŽ¨è«–ã¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æŠ½å‡º ---\n",
        "    with torch.no_grad():\n",
        "        out, (edge_index, att_weights) = model(data_item, return_attention=True)\n",
        "        prob = torch.softmax(out, dim=1)[0, 1].item()\n",
        "        pred = out.argmax(dim=1).item()\n",
        "        label = data_item.y.item()\n",
        "\n",
        "    # --- åº§æ¨™ã¨é€Ÿåº¦ã®å¾©å…ƒ ---\n",
        "    pos = data_item.pos.cpu().numpy()\n",
        "    vel = data_item.vel.cpu().numpy()\n",
        "    pos_plot = np.zeros_like(pos)\n",
        "    pos_plot[:, 0] = pos[:, 0] * 52.5\n",
        "    pos_plot[:, 1] = pos[:, 1] * 34.0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "    # --- ã‚µãƒƒã‚«ãƒ¼å ´ã®æç”» (zorder=0-1) ---\n",
        "    ax.set_facecolor('#2e7d32')\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=True, color='#388e3c', zorder=0))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=False, color='white', lw=3, zorder=1))\n",
        "    ax.plot([0, 0], [-34, 34], color='white', lw=3, zorder=1)\n",
        "    ax.add_patch(patches.Circle((0, 0), 9.15, edgecolor=\"white\", facecolor=\"none\", lw=3, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((52.5-16.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "\n",
        "    # --- 2. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã®æç”» & æ•°å€¤å‡ºåŠ› (zorder=2) ---\n",
        "    att_weights = att_weights.cpu().numpy().flatten()\n",
        "    edge_index = edge_index.cpu().numpy()\n",
        "\n",
        "    if len(att_weights) > 0:\n",
        "        threshold = np.percentile(att_weights, 98) # ä¸Šä½2%ã«çµžã‚Šè¾¼ã¿\n",
        "        max_att = att_weights.max()\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\" {title} - TOP 2% ATTENTION DETAILS\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"{'Source':<8} | {'Dest':<8} | {'Weight':<10} | {'Team Relation'}\")\n",
        "        print(f\"{'-'*50}\")\n",
        "\n",
        "        # ãƒãƒ¼ãƒ IDå–å¾— (0:æ”»æ’ƒ, 1:å®ˆå‚™, 2:ãƒœãƒ¼ãƒ«)\n",
        "        team_ids = data_item.x[:, 6].cpu().numpy()\n",
        "\n",
        "        for i in range(edge_index.shape[1]):\n",
        "            if att_weights[i] > threshold:\n",
        "                src, dst = edge_index[0, i], edge_index[1, i]\n",
        "                weight = att_weights[i]\n",
        "\n",
        "                # ãƒãƒ¼ãƒ é–¢ä¿‚ã®è¨€èªžåŒ–\n",
        "                rel = \"Teammate\" if team_ids[src] == team_ids[dst] else \"Opponent\"\n",
        "                if team_ids[src] == 2.0: rel = \"Ball -> Player\"\n",
        "\n",
        "                # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«æ•°å€¤ã‚’è¡¨ç¤º\n",
        "                print(f\"Node {src:2d} -> Node {dst:2d} | {weight:.4f}     | {rel}\")\n",
        "\n",
        "                # æç”»\n",
        "                alpha_val = (weight - threshold) / (max_att - threshold + 1e-9)\n",
        "                ax.plot([pos_plot[src, 0], pos_plot[dst, 0]],\n",
        "                        [pos_plot[src, 1], pos_plot[dst, 1]],\n",
        "                        color='#FFFF00', alpha=alpha_val * 0.8, lw=2.0 + alpha_val*4, zorder=2)\n",
        "\n",
        "    # --- 3. é¸æ‰‹ã¨é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã®æç”» (zorder=10-20) ---\n",
        "    num_nodes = pos.shape[0]\n",
        "    vel_scale = 15.0 # é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã®è¦–èªæ€§ã‚’ç¢ºä¿\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        if team_ids[i] == 2.0: # ãƒœãƒ¼ãƒ«\n",
        "            color, marker, size, z = 'gold', '*', 600, 15\n",
        "        elif team_ids[i] == 0.0: # æ”»æ’ƒãƒãƒ¼ãƒ \n",
        "            color, marker, size, z = '#0288d1', 'o', 300, 10\n",
        "        else: # å®ˆå‚™ãƒãƒ¼ãƒ \n",
        "            color, marker, size, z = '#d32f2f', 'o', 300, 10\n",
        "\n",
        "        # é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ« (ax.quiver ã§ç¢ºå®Ÿã«æç”»)\n",
        "        if team_ids[i] != 2.0:\n",
        "            ax.quiver(pos_plot[i, 0], pos_plot[i, 1],\n",
        "                      vel[i, 0], vel[i, 1],\n",
        "                      color='white', alpha=0.9,\n",
        "                      angles='xy', scale_units='xy', scale=1/vel_scale,\n",
        "                      width=0.005, headwidth=4, headlength=5, zorder=20)\n",
        "\n",
        "        # é¸æ‰‹ãƒŽãƒ¼ãƒ‰\n",
        "        ax.scatter(pos_plot[i, 0], pos_plot[i, 1], c=color, marker=marker, s=size,\n",
        "                   edgecolors='white', linewidth=1.5, zorder=15)\n",
        "\n",
        "    # --- ã‚¿ã‚¤ãƒˆãƒ«ã¨è¡¨ç¤ºè¨­å®š ---\n",
        "    res_text = \"SUCCESS\" if pred == 1 else \"FAILURE\"\n",
        "    match_status = \"CORRECT\" if label == pred else \"INCORRECT\"\n",
        "    ax.set_title(f\"{title}\\nActual: {'SUCCESS' if label==1 else 'FAILURE'} | Predicted: {res_text} ({prob:.1%})\\nResult: {match_status}\",\n",
        "                 fontsize=18, fontweight='bold', pad=20)\n",
        "\n",
        "    ax.set_xlim(-60, 60); ax.set_ylim(-40, 40)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 2. è‡ªå‹•æ¯”è¼ƒå®Ÿè¡Œãƒ«ãƒ¼ãƒ—\n",
        "# ==========================================\n",
        "def run_comparison_visualizer(model, data_list, device):\n",
        "    success_case, failure_case = None, None\n",
        "    model.eval()\n",
        "\n",
        "    for data in data_list:\n",
        "        with torch.no_grad():\n",
        "            d_gpu = data.to(device)\n",
        "            out, _ = model(d_gpu, return_attention=True)\n",
        "            pred = out.argmax(dim=1).item()\n",
        "            label = d_gpu.y.item()\n",
        "\n",
        "            if pred == label:\n",
        "                if label == 1 and success_case is None: success_case = data\n",
        "                elif label == 0 and failure_case is None: failure_case = data\n",
        "\n",
        "        if success_case and failure_case: break\n",
        "\n",
        "    if success_case:\n",
        "        visualize_pignn_tactical_analysis(model, success_case, device, title=\"Tactical Analysis: Successful Counter\")\n",
        "    if failure_case:\n",
        "        visualize_pignn_tactical_analysis(model, failure_case, device, title=\"Tactical Analysis: Failed Counter\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³\n",
        "# ==========================================\n",
        "# PIGNNãƒ¢ãƒ‡ãƒ«ã®æº–å‚™\n",
        "pignn_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "\n",
        "pignn_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/alpha_10_0_take3/pignn_testmatch_1.pth\"\n",
        "if os.path.exists(pignn_path):\n",
        "    state_dict = torch.load(pignn_path, map_location=device)\n",
        "    pignn_model.load_state_dict(state_dict, strict=False)\n",
        "    print(\" PIGNNé‡ã¿ã®ãƒ­ãƒ¼ãƒ‰ã«æˆåŠŸã—ã¾ã—ãŸã€‚\")\n",
        "\n",
        "    # å®Ÿè¡Œ\n",
        "    run_comparison_visualizer(pignn_model, all_data_list, device)\n",
        "else:\n",
        "    print(f\" ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pignn_path}\")"
      ],
      "metadata": {
        "id": "fQrj-UNc-OaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "import os\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# ==========================================\n",
        "# 1. å¯è¦–åŒ– & æ•°å€¤æŠ½å‡ºãƒ¡ã‚¤ãƒ³é–¢æ•°\n",
        "# ==========================================\n",
        "def visualize_pignn_tactical_analysis(model, data_item, device, title=\"PIGNN Tactical Analysis\"):\n",
        "    model.eval()\n",
        "    data_item = data_item.to(device)\n",
        "\n",
        "    # --- æŽ¨è«–ã¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æŠ½å‡º ---\n",
        "    with torch.no_grad():\n",
        "        out, (edge_index, att_weights) = model(data_item, return_attention=True)\n",
        "        prob = torch.softmax(out, dim=1)[0, 1].item()\n",
        "        pred = out.argmax(dim=1).item()\n",
        "        label = data_item.y.item()\n",
        "\n",
        "    # --- åº§æ¨™ã¨é€Ÿåº¦ã®å¾©å…ƒ ---\n",
        "    pos = data_item.pos.cpu().numpy()\n",
        "    vel = data_item.vel.cpu().numpy()\n",
        "    pos_plot = np.zeros_like(pos)\n",
        "    pos_plot[:, 0] = pos[:, 0] * 52.5\n",
        "    pos_plot[:, 1] = pos[:, 1] * 34.0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "    # --- ã‚µãƒƒã‚«ãƒ¼å ´ã®æç”» (zorder=0-1) ---\n",
        "    ax.set_facecolor('#2e7d32')\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=True, color='#388e3c', zorder=0))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=False, color='white', lw=3, zorder=1))\n",
        "    ax.plot([0, 0], [-34, 34], color='white', lw=3, zorder=1)\n",
        "    ax.add_patch(patches.Circle((0, 0), 9.15, edgecolor=\"white\", facecolor=\"none\", lw=3, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((52.5-16.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "\n",
        "    # --- 2. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã®æç”» & æ•°å€¤å‡ºåŠ› (zorder=2) ---\n",
        "    att_weights = att_weights.cpu().numpy().flatten()\n",
        "    edge_index = edge_index.cpu().numpy()\n",
        "\n",
        "    if len(att_weights) > 0:\n",
        "        threshold = np.percentile(att_weights, 98) # ä¸Šä½2%ã«çµžã‚Šè¾¼ã¿\n",
        "        max_att = att_weights.max()\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"ðŸ“Š {title} - TOP 2% ATTENTION DETAILS\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"{'Source':<8} | {'Dest':<8} | {'Weight':<10} | {'Team Relation'}\")\n",
        "        print(f\"{'-'*50}\")\n",
        "\n",
        "        # ãƒãƒ¼ãƒ IDå–å¾— (0:æ”»æ’ƒ, 1:å®ˆå‚™, 2:ãƒœãƒ¼ãƒ«)\n",
        "        team_ids = data_item.x[:, 6].cpu().numpy()\n",
        "\n",
        "        for i in range(edge_index.shape[1]):\n",
        "            if att_weights[i] > threshold:\n",
        "                src, dst = edge_index[0, i], edge_index[1, i]\n",
        "                weight = att_weights[i]\n",
        "\n",
        "                # ãƒãƒ¼ãƒ é–¢ä¿‚ã®è¨€èªžåŒ–\n",
        "                rel = \"Teammate\" if team_ids[src] == team_ids[dst] else \"Opponent\"\n",
        "                if team_ids[src] == 2.0: rel = \"Ball -> Player\"\n",
        "\n",
        "                # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«æ•°å€¤ã‚’è¡¨ç¤º\n",
        "                print(f\"Node {src:2d} -> Node {dst:2d} | {weight:.4f}     | {rel}\")\n",
        "\n",
        "                # æç”»\n",
        "                alpha_val = (weight - threshold) / (max_att - threshold + 1e-9)\n",
        "                ax.plot([pos_plot[src, 0], pos_plot[dst, 0]],\n",
        "                        [pos_plot[src, 1], pos_plot[dst, 1]],\n",
        "                        color='#FFFF00', alpha=alpha_val * 0.8, lw=2.0 + alpha_val*4, zorder=2)\n",
        "\n",
        "    # --- 3. é¸æ‰‹ã¨é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã®æç”» (zorder=10-20) ---\n",
        "    num_nodes = pos.shape[0]\n",
        "    vel_scale = 15.0 # é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã®è¦–èªæ€§ã‚’ç¢ºä¿\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        if team_ids[i] == 2.0: # ãƒœãƒ¼ãƒ«\n",
        "            color, marker, size, z = 'gold', '*', 600, 15\n",
        "        elif team_ids[i] == 0.0: # æ”»æ’ƒãƒãƒ¼ãƒ \n",
        "            color, marker, size, z = '#0288d1', 'o', 300, 10\n",
        "        else: # å®ˆå‚™ãƒãƒ¼ãƒ \n",
        "            color, marker, size, z = '#d32f2f', 'o', 300, 10\n",
        "\n",
        "        # é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ« (ax.quiver ã§ç¢ºå®Ÿã«æç”»)\n",
        "        if team_ids[i] != 2.0:\n",
        "            ax.quiver(pos_plot[i, 0], pos_plot[i, 1],\n",
        "                      vel[i, 0], vel[i, 1],\n",
        "                      color='white', alpha=0.9,\n",
        "                      angles='xy', scale_units='xy', scale=1/vel_scale,\n",
        "                      width=0.005, headwidth=4, headlength=5, zorder=20)\n",
        "\n",
        "        # é¸æ‰‹ãƒŽãƒ¼ãƒ‰\n",
        "        ax.scatter(pos_plot[i, 0], pos_plot[i, 1], c=color, marker=marker, s=size,\n",
        "                   edgecolors='white', linewidth=1.5, zorder=15)\n",
        "\n",
        "    # --- ã‚¿ã‚¤ãƒˆãƒ«ã¨è¡¨ç¤ºè¨­å®š ---\n",
        "    res_text = \"SUCCESS\" if pred == 1 else \"FAILURE\"\n",
        "    match_status = \"CORRECT\" if label == pred else \"INCORRECT\"\n",
        "    ax.set_title(f\"{title}\\nActual: {'SUCCESS' if label==1 else 'FAILURE'} | Predicted: {res_text} ({prob:.1%})\\nResult: {match_status}\",\n",
        "                 fontsize=18, fontweight='bold', pad=20)\n",
        "\n",
        "    ax.set_xlim(-60, 60); ax.set_ylim(-40, 40)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 2. è‡ªå‹•æ¯”è¼ƒå®Ÿè¡Œãƒ«ãƒ¼ãƒ—\n",
        "# ==========================================\n",
        "def run_comparison_visualizer(model, data_list, device):\n",
        "    success_case, failure_case = None, None\n",
        "    model.eval()\n",
        "\n",
        "    for data in data_list:\n",
        "        with torch.no_grad():\n",
        "            d_gpu = data.to(device)\n",
        "            out, _ = model(d_gpu, return_attention=True)\n",
        "            pred = out.argmax(dim=1).item()\n",
        "            label = d_gpu.y.item()\n",
        "\n",
        "            if pred == label:\n",
        "                if label == 1 and success_case is None: success_case = data\n",
        "                elif label == 0 and failure_case is None: failure_case = data\n",
        "\n",
        "        if success_case and failure_case: break\n",
        "\n",
        "    if success_case:\n",
        "        visualize_pignn_tactical_analysis(model, success_case, device, title=\"Tactical Analysis: Successful Counter\")\n",
        "    if failure_case:\n",
        "        visualize_pignn_tactical_analysis(model, failure_case, device, title=\"Tactical Analysis: Failed Counter\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³\n",
        "# ==========================================\n",
        "# PIGNNãƒ¢ãƒ‡ãƒ«ã®æº–å‚™\n",
        "pignn_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "\n",
        "pignn_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/alpha_100_0_take3/pignn_testmatch_3.pth\"\n",
        "if os.path.exists(pignn_path):\n",
        "    state_dict = torch.load(pignn_path, map_location=device)\n",
        "    pignn_model.load_state_dict(state_dict, strict=False)\n",
        "    print(\" PIGNNé‡ã¿ã®ãƒ­ãƒ¼ãƒ‰ã«æˆåŠŸã—ã¾ã—ãŸã€‚\")\n",
        "\n",
        "    # å®Ÿè¡Œ\n",
        "    run_comparison_visualizer(pignn_model, all_data_list, device)\n",
        "else:\n",
        "    print(f\" ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pignn_path}\")"
      ],
      "metadata": {
        "id": "ZWBZZZdHzSwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "import os\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# ==========================================\n",
        "# 1. å¯è¦–åŒ– & æ•°å€¤æŠ½å‡ºãƒ¡ã‚¤ãƒ³é–¢æ•°\n",
        "# ==========================================\n",
        "def visualize_pignn_tactical_analysis(model, data_item, device, title=\"PIGNN Tactical Analysis\"):\n",
        "    model.eval()\n",
        "    data_item = data_item.to(device)\n",
        "\n",
        "    # --- æŽ¨è«–ã¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æŠ½å‡º ---\n",
        "    with torch.no_grad():\n",
        "        out, (edge_index, att_weights) = model(data_item, return_attention=True)\n",
        "        prob = torch.softmax(out, dim=1)[0, 1].item()\n",
        "        pred = out.argmax(dim=1).item()\n",
        "        label = data_item.y.item()\n",
        "\n",
        "    # --- åº§æ¨™ã¨é€Ÿåº¦ã®å¾©å…ƒ ---\n",
        "    pos = data_item.pos.cpu().numpy()\n",
        "    vel = data_item.vel.cpu().numpy()\n",
        "    pos_plot = np.zeros_like(pos)\n",
        "    pos_plot[:, 0] = pos[:, 0] * 52.5\n",
        "    pos_plot[:, 1] = pos[:, 1] * 34.0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "    # --- ã‚µãƒƒã‚«ãƒ¼å ´ã®æç”» (zorder=0-1) ---\n",
        "    ax.set_facecolor('#2e7d32')\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=True, color='#388e3c', zorder=0))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=False, color='white', lw=3, zorder=1))\n",
        "    ax.plot([0, 0], [-34, 34], color='white', lw=3, zorder=1)\n",
        "    ax.add_patch(patches.Circle((0, 0), 9.15, edgecolor=\"white\", facecolor=\"none\", lw=3, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((52.5-16.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "\n",
        "    # --- 2. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã®æç”» & æ•°å€¤å‡ºåŠ› (zorder=2) ---\n",
        "    att_weights = att_weights.cpu().numpy().flatten()\n",
        "    edge_index = edge_index.cpu().numpy()\n",
        "\n",
        "    if len(att_weights) > 0:\n",
        "        threshold = np.percentile(att_weights, 98) # ä¸Šä½2%ã«çµžã‚Šè¾¼ã¿\n",
        "        max_att = att_weights.max()\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"ðŸ“Š {title} - TOP 2% ATTENTION DETAILS\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"{'Source':<8} | {'Dest':<8} | {'Weight':<10} | {'Team Relation'}\")\n",
        "        print(f\"{'-'*50}\")\n",
        "\n",
        "        # ãƒãƒ¼ãƒ IDå–å¾— (0:æ”»æ’ƒ, 1:å®ˆå‚™, 2:ãƒœãƒ¼ãƒ«)\n",
        "        team_ids = data_item.x[:, 6].cpu().numpy()\n",
        "\n",
        "        for i in range(edge_index.shape[1]):\n",
        "            if att_weights[i] > threshold:\n",
        "                src, dst = edge_index[0, i], edge_index[1, i]\n",
        "                weight = att_weights[i]\n",
        "\n",
        "                # ãƒãƒ¼ãƒ é–¢ä¿‚ã®è¨€èªžåŒ–\n",
        "                rel = \"Teammate\" if team_ids[src] == team_ids[dst] else \"Opponent\"\n",
        "                if team_ids[src] == 2.0: rel = \"Ball -> Player\"\n",
        "\n",
        "                # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«æ•°å€¤ã‚’è¡¨ç¤º\n",
        "                print(f\"Node {src:2d} -> Node {dst:2d} | {weight:.4f}     | {rel}\")\n",
        "\n",
        "                # æç”»\n",
        "                alpha_val = (weight - threshold) / (max_att - threshold + 1e-9)\n",
        "                ax.plot([pos_plot[src, 0], pos_plot[dst, 0]],\n",
        "                        [pos_plot[src, 1], pos_plot[dst, 1]],\n",
        "                        color='#FFFF00', alpha=alpha_val * 0.8, lw=2.0 + alpha_val*4, zorder=2)\n",
        "\n",
        "    # --- 3. é¸æ‰‹ã¨é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã®æç”» (zorder=10-20) ---\n",
        "    num_nodes = pos.shape[0]\n",
        "    vel_scale = 15.0 # é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã®è¦–èªæ€§ã‚’ç¢ºä¿\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        if team_ids[i] == 2.0: # ãƒœãƒ¼ãƒ«\n",
        "            color, marker, size, z = 'gold', '*', 600, 15\n",
        "        elif team_ids[i] == 0.0: # æ”»æ’ƒãƒãƒ¼ãƒ \n",
        "            color, marker, size, z = '#0288d1', 'o', 300, 10\n",
        "        else: # å®ˆå‚™ãƒãƒ¼ãƒ \n",
        "            color, marker, size, z = '#d32f2f', 'o', 300, 10\n",
        "\n",
        "        # é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ« (ax.quiver ã§ç¢ºå®Ÿã«æç”»)\n",
        "        if team_ids[i] != 2.0:\n",
        "            ax.quiver(pos_plot[i, 0], pos_plot[i, 1],\n",
        "                      vel[i, 0], vel[i, 1],\n",
        "                      color='white', alpha=0.9,\n",
        "                      angles='xy', scale_units='xy', scale=1/vel_scale,\n",
        "                      width=0.005, headwidth=4, headlength=5, zorder=20)\n",
        "\n",
        "        # é¸æ‰‹ãƒŽãƒ¼ãƒ‰\n",
        "        ax.scatter(pos_plot[i, 0], pos_plot[i, 1], c=color, marker=marker, s=size,\n",
        "                   edgecolors='white', linewidth=1.5, zorder=15)\n",
        "\n",
        "    # --- ã‚¿ã‚¤ãƒˆãƒ«ã¨è¡¨ç¤ºè¨­å®š ---\n",
        "    res_text = \"SUCCESS\" if pred == 1 else \"FAILURE\"\n",
        "    match_status = \"CORRECT\" if label == pred else \"INCORRECT\"\n",
        "    ax.set_title(f\"{title}\\nActual: {'SUCCESS' if label==1 else 'FAILURE'} | Predicted: {res_text} ({prob:.1%})\\nResult: {match_status}\",\n",
        "                 fontsize=18, fontweight='bold', pad=20)\n",
        "\n",
        "    ax.set_xlim(-60, 60); ax.set_ylim(-40, 40)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 2. è‡ªå‹•æ¯”è¼ƒå®Ÿè¡Œãƒ«ãƒ¼ãƒ—\n",
        "# ==========================================\n",
        "def run_comparison_visualizer(model, data_list, device):\n",
        "    success_case, failure_case = None, None\n",
        "    model.eval()\n",
        "\n",
        "    for data in data_list:\n",
        "        with torch.no_grad():\n",
        "            d_gpu = data.to(device)\n",
        "            out, _ = model(d_gpu, return_attention=True)\n",
        "            pred = out.argmax(dim=1).item()\n",
        "            label = d_gpu.y.item()\n",
        "\n",
        "            if pred == label:\n",
        "                if label == 1 and success_case is None: success_case = data\n",
        "                elif label == 0 and failure_case is None: failure_case = data\n",
        "\n",
        "        if success_case and failure_case: break\n",
        "\n",
        "    if success_case:\n",
        "        visualize_pignn_tactical_analysis(model, success_case, device, title=\"Tactical Analysis: Successful Counter\")\n",
        "    if failure_case:\n",
        "        visualize_pignn_tactical_analysis(model, failure_case, device, title=\"Tactical Analysis: Failed Counter\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³\n",
        "# ==========================================\n",
        "# PIGNNãƒ¢ãƒ‡ãƒ«ã®æº–å‚™\n",
        "pignn_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "\n",
        "pignn_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/alpha_100_0_take3/pignn_testmatch_2.pth\"\n",
        "if os.path.exists(pignn_path):\n",
        "    state_dict = torch.load(pignn_path, map_location=device)\n",
        "    pignn_model.load_state_dict(state_dict, strict=False)\n",
        "    print(\" PIGNNé‡ã¿ã®ãƒ­ãƒ¼ãƒ‰ã«æˆåŠŸã—ã¾ã—ãŸã€‚\")\n",
        "\n",
        "    # å®Ÿè¡Œ\n",
        "    run_comparison_visualizer(pignn_model, all_data_list, device)\n",
        "else:\n",
        "    print(f\" ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pignn_path}\")"
      ],
      "metadata": {
        "id": "Rg-T-Om4w4Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Explainerã®ãŸã‚ã«åˆã‚ã›ã‚‹\n",
        "\n",
        "class PIGNNClassifier(nn.Module):\n",
        "    def __init__(self, hidden_channels=64):\n",
        "        super(PIGNNClassifier, self).__init__()\n",
        "        self.conv1 = PIGNNLayer(7, hidden_channels, tau=0.2)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels, tau=0.2)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    #  GNNExplainerãŒx, edge_index, batchãªã©ã‚’å€‹åˆ¥ã«æ¸¡ã›ã‚‹ã‚ˆã†ã«ä¿®æ­£\n",
        "    def forward(self, x, edge_index, batch=None, pos=None, vel=None, return_attention=False, **kwargs):\n",
        "        # batchãŒNoneã®å ´åˆã¯å˜ä¸€ã‚°ãƒ©ãƒ•ã¨ã—ã¦æ‰±ã†\n",
        "        if batch is None:\n",
        "            batch = torch.zeros(x.size(0), dtype=torch.long, device=x.device)\n",
        "\n",
        "        if return_attention:\n",
        "            x, (edge_idx_out, att_weights) = self.conv1(x, edge_index, pos, vel, return_attention=True)\n",
        "            x = F.elu(x)\n",
        "            x = self.conv2(x, edge_index, pos, vel)\n",
        "        else:\n",
        "            x = F.elu(self.conv1(x, edge_index, pos, vel))\n",
        "            x = self.conv2(x, edge_index, pos, vel)\n",
        "\n",
        "        x_pool = global_mean_pool(x, batch)\n",
        "        logits = F.log_softmax(self.lin(x_pool), dim=1)\n",
        "\n",
        "        if return_attention:\n",
        "            return logits, (edge_idx_out, att_weights)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "tTzJuQzkmI4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.explain import Explainer, GNNExplainer\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã¨é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ‘ã‚¹ã¯ã‚ãªãŸã®ç’°å¢ƒã«åˆã‚ã›ã¦ï¼‰\n",
        "pignn_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "pignn_model.load_state_dict(torch.load(pignn_path))\n",
        "\n",
        "# Explainerã®è¨­å®šï¼ˆã“ã“ã¯ä»¥å‰ã¨åŒã˜ï¼‰\n",
        "explainer = Explainer(\n",
        "    model=pignn_model,\n",
        "    algorithm=GNNExplainer(epochs=200),\n",
        "    explanation_type='model',\n",
        "    node_mask_type='attributes',\n",
        "    edge_mask_type='object',\n",
        "    model_config=dict(\n",
        "        mode='multiclass_classification',\n",
        "        task_level='graph',\n",
        "        return_type='log_probs',\n",
        "    ),\n",
        ")\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "data_single = all_data_list[0].to(device)\n",
        "explanation = explainer(\n",
        "    x=data_single.x,\n",
        "    edge_index=data_single.edge_index,\n",
        "    batch=torch.zeros(data_single.x.size(0), dtype=torch.long, device=device),\n",
        "    pos=data_single.pos,\n",
        "    vel=data_single.vel\n",
        ")"
      ],
      "metadata": {
        "id": "mco-rzpqjBSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8857f523"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ç‰¹å¾´é‡ãƒ©ãƒ™ãƒ«ï¼ˆã‚ãªãŸã®ãƒ‡ãƒ¼ã‚¿å½¢å¼ã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„ï¼‰\n",
        "# ä¾‹: 0:x, 1:y, 2:vx, 3:vy, 4:dist_goal, 5:dist_ball, 6:team_id ...\n",
        "labels = [\n",
        "    'Position_X',        # Index 0\n",
        "    'Position_Y',        # Index 1\n",
        "    'Velocity_X',        # Index 2\n",
        "    'Velocity_Y',        # Index 3\n",
        "    'Distance_to_Goal',  # Index 4\n",
        "    'Distance_to_Ball',  # Index 5\n",
        "    'Team_Flag'          # Index 6\n",
        "]\n",
        "\n",
        "# ç‰¹å¾´é‡ã”ã¨ã®é‡è¦åº¦ï¼ˆnode_maskï¼‰ã‚’å…¨ãƒŽãƒ¼ãƒ‰ã§é›†è¨ˆ\n",
        "feat_importance = explanation.node_mask.abs().sum(dim=0).cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(labels[:len(feat_importance)], feat_importance)\n",
        "plt.title(\"Feature Importance (GNNExplainer)\")\n",
        "plt.ylabel(\"Importance Score\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def explain_and_visualize_comparison(explainer, data_list, device):\n",
        "    # --- 1. æˆåŠŸã‚·ãƒ¼ãƒ³(ProbãŒé«˜ã„)ã¨å¤±æ•—ã‚·ãƒ¼ãƒ³(ProbãŒä½Žã„)ã‚’æŽ¢ç´¢ ---\n",
        "    target_success_idx = -1\n",
        "    target_failure_idx = -1\n",
        "\n",
        "    print(\" Searching for comparison cases...\")\n",
        "    pignn_model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(data_list):\n",
        "            data = data.to(device)\n",
        "            if not hasattr(data, 'batch') or data.batch is None:\n",
        "                data.batch = torch.zeros(data.x.size(0), dtype=torch.long, device=device)\n",
        "\n",
        "            out = pignn_model(data)\n",
        "            prob = torch.softmax(out, dim=1)[0, 1].item()\n",
        "\n",
        "            if prob > 0.9 and target_success_idx == -1:\n",
        "                target_success_idx = i\n",
        "            if prob < 0.1 and target_failure_idx == -1:\n",
        "                target_failure_idx = i\n",
        "\n",
        "            if target_success_idx != -1 and target_failure_idx != -1:\n",
        "                break\n",
        "\n",
        "    # è¡¨ç¤ºåã‚’è‹±èªžã«å¤‰æ›´ã—ã¦æ–‡å­—åŒ–ã‘ã‚’å›žé¿\n",
        "    indices = {\"AI Prediction: SUCCESS\": target_success_idx, \"AI Prediction: FAILURE\": target_failure_idx}\n",
        "\n",
        "    # --- 2. é †ã«å¯è¦–åŒ– ---\n",
        "    for title_prefix, idx in indices.items():\n",
        "        if idx == -1:\n",
        "            print(f\" {title_prefix} matching scene not found.\")\n",
        "            continue\n",
        "\n",
        "        data_single = data_list[idx].to(device)\n",
        "        print(f\"\\nðŸŽ¬ Analyzing {title_prefix} (Index: {idx})...\")\n",
        "\n",
        "        explanation = explainer(\n",
        "            x=data_single.x,\n",
        "            edge_index=data_single.edge_index,\n",
        "            batch=torch.zeros(data_single.x.size(0), dtype=torch.long, device=device),\n",
        "            pos=data_single.pos,\n",
        "            vel=data_single.vel\n",
        "        )\n",
        "\n",
        "        # --- A. æ£’ã‚°ãƒ©ãƒ• (ç‰¹å¾´é‡é‡è¦åº¦) ---\n",
        "        feat_importance = explanation.node_mask.abs().sum(dim=0).cpu().numpy()\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        # æˆåŠŸãªã‚‰Skyblue, å¤±æ•—ãªã‚‰Salmonã§è‰²åˆ†ã‘\n",
        "        bar_color = 'skyblue' if \"SUCCESS\" in title_prefix else 'salmon'\n",
        "        plt.bar(labels[:len(feat_importance)], feat_importance, color=bar_color)\n",
        "        plt.title(f\"Feature Importance: {title_prefix}\")\n",
        "        plt.ylabel(\"Importance Score\")\n",
        "        plt.xticks(rotation=30)\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.5) # ã‚°ãƒªãƒƒãƒ‰ã‚’è¿½åŠ ã—ã¦è¦‹ã‚„ã™ã\n",
        "        plt.tight_layout() # ãƒ©ãƒ™ãƒ«ã®åˆ‡ã‚Œã‚’é˜²æ­¢\n",
        "        plt.show()\n",
        "\n",
        "        # --- B. ãƒ”ãƒƒãƒå›³ (æˆ¦è¡“å¯è¦–åŒ–) ---\n",
        "        visualize_explanation_on_pitch_fixed(\n",
        "            explanation, data_single,\n",
        "            title=f\"{title_prefix}\\n(GT: {'Success' if data_single.y.item()==1 else 'Failure'})\"\n",
        "        )\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "explain_and_visualize_comparison(explainer, all_data_list, device)"
      ],
      "metadata": {
        "id": "IuHGmzUl3iRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# é‡è¦åº¦ã®é«˜ã„ã‚¨ãƒƒã‚¸ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŠ½å‡º\n",
        "edge_mask = explanation.edge_mask.cpu().numpy()\n",
        "edge_index = data_single.edge_index.cpu().numpy()\n",
        "\n",
        "# ä¸Šä½5%ã®ã‚¨ãƒƒã‚¸ã‚’æŠ½å‡º\n",
        "threshold = np.percentile(edge_mask, 95)\n",
        "\n",
        "# ã“ã®ä¸Šä½ã‚¨ãƒƒã‚¸ã‚’ã€ä»¥å‰ä½œæˆã—ãŸã€Œãƒ”ãƒƒãƒå¯è¦–åŒ–ã‚³ãƒ¼ãƒ‰ã€ã®\n",
        "# é»„è‰²ã„ç·šã®ä»£ã‚ã‚Šã«æç”»ã™ã‚Œã°ã€ã€ŒAIãŒé¸ã‚“ã çœŸã®æˆ¦è¡“ã€ãŒæµ®ã‹ã³ä¸ŠãŒã‚Šã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "lnGLcPPGmwXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_explanation_on_pitch_fixed(explanation, data, title=\"GNNExplainer Tactical Insight\"):\n",
        "    # --- 1. åº§æ¨™ã®å¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£ ---\n",
        "    pos = data.pos.cpu().numpy()\n",
        "\n",
        "    # æ­£è¦åŒ–ç¯„å›²ãŒ -1 ~ 1 ã®å ´åˆï¼š\n",
        "    # ãã®ã¾ã¾ãƒ”ãƒƒãƒã‚µã‚¤ã‚ºï¼ˆåŠåˆ†ï¼‰ã‚’æŽ›ã‘ã‚‹ã ã‘ã§ã€ä¸­å¤®(0,0)ã‚’åŸºæº–ã«é…ç½®ã•ã‚Œã¾ã™\n",
        "    pos_plot_x = pos[:, 0] * 52.5  # -1*52.5 = -52.5 (å·¦ç«¯), 1*52.5 = 52.5 (å³ç«¯)\n",
        "    pos_plot_y = pos[:, 1] * 34.0  # -1*34.0 = -34.0 (ä¸‹ç«¯), 1*34.0 = 34.0 (ä¸Šç«¯)\n",
        "\n",
        "    edge_index = data.edge_index.cpu().numpy()\n",
        "    edge_mask = explanation.edge_mask.cpu().numpy()\n",
        "\n",
        "    # ãƒãƒ¼ãƒ IDå–å¾— (xã®7ç•ªç›®: index 6)\n",
        "    # 0.0:æ”»æ’ƒ, 1.0:å®ˆå‚™, 2.0:ãƒœãƒ¼ãƒ«\n",
        "    team_ids = data.x[:, 6].cpu().numpy()\n",
        "\n",
        "    # --- 2. ãƒ”ãƒƒãƒæç”» (èƒŒæ™¯) ---\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    ax.set_facecolor('#2e7d32')\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=True, color='#388e3c', zorder=0))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=False, color='white', lw=2, zorder=1))\n",
        "    ax.plot([0, 0], [-34, 34], color='white', lw=2, zorder=1)\n",
        "    ax.add_patch(patches.Circle((0, 0), 9.15, edgecolor=\"white\", facecolor=\"none\", lw=2, zorder=1))\n",
        "\n",
        "    # --- 3. é‡è¦ã‚¨ãƒƒã‚¸ã®æç”» ---\n",
        "    threshold = np.percentile(edge_mask, 98)\n",
        "    max_val = edge_mask.max()\n",
        "\n",
        "    for i in range(len(edge_mask)):\n",
        "        if edge_mask[i] >= threshold:\n",
        "            src, dst = edge_index[0, i], edge_index[1, i]\n",
        "            val_norm = (edge_mask[i] - threshold) / (max_val - threshold + 1e-9)\n",
        "            ax.plot([pos_plot_x[src], pos_plot_x[dst]],\n",
        "                    [pos_plot_y[src], pos_plot_y[dst]],\n",
        "                    color='yellow', alpha=0.4 + 0.6*val_norm,\n",
        "                    lw=2 + 6*val_norm, zorder=2)\n",
        "\n",
        "    # --- 4. é¸æ‰‹ãƒ»ãƒœãƒ¼ãƒ«ã®æç”» ---\n",
        "    for i in range(len(pos)):\n",
        "        # ãƒãƒ¼ãƒ ãƒ©ãƒ™ãƒ«ã«åŸºã¥ã„ãŸè‰²åˆ†ã‘\n",
        "        if team_ids[i] == 2.0: # ãƒœãƒ¼ãƒ«\n",
        "            color, marker, size = 'gold', '*', 500\n",
        "        elif team_ids[i] == 0.0: # æ”»æ’ƒ\n",
        "            color, marker, size = '#0288d1', 'o', 250\n",
        "        else: # å®ˆå‚™\n",
        "            color, marker, size = '#d32f2f', 'o', 250\n",
        "\n",
        "        ax.scatter(pos_plot_x[i], pos_plot_y[i], c=color, marker=marker,\n",
        "                   s=size, edgecolors='white', linewidth=1.5, zorder=3)\n",
        "\n",
        "    ax.set_title(title, fontsize=15, pad=20)\n",
        "    ax.set_xlim(-60, 60); ax.set_ylim(-40, 40)\n",
        "    plt.show()\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "visualize_explanation_on_pitch_fixed(explanation, data_single)"
      ],
      "metadata": {
        "id": "x1NJ_a8UnFgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãƒ©ãƒ™ãƒ«: {explanation.prediction.argmax().item()} (0:å¤±æ•—, 1:æˆåŠŸ)\")\n",
        "print(f\"å®Ÿéš›ã®æ­£è§£ãƒ©ãƒ™ãƒ«: {data_single.y.item()} (0:å¤±æ•—, 1:æˆåŠŸ)\")"
      ],
      "metadata": {
        "id": "831d8abIFULg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_and_visualize_tp_final(model, dataset, num_samples=1):\n",
        "    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "    tp_indices = []\n",
        "\n",
        "    print(f\" æˆåŠŸçš„ä¸­(True Positive)ã‚·ãƒ¼ãƒ³ã‚’æŽ¢ç´¢ä¸­... (Device: {device})\")\n",
        "    for i, data in enumerate(dataset):\n",
        "        # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€\n",
        "        x = data.x.to(device).to(torch.float)\n",
        "        edge_index = data.edge_index.to(device).to(torch.long)\n",
        "        pos = data.pos.to(device).to(torch.float)\n",
        "        vel = data.vel.to(device).to(torch.float)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # forwardã®å¼•æ•°åã¯ã‚ãªãŸã®ãƒ¢ãƒ‡ãƒ«å®šç¾©ã«åˆã‚ã›ã¦é©å®œèª¿æ•´ã—ã¦ãã ã•ã„\n",
        "            # ã“ã“ã§ã¯ (x, edge_index, pos, vel) ã‚’æƒ³å®š\n",
        "            out = model(x, edge_index, pos=pos, vel=vel)\n",
        "\n",
        "            pred = out.argmax(dim=1).item()\n",
        "            actual = data.y.item()\n",
        "\n",
        "            if pred == 1 and actual == 1:\n",
        "                prob = torch.exp(out[0][1]).item()\n",
        "                tp_indices.append((i, prob))\n",
        "\n",
        "    tp_indices = sorted(tp_indices, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if not tp_indices:\n",
        "        print(\"âŒ æˆåŠŸçš„ä¸­ã‚·ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "        return\n",
        "\n",
        "    print(f\" {len(tp_indices)} ä»¶ã®æˆåŠŸçš„ä¸­ã‚·ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ä¸Šä½ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚\")\n",
        "\n",
        "    from torch_geometric.explain import Explainer, GNNExplainer\n",
        "\n",
        "    # Explainerç”¨ã®ãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆãƒ‡ãƒã‚¤ã‚¹å¯¾å¿œï¼‰\n",
        "    class ExplainerWrapper(torch.nn.Module):\n",
        "        def __init__(self, model):\n",
        "            super().__init__()\n",
        "            self.model = model\n",
        "\n",
        "        def forward(self, x, edge_index, **kwargs):\n",
        "            # Explainerã‹ã‚‰æ¸¡ã•ã‚Œã‚‹åº§æ¨™ã‚„é€Ÿåº¦ã‚‚ãƒ‡ãƒã‚¤ã‚¹ã‚’åˆã‚ã›ã‚‹\n",
        "            pos = kwargs.get('pos')\n",
        "            vel = kwargs.get('vel')\n",
        "            return self.model(x, edge_index, pos=pos, vel=vel)\n",
        "\n",
        "    wrapper = ExplainerWrapper(model)\n",
        "\n",
        "    explainer = Explainer(\n",
        "        model=wrapper,\n",
        "        algorithm=GNNExplainer(epochs=200),\n",
        "        explanation_type='model',\n",
        "        node_mask_type='attributes',\n",
        "        edge_mask_type='object',\n",
        "        model_config=dict(\n",
        "            mode='multiclass_classification',\n",
        "            task_level='graph',\n",
        "            return_type='log_probs',\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    for i in range(min(num_samples, len(tp_indices))):\n",
        "        idx, prob = tp_indices[i]\n",
        "        # å…ƒãƒ‡ãƒ¼ã‚¿ã‚’GPUã«é€ã£ã¦ã‹ã‚‰Explainerã«æ¸¡ã™\n",
        "        data_single = dataset[idx].to(device)\n",
        "\n",
        "        explanation = explainer(\n",
        "            data_single.x,\n",
        "            data_single.edge_index,\n",
        "            pos=data_single.pos,\n",
        "            vel=data_single.vel\n",
        "        )\n",
        "\n",
        "        title = f\"TRUE POSITIVE (AI Correct!)\\nActual: Success, Pred: Success (Prob: {prob:.2f})\"\n",
        "        visualize_explanation_on_pitch_fixed(explanation, data_single, title=title)\n",
        "        print(f\" Index {idx} ã®å¯è¦–åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "find_and_visualize_tp_final(pignn_model, all_data_list, num_samples=3)"
      ],
      "metadata": {
        "id": "L3pNzjCRFsBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_tp_by_rank(model, dataset, rank=0):\n",
        "    \"\"\"\n",
        "    rank: 0ãªã‚‰æœ€ã‚‚ç¢ºçŽ‡ãŒé«˜ã„ã‚·ãƒ¼ãƒ³ã€1ãªã‚‰2ç•ªç›®...ã‚’è¡¨ç¤º\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "    tp_indices = []\n",
        "\n",
        "    # 1. ã¾ãšå…¨ã¦ã®æˆåŠŸçš„ä¸­(TP)ã‚·ãƒ¼ãƒ³ã‚’æŠ½å‡º\n",
        "    print(f\" æˆåŠŸçš„ä¸­(True Positive)ã‚·ãƒ¼ãƒ³ã‚’ãƒ©ãƒ³ã‚­ãƒ³ã‚°åŒ–ä¸­...\")\n",
        "    for i, data in enumerate(dataset):\n",
        "        x = data.x.to(device).to(torch.float)\n",
        "        edge_index = data.edge_index.to(device).to(torch.long)\n",
        "        pos = data.pos.to(device).to(torch.float)\n",
        "        vel = data.vel.to(device).to(torch.float)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(x, edge_index, pos=pos, vel=vel)\n",
        "            pred = out.argmax(dim=1).item()\n",
        "            actual = data.y.item()\n",
        "\n",
        "            if pred == 1 and actual == 1:\n",
        "                # æˆåŠŸ(index 1)ã®ç¢ºçŽ‡ã‚’å–å¾—\n",
        "                prob = torch.exp(out[0][1]).item()\n",
        "                tp_indices.append((i, prob))\n",
        "\n",
        "    # 2. ç¢ºçŽ‡ãŒé«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ\n",
        "    tp_indices = sorted(tp_indices, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if not tp_indices:\n",
        "        print(\"âŒ æˆåŠŸçš„ä¸­ã‚·ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "        return\n",
        "\n",
        "    if rank >= len(tp_indices):\n",
        "        print(f\"âŒ æŒ‡å®šã•ã‚ŒãŸé †ä½(rank={rank})ã¯ç¯„å›²å¤–ã§ã™ã€‚å…¨{len(tp_indices)}ä»¶ã§ã™ã€‚\")\n",
        "        return\n",
        "\n",
        "    # 3. æŒ‡å®šã•ã‚ŒãŸé †ä½ã®ã‚·ãƒ¼ãƒ³ã‚’1æžšã ã‘è©³ã—ãè§£æž\n",
        "    idx, prob = tp_indices[rank]\n",
        "    print(f\" è‡ªä¿¡åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç¬¬ {rank+1} ä½ (Index: {idx}, Prob: {prob:.4f}) ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚\")\n",
        "\n",
        "    from torch_geometric.explain import Explainer, GNNExplainer\n",
        "\n",
        "    class ExplainerWrapper(torch.nn.Module):\n",
        "        def __init__(self, model):\n",
        "            super().__init__()\n",
        "            self.model = model\n",
        "        def forward(self, x, edge_index, **kwargs):\n",
        "            return self.model(x, edge_index, pos=kwargs.get('pos'), vel=kwargs.get('vel'))\n",
        "\n",
        "    wrapper = ExplainerWrapper(model)\n",
        "    explainer = Explainer(\n",
        "        model=wrapper,\n",
        "        algorithm=GNNExplainer(epochs=200), # 200ã‚ã‚Œã°ååˆ†ã§ã™\n",
        "        explanation_type='model',\n",
        "        node_mask_type='attributes',\n",
        "        edge_mask_type='object',\n",
        "        model_config=dict(mode='multiclass_classification', task_level='graph', return_type='log_probs'),\n",
        "    )\n",
        "\n",
        "    data_single = dataset[idx].to(device)\n",
        "    explanation = explainer(data_single.x, data_single.edge_index, pos=data_single.pos, vel=data_single.vel)\n",
        "\n",
        "    title = f\"TRUE POSITIVE - Rank: {rank+1}\\nActual: Success, Pred: Success (Prob: {prob:.3f})\\nData Index: {idx}\"\n",
        "    visualize_explanation_on_pitch_fixed(explanation, data_single, title=title)\n",
        "\n",
        "# --- å®Ÿè¡Œã‚¨ãƒªã‚¢ ---\n",
        "# rank=0 ã§æœ€å¼·ã®æˆåŠŸã‚·ãƒ¼ãƒ³ã€rank=1, 2... ã¨å¤‰ãˆã¦å®Ÿè¡Œã—ã¦ãã ã•ã„\n",
        "visualize_tp_by_rank(pignn_model, all_data_list, rank=30)"
      ],
      "metadata": {
        "id": "sBj_un5RGrjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_and_visualize_by_case(model, dataset, case_type='TP', num_samples=1):\n",
        "    \"\"\"\n",
        "    case_type:\n",
        "        'TP' (True Positive) : å®Ÿéš›æˆåŠŸ / äºˆæ¸¬æˆåŠŸ (çš„ä¸­)\n",
        "        'TN' (True Negative) : å®Ÿéš›å¤±æ•— / äºˆæ¸¬å¤±æ•— (çš„ä¸­) -> å¤±æ•—ã‚·ãƒ¼ãƒ³ã®çš„ä¸­\n",
        "        'FP' (False Positive): å®Ÿéš›å¤±æ•— / äºˆæ¸¬æˆåŠŸ (ç©ºæŒ¯ã‚Š) -> å¤±æ•—ã‚·ãƒ¼ãƒ³ã®èª¤èª\n",
        "        'FN' (False Negative): å®Ÿéš›æˆåŠŸ / äºˆæ¸¬å¤±æ•— (è¦‹é€ƒã—)\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    model.eval()\n",
        "    indices = []\n",
        "\n",
        "    print(f\"ðŸ”Ž {case_type} ã‚·ãƒ¼ãƒ³ã‚’æŽ¢ç´¢ä¸­... (Device: {device})\")\n",
        "\n",
        "    for i, data in enumerate(dataset):\n",
        "        x = data.x.to(device).to(torch.float)\n",
        "        edge_index = data.edge_index.to(device).to(torch.long)\n",
        "        pos = data.pos.to(device).to(torch.float)\n",
        "        vel = data.vel.to(device).to(torch.float)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(x, edge_index, pos=pos, vel=vel)\n",
        "            pred = out.argmax(dim=1).item()\n",
        "            actual = data.y.item()\n",
        "            prob = torch.exp(out[0][pred]).item() # äºˆæ¸¬ã—ãŸã‚¯ãƒ©ã‚¹ã®ç¢ºä¿¡åº¦\n",
        "\n",
        "            # æ¡ä»¶åˆ¤å®š\n",
        "            is_match = False\n",
        "            if case_type == 'TP' and pred == 1 and actual == 1: is_match = True\n",
        "            elif case_type == 'TN' and pred == 0 and actual == 0: is_match = True\n",
        "            elif case_type == 'FP' and pred == 1 and actual == 0: is_match = True\n",
        "            elif case_type == 'FN' and pred == 0 and actual == 1: is_match = True\n",
        "\n",
        "            if is_match:\n",
        "                indices.append((i, prob))\n",
        "\n",
        "    # ç¢ºä¿¡åº¦ï¼ˆç¢ºçŽ‡ï¼‰ãŒé«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ\n",
        "    indices = sorted(indices, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if not indices:\n",
        "        print(f\"âŒ {case_type} ã‚·ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "        return\n",
        "\n",
        "    print(f\" {len(indices)} ä»¶ã® {case_type} ã‚·ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚ä¸Šä½ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚\")\n",
        "\n",
        "    # Explainerã®è¨­å®šï¼ˆãƒ‡ãƒã‚¤ã‚¹å¯¾å¿œï¼‰\n",
        "    from torch_geometric.explain import Explainer, GNNExplainer\n",
        "    class ExplainerWrapper(torch.nn.Module):\n",
        "        def __init__(self, model):\n",
        "            super().__init__()\n",
        "            self.model = model\n",
        "        def forward(self, x, edge_index, **kwargs):\n",
        "            return self.model(x, edge_index, pos=kwargs.get('pos'), vel=kwargs.get('vel'))\n",
        "\n",
        "    explainer = Explainer(\n",
        "        model=ExplainerWrapper(model),\n",
        "        algorithm=GNNExplainer(epochs=200),\n",
        "        explanation_type='model',\n",
        "        node_mask_type='attributes',\n",
        "        edge_mask_type='object',\n",
        "        model_config=dict(\n",
        "            mode='multiclass_classification',\n",
        "            task_level='graph',\n",
        "            return_type='log_probs',\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    for i in range(min(num_samples, len(indices))):\n",
        "        idx, prob = indices[i]\n",
        "        data_single = dataset[idx].to(device)\n",
        "\n",
        "        explanation = explainer(\n",
        "            data_single.x,\n",
        "            data_single.edge_index,\n",
        "            pos=data_single.pos,\n",
        "            vel=data_single.vel\n",
        "        )\n",
        "\n",
        "        # ã‚±ãƒ¼ã‚¹ã«å¿œã˜ãŸã‚¿ã‚¤ãƒˆãƒ«ä½œæˆ\n",
        "        status_map = {\n",
        "            'TP': \"TRUE POSITIVE (AI Correct! - Success)\",\n",
        "            'TN': \"TRUE NEGATIVE (AI Correct! - Failure)\",\n",
        "            'FP': \"FALSE POSITIVE (AI Mistake! - Predicted Success but Failed)\",\n",
        "            'FN': \"FALSE NEGATIVE (AI Mistake! - Predicted Failure but Successed)\"\n",
        "        }\n",
        "        actual_label = \"Success\" if data_single.y.item() == 1 else \"Failure\"\n",
        "        pred_label = \"Success\" if case_type in ['TP', 'FP'] else \"Failure\"\n",
        "\n",
        "        title = f\"{status_map[case_type]}\\nActual: {actual_label}, Pred: {pred_label} (Prob: {prob:.2f})\"\n",
        "\n",
        "        visualize_explanation_on_pitch_fixed(explanation, data_single, title=title)\n",
        "        print(f\" Index {idx} ({case_type}) ã®å¯è¦–åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
        "\n",
        "# --- ä½¿ã„æ–¹ ---\n",
        "\n",
        "# 1. å¤±æ•—ã‚’å¤±æ•—ã¨æ­£ã—ãåˆ¤å®šã—ãŸã‚·ãƒ¼ãƒ³ï¼ˆãªãœå¤±æ•—ã™ã‚‹ã¨AIãŒåˆ¤æ–­ã—ãŸã‹ã®æ ¹æ‹ ï¼‰\n",
        "find_and_visualize_by_case(pignn_model, all_data_list, case_type='TN', num_samples=3)\n",
        "\n",
        "# 2. å¤±æ•—ã—ãŸã®ã«æˆåŠŸã¨èª¤èªã—ãŸã‚·ãƒ¼ãƒ³ï¼ˆAIãŒã©ã“ã«é¨™ã•ã‚ŒãŸã‹ã®æ ¹æ‹ ï¼‰\n",
        "find_and_visualize_by_case(pignn_model, all_data_list, case_type='FP', num_samples=3)"
      ],
      "metadata": {
        "id": "tU1BbgiKizNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# ==========================================\n",
        "# 1. æç”»é–¢æ•°ã®å†å®šç¾©ï¼ˆå¼•æ•°ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆï¼‰\n",
        "# ==========================================\n",
        "def visualize_pignn_tactical_analysis(model, data_item, device, title=\"Analysis\"):\n",
        "    model.eval()\n",
        "    data_item = data_item.to(device)\n",
        "\n",
        "    # --- æŽ¨è«–ã¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æŠ½å‡ºï¼ˆå®Œå…¨æŒ‡å®šç‰ˆï¼‰ ---\n",
        "    with torch.no_grad():\n",
        "        out, (edge_index, att_weights) = model(\n",
        "            data=data_item,\n",
        "            x=data_item.x,\n",
        "            edge_index=data_item.edge_index,\n",
        "            batch=data_item.batch,\n",
        "            pos=data_item.pos,\n",
        "            vel=data_item.vel,\n",
        "            return_attention=True\n",
        "        )\n",
        "        prob = torch.softmax(out, dim=1)[0, 1].item()\n",
        "        pred = out.argmax(dim=1).item()\n",
        "        label = data_item.y.item()\n",
        "\n",
        "    # --- åº§æ¨™ã¨é€Ÿåº¦ã®å¾©å…ƒ ---\n",
        "    pos = data_item.pos.cpu().numpy()\n",
        "    vel = data_item.vel.cpu().numpy()\n",
        "    # åº§æ¨™ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´ï¼ˆãƒ”ãƒƒãƒã‚µã‚¤ã‚ºã«åˆã‚ã›ã‚‹ï¼‰\n",
        "    pos_plot = np.zeros_like(pos)\n",
        "    pos_plot[:, 0] = pos[:, 0] * 52.5\n",
        "    pos_plot[:, 1] = pos[:, 1] * 34.0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # ã‚µãƒƒã‚«ãƒ¼ãƒ”ãƒƒãƒã®æç”»\n",
        "    ax.set_facecolor('#2e7d32')\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=True, color='#388e3c', zorder=0))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=False, color='white', lw=2, zorder=1))\n",
        "    ax.plot([0, 0], [-34, 34], color='white', lw=2, zorder=1)\n",
        "\n",
        "    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã®æç”»\n",
        "    att_weights = att_weights.cpu().numpy().flatten()\n",
        "    edge_index = edge_index.cpu().numpy()\n",
        "    if len(att_weights) > 0:\n",
        "        threshold = np.percentile(att_weights, 98) # ä¸Šä½2%ã‚’å¯è¦–åŒ–\n",
        "        for i in range(edge_index.shape[1]):\n",
        "            if att_weights[i] > threshold:\n",
        "                src, dst = edge_index[0, i], edge_index[1, i]\n",
        "                alpha_val = min((att_weights[i] / att_weights.max()), 1.0)\n",
        "                ax.plot([pos_plot[src, 0], pos_plot[dst, 0]],\n",
        "                        [pos_plot[src, 1], pos_plot[dst, 1]],\n",
        "                        color='#FFFF00', alpha=alpha_val, lw=2, zorder=2)\n",
        "\n",
        "    # é¸æ‰‹ãƒŽãƒ¼ãƒ‰ã®æç”»\n",
        "    team_ids = data_item.x[:, 6].cpu().numpy()\n",
        "    for i in range(len(pos)):\n",
        "        if team_ids[i] == 2.0: # ãƒœãƒ¼ãƒ«\n",
        "            color, size, marker = 'gold', 400, '*'\n",
        "        elif team_ids[i] == 0.0: # æ”»æ’ƒ\n",
        "            color, size, marker = '#0288d1', 200, 'o'\n",
        "        else: # å®ˆå‚™\n",
        "            color, size, marker = '#d32f2f', 200, 'o'\n",
        "\n",
        "        ax.scatter(pos_plot[i, 0], pos_plot[i, 1], c=color, s=size, marker=marker,\n",
        "                   edgecolors='white', linewidth=1, zorder=10)\n",
        "\n",
        "    ax.set_title(f\"{title}\\nSuccess Prob: {prob:.1%}\", fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 2. ãƒ‡ãƒ¼ã‚¿ã®ã€Œå®Œå…¨CPUåŒ–ã€ã¨å†æ§‹æˆ\n",
        "# ==========================================\n",
        "print(\" ãƒ‡ãƒ¼ã‚¿ã®ãƒ‡ãƒã‚¤ã‚¹æ•´åˆæ€§ã‚’ä¿®æ­£ä¸­...\")\n",
        "if 'all_data_list' in locals():\n",
        "    all_data_list_cpu = [d.to('cpu') for d in all_data_list]\n",
        "    cv_test_loader = DataLoader(all_data_list_cpu, batch_size=1, shuffle=False)\n",
        "    print(\" å…¨ãƒ‡ãƒ¼ã‚¿ã‚’CPUã«é…ç½®ã—ã€cv_test_loaderã‚’å†æ§‹æˆã—ã¾ã—ãŸã€‚\")\n",
        "else:\n",
        "    print(\" all_data_listãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. æˆ¦ç•¥çš„æ¯”è¼ƒé–¢æ•°ã®å®šç¾©ï¼ˆãƒžã‚¤ãƒ«ãƒ‰ãªé–¾å€¤è¨­å®šï¼‰\n",
        "# ==========================================\n",
        "def run_strategic_comparison(model, loader, device):\n",
        "    model.eval()\n",
        "    all_results = []\n",
        "\n",
        "    print(\" æ¯”è¼ƒã‚·ãƒ¼ãƒ³ã‚’æŽ¢ç´¢ä¸­... (Device: {})\".format(device))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to('cpu')\n",
        "            data = preprocess_batch(data, device)\n",
        "\n",
        "            if not hasattr(data, 'batch') or data.batch is None:\n",
        "                data.batch = torch.zeros(data.x.size(0), dtype=torch.long, device=device)\n",
        "\n",
        "            # ãƒ¢ãƒ‡ãƒ«æŽ¨è«–\n",
        "            out, (edge_idx, att) = model(\n",
        "                data=data, x=data.x, edge_index=data.edge_index,\n",
        "                batch=data.batch, pos=data.pos, vel=data.vel, return_attention=True\n",
        "            )\n",
        "            prob = torch.softmax(out, dim=1)[0, 1].item()\n",
        "\n",
        "            atk_mask = (data.x[:, 6] == 0.0)\n",
        "            avg_vx = data.vel[atk_mask, 0].mean().item() if atk_mask.any() else 0.0\n",
        "\n",
        "            all_results.append({\n",
        "                'data': data.cpu(),\n",
        "                'avg_vx': avg_vx,\n",
        "                'prob': prob,\n",
        "                'label': data.y.item()\n",
        "            })\n",
        "\n",
        "    if not all_results:\n",
        "        print(\" ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "        return\n",
        "\n",
        "    # æ¡ä»¶A: é€Ÿåº¦ã¯ã‚ã‚‹ãŒã€AIãŒå¤±æ•—ã¨äºˆæ¸¬ï¼ˆæˆ¦è¡“çš„å¤±æ•—ï¼‰\n",
        "    # é–¾å€¤ã‚’0.2ä»¥ä¸Š / 0.5ä»¥ä¸‹ã«ãƒžã‚¤ãƒ«ãƒ‰åŒ–\n",
        "    candidates_a = [res for res in all_results if res['avg_vx'] > 0.10 and res['prob'] < 0.5]\n",
        "    case_a = sorted(candidates_a, key=lambda x: x['prob'])[0] if candidates_a else None\n",
        "\n",
        "    # æ¡ä»¶B: é€Ÿåº¦ã¯å¹³å‡¡ã ãŒã€AIãŒæˆåŠŸã¨äºˆæ¸¬ï¼ˆæˆ¦è¡“çš„æˆåŠŸï¼‰\n",
        "    # é–¾å€¤ã‚’0.2ä»¥ä¸‹ / 0.6ä»¥ä¸Šã«ãƒžã‚¤ãƒ«ãƒ‰åŒ–\n",
        "    candidates_b = [res for res in all_results if res['avg_vx'] < 0.2 and res['prob'] > 0.6]\n",
        "    case_b = sorted(candidates_b, key=lambda x: x['prob'], reverse=True)[0] if candidates_b else None\n",
        "\n",
        "    # æç”»\n",
        "    if case_a:\n",
        "        print(f\" æ¡ä»¶Aç™ºè¦‹: Avg_Vx={case_a['avg_vx']:.3f}, Prob={case_a['prob']:.3f}\")\n",
        "        visualize_pignn_tactical_analysis(model, case_a['data'], device, title=\"Condition A: High Physics but Tactical Failure\")\n",
        "    else:\n",
        "        print(\" æ¡ä»¶AãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "\n",
        "    if case_b:\n",
        "        print(f\" æ¡ä»¶Bç™ºè¦‹: Avg_Vx={case_b['avg_vx']:.3f}, Prob={case_b['prob']:.3f}\")\n",
        "        visualize_pignn_tactical_analysis(model, case_b['data'], device, title=\"Condition B: Normal Physics but Tactical Success\")\n",
        "    else:\n",
        "        print(\" æ¡ä»¶BãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³\n",
        "# ==========================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "pignn_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "pignn_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/alpha_100_0_take3/pignn_testmatch_1.pth\"\n",
        "\n",
        "if os.path.exists(pignn_path):\n",
        "    pignn_model.load_state_dict(torch.load(pignn_path, map_location=device), strict=False)\n",
        "    print(\" PIGNNé‡ã¿ãƒ­ãƒ¼ãƒ‰å®Œäº†ã€‚è§£æžã‚’é–‹å§‹ã—ã¾ã™ã€‚\")\n",
        "    run_strategic_comparison(pignn_model, cv_test_loader, device)\n",
        "else:\n",
        "    print(f\" ãƒ•ã‚¡ã‚¤ãƒ«æœªæ¤œå‡º: {pignn_path}\")"
      ],
      "metadata": {
        "id": "3YSJVryuLAyQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}