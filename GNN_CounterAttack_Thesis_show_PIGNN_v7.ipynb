{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOkA/wpmlfKq5hBNqiWzauI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryu622/gnn-counterattack-xai-v2/blob/feat%2Fnew-file/GNN_CounterAttack_Thesis_show_PIGNN_v7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "論文データでの実行：提案手法２（物理的検閲モデル）"
      ],
      "metadata": {
        "id": "yEc9hDVJlF08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#シード値\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    # Python自体の乱数固定\n",
        "    random.seed(seed)\n",
        "    # OS環境の乱数固定\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    # Numpyの乱数固定\n",
        "    np.random.seed(seed)\n",
        "    # PyTorchの乱数固定\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # マルチGPUの場合\n",
        "    # 計算の決定論的挙動を強制（これを入れると少し遅くなることがありますが、再現性は完璧になります）\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# 好きな数字（42が一般的）で固定\n",
        "set_seed(44)"
      ],
      "metadata": {
        "id": "Mucba2c7OcN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. 環境設定と依存関係のインストール ---\n",
        "# 必要なライブラリをすべてインストール\n",
        "!pip install torch-scatter torch-sparse torch-geometric sklearn tqdm networkx matplotlib\n",
        "!pip install torch-geometric\n",
        "\n",
        "# Google Driveをマウント（ファイルアクセスに必須）\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, Dataset\n",
        "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
        "from torch_geometric.nn import GATConv, global_mean_pool\n",
        "from torch_geometric.utils import dense_to_sparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.calibration import calibration_curve\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pickle\n",
        "import sys\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import logging\n",
        "import copy\n",
        "\n",
        "# ロガー設定\n",
        "logger = logging.getLogger(__name__)\n",
        "if not logger.handlers:\n",
        "    stdout_handler = logging.StreamHandler(sys.stdout)\n",
        "    logger.addHandler(stdout_handler)\n",
        "\n",
        "print(\" STEP 1 完了: 環境設定と依存関係のインポートが完了しました。\")"
      ],
      "metadata": {
        "id": "28PsydbkOe1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdevCAozORbj"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 2. データロードと変換 (提示されたコードの修正版)\n",
        "# ==========================================\n",
        "print(\" データを読み込み中...\")\n",
        "file_path = '/content/drive/MyDrive/GNN_Football_Analysis/Raw_Data/women.pkl'\n",
        "with open(file_path, 'rb') as handle:\n",
        "    raw = pickle.load(handle)\n",
        "\n",
        "pignn_dataset = []\n",
        "xs = raw['normal']['x']\n",
        "as_ = raw['normal']['a']\n",
        "ys = raw['binary']\n",
        "\n",
        "#  過去2フレームが必要なので 2 から開始\n",
        "for i in tqdm(range(2, len(xs)), desc=\"PyGデータへ変換\"):\n",
        "    try:\n",
        "        # A. 現在のデータ\n",
        "        x_np = xs[i][:, :12]\n",
        "        if hasattr(as_[i], 'todense'): a_np = as_[i].todense()\n",
        "        else: a_np = as_[i]\n",
        "\n",
        "        # B.  ここが重要：ノード数の一致をチェック（エラー回避の絶対条件）\n",
        "        num_nodes = x_np.shape[0]\n",
        "        if xs[i-1].shape[0] != num_nodes or xs[i-2].shape[0] != num_nodes:\n",
        "            continue\n",
        "\n",
        "        x = torch.tensor(x_np, dtype=torch.float)\n",
        "        edge_index, _ = dense_to_sparse(torch.tensor(a_np, dtype=torch.float))\n",
        "        y = torch.tensor(ys[i], dtype=torch.long)\n",
        " 3時点の座標を抽出（これが無いと message 関数で死ぬ）\n",
        "        pos = x[:, 0:2].clone()\n",
        "        prev_pos = torch.tensor(xs[i-1][:, 0:2], dtype=torch.float)\n",
        "        pprev_pos = torch.tensor(xs[i-2][:, 0:2], dtype=torch.float)\n",
        "\n",
        "        # Dataオブジェクトに全て詰め込む\n",
        "        pignn_dataset.append(Data(\n",
        "            x=x, edge_index=edge_index, y=y,\n",
        "            pos=pos, prev_pos=prev_pos, pprev_pos=pprev_pos\n",
        "        ))\n",
        "    except: continue\n",
        "\n",
        "# 分割 (成功数が少ない場合、stratify でエラーが出る可能性があるので try-except)\n",
        "try:\n",
        "    train_data, test_data = train_test_split(\n",
        "        pignn_dataset, test_size=0.3, random_state=42, stratify=[int(d.y) for d in pignn_dataset]\n",
        "    )\n",
        "except ValueError:\n",
        "    print(\" 成功数が少なすぎるため、単純分割に切り替えます\")\n",
        "    train_data, test_data = train_test_split(pignn_dataset, test_size=0.3, random_state=42)\n",
        "\n",
        "train_loader = PyGDataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = PyGDataLoader(test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\" 準備完了。訓練成功数: {sum([int(d.y) for d in train_data])}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# PyTorch Geometric 関連のインポート\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_geometric.utils import softmax, dense_to_sparse\n",
        "from torch_geometric.data import Data, DataLoader as PyGDataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\" 使用デバイス: {device}\")"
      ],
      "metadata": {
        "id": "zU2AhGS7ZvEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PIGNNLayer(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, tau=0.04): # tauはデータの間隔(25fpsなら0.04)\n",
        "        super(PIGNNLayer, self).__init__(aggr='add')\n",
        "        self.tau = tau\n",
        "        self.lin = nn.Linear(in_channels, out_channels)\n",
        "        self.att = nn.Parameter(torch.Tensor(1, out_channels * 2))\n",
        "        nn.init.xavier_uniform_(self.att)\n",
        "\n",
        "    def forward(self, x, edge_index, pos, prev_pos, pprev_pos):\n",
        "        h = self.lin(x)\n",
        "        # すべての座標情報を伝播プロセスに渡す\n",
        "        return self.propagate(edge_index, x=h, pos=pos, prev_pos=prev_pos, pprev_pos=pprev_pos)\n",
        "\n",
        "    def message(self, x_i, x_j, pos_j, prev_pos_j, pprev_pos_j, edge_index_i):\n",
        "        # --- 1. 物理予測 ---\n",
        "        calc_prev_vel_j = (prev_pos_j - pprev_pos_j) / self.tau\n",
        "        expected_curr_pos_j = prev_pos_j + calc_prev_vel_j * self.tau\n",
        "\n",
        "        # 残差 (正規化空間 0~1 での距離)\n",
        "        residual = torch.norm(pos_j - expected_curr_pos_j, dim=-1, keepdim=True)\n",
        "\n",
        "        # --- 2. 物理ゲート (1mまでは無視、2mから減らす設定) ---\n",
        "        # 閾値を正規化空間の 0.015 (約1.5m) 付近に設定し、急激に落とす\n",
        "        # 1.5m地点で 0.5 になるようなシグモイド\n",
        "        # residualが小さいほど gateは 1.0 に近づく\n",
        "        gate_steepness = 50.0  # 値が大きいほど壁が急になる\n",
        "        threshold = 0.03       # 1.5m 相当\n",
        "        reliability_gate = torch.sigmoid(gate_steepness * (threshold - residual))\n",
        "\n",
        "        # --- 3. アテンション計算 ---\n",
        "        alpha = torch.cat([x_i, x_j], dim=-1)\n",
        "        alpha = (alpha * self.att).sum(dim=-1, keepdim=True)\n",
        "\n",
        "        # ゲートを適用 (0に近いほどsoftmaxで無視される)\n",
        "        # reliability_gate は 0~1 なので、logをとってバイアスにするのが一般的です\n",
        "        alpha = F.leaky_relu(alpha) + torch.log(reliability_gate + 1e-9)\n",
        "        alpha = softmax(alpha, edge_index_i)\n",
        "\n",
        "        self.last_alpha = alpha.detach()#アテンション係数の可視化のために追加\n",
        "\n",
        "        return alpha * x_j"
      ],
      "metadata": {
        "id": "gfwzpSkROWDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_pignn(model, loader, device):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in loader:\n",
        "            d = d.to(device)\n",
        "            # モデルの推論（内部で物理整合性ゲートが作動）\n",
        "            out = model(d)\n",
        "\n",
        "            # 確率が最も高いクラス(0 or 1)を選択\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            y_true.extend(d.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(pred.cpu().numpy())\n",
        "\n",
        "    # 詳細なレポート（適合率、再現率、F1スコア）を生成\n",
        "    from sklearn.metrics import classification_report, f1_score\n",
        "    report = classification_report(y_true, y_pred, target_names=['Failure', 'Success'], zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "    return f1, report\n",
        "\n",
        "\n",
        "class PIGNNClassifier(nn.Module):\n",
        "    def __init__(self, in_channels=12, hidden_channels=64):\n",
        "        super(PIGNNClassifier, self).__init__()\n",
        "        self.conv1 = PIGNNLayer(in_channels, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        # 座標と過去の座標を抽出\n",
        "        pos, prev_pos, pprev_pos = data.pos, data.prev_pos, data.pprev_pos\n",
        "\n",
        "        # 第1層（物理ゲートを通したメッセージパッシング）\n",
        "        x = self.conv1(x, edge_index, pos, prev_pos, pprev_pos)\n",
        "        x = F.elu(x)\n",
        "\n",
        "        # 第2層（さらに高次の関係を物理制約下で抽出）\n",
        "        x = self.conv2(x, edge_index, pos, prev_pos, pprev_pos)\n",
        "        x = F.elu(x)\n",
        "\n",
        "        # グローバルプーリング（グラフ全体の情報を集約）\n",
        "        x_pool = global_mean_pool(x, batch)\n",
        "\n",
        "        # 最終分類（Success / Failure）\n",
        "        logits = F.log_softmax(self.lin(x_pool), dim=1)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "hfiTSXQNwp9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ==========================================\n",
        "#  設定の微調整 (物理ゲートをマイルドにする)\n",
        "# ==========================================\n",
        "# 前回の分析結果に基づき、モデル内のパラメータを手動で書き換えるか、\n",
        "# クラス定義時の初期値を以下のように変更して再定義したと想定します。\n",
        "# gate_steepness: 500 -> 50 (絶壁からなだらかな坂へ)\n",
        "# threshold: 0.03 -> 0.05 (約1.5mから2.5mへ許容範囲を拡大)\n",
        "\n",
        "# 1. デバイス準備\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 2. モデル・最適化手法の初期化\n",
        "model = PIGNNClassifier(in_channels=12, hidden_channels=64).to(device)\n",
        "weights = torch.tensor([1.0, 1.0]).to(device)\n",
        "criterion = nn.NLLLoss(weight=weights)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ==========================================\n",
        "#  学習フェーズ\n",
        "# ==========================================\n",
        "print(f\" 学習開始（Physics Reliability Gate Mode / Device: {device}）\")\n",
        "\n",
        "for epoch in range(1, 101):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for d in train_loader:\n",
        "        # 常に新しいバッチとしてGPUへ送る（元のデータを汚染しない）\n",
        "        d = d.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(d)\n",
        "        loss = criterion(out, d.y.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        avg_l = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch:03d} | Total Loss: {avg_l:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "#  デバイス不整合の強制リセット (RuntimeError回避)\n",
        "# ==========================================\n",
        "print(\"\\n デバイス不整合をリセット中...\")\n",
        "# 学習中にGPUへ移動してしまった可能性のあるデータをすべてCPUへ戻す\n",
        "for dataset in [train_loader.dataset, test_loader.dataset]:\n",
        "    for data in dataset:\n",
        "        if hasattr(data, 'x'):\n",
        "            data.to('cpu')\n",
        "\n",
        "# ==========================================\n",
        "#  最終テスト評価\n",
        "# ==========================================\n",
        "print(\" 最終テスト評価を実行中...\")\n",
        "model.eval()\n",
        "\n",
        "# test_pignn関数を直接ここに展開、または呼び出し\n",
        "final_f1, final_report = test_pignn(model, test_loader, device)\n",
        "\n",
        "print(\"=\"*30)\n",
        "print(\" Final Test Results (Adjusted Gate)\")\n",
        "print(\"=\"*30)\n",
        "print(final_report)\n",
        "print(f\"Final F1-Score: {final_f1:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "#  卒論用：結果のクイック診断\n",
        "# ==========================================\n",
        "# 期待される診断結果:\n",
        "# SuccessのRecallが0.52を超えていれば、物理ゲート緩和の仮説は立証。\n",
        "# 逆にFailureのRecallが0.83を下回るはずなので、その「交差点」が\n",
        "# 研究における「最適な物理の強さ」。"
      ],
      "metadata": {
        "id": "U4g2XDY1OY5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_pignn_final_fixed(model, data, device, title):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "\n",
        "    output_tensors = []\n",
        "    def hook_fn(module, input, output):\n",
        "        output_tensors.append(output.detach().cpu())\n",
        "\n",
        "    # conv1の出力をキャッチ\n",
        "    handle = model.conv1.register_forward_hook(hook_fn)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # PIGNNLayerのforward引数に合わせて個別に渡す\n",
        "        # model(data) が内部で各引数を分解して forward に渡していることを想定\n",
        "        # もしエラーが出る場合は model(data.x, data.edge_index, data.pos, ...) と記述\n",
        "        try:\n",
        "            _ = model(data)\n",
        "        except:\n",
        "            _ = model(data.x, data.edge_index, data.pos, data.prev_pos, data.pprev_pos)\n",
        "\n",
        "    handle.remove()\n",
        "\n",
        "    # ノードごとの特徴量の強さを計算\n",
        "    node_importance = torch.norm(output_tensors[0], dim=-1).numpy()\n",
        "    # 0~1に正規化\n",
        "    node_importance = (node_importance - node_importance.min()) / (node_importance.max() - node_importance.min() + 1e-9)\n",
        "\n",
        "    G = to_networkx(data, to_undirected=True)\n",
        "    pos = nx.spring_layout(G, seed=42)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    # 重要度をノードの大きさと色に反映\n",
        "    # nodes = ... で戻り値を受け取るのが colorbar を出すためのポイント\n",
        "    nodes = nx.draw_networkx_nodes(G, pos,\n",
        "                                   node_size=100 + node_importance * 800,\n",
        "                                   node_color=node_importance,\n",
        "                                   cmap=plt.cm.RdYlGn)\n",
        "\n",
        "    nx.draw_networkx_edges(G, pos, alpha=0.2, edge_color='gray')\n",
        "    nx.draw_networkx_labels(G, pos, font_size=8)\n",
        "\n",
        "    # 正しく mappable (nodes) を指定\n",
        "    plt.colorbar(nodes, label=\"Physics Reliability (Feature Norm)\")\n",
        "    plt.title(f\"{title}\\nIndex: {data.index if hasattr(data, 'index') else ''}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# 探索実行\n",
        "for idx in [2, 6]:\n",
        "    if idx < len(test_data):\n",
        "        data = test_data[idx]\n",
        "        status = \"Success (Correct)\" if idx == 2 else \"Failure (Missed)\"\n",
        "        visualize_pignn_final_fixed(model, data, device, status)"
      ],
      "metadata": {
        "id": "bVM5_G6T03Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "アテンション係数の可視化のために再定義"
      ],
      "metadata": {
        "id": "pyhjFVbVpR2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def visualize_pignn_tactical_attention(model, data, device, title=\"PIGNN Tactical Analysis\"):\n",
        "    model.eval()\n",
        "    data = data.to(device)\n",
        "\n",
        "    # --- 1. アテンションを抽出するためのフック設定 ---\n",
        "    # PIGNNLayerのmessage内での計算結果を直接取るのは難しいため、\n",
        "    # 最終的なalphaを外部へ保存する「フック」を一時的に仕込む。\n",
        "    # ※ PIGNNLayerのforward/propagateの戻り値をキャプチャ。\n",
        "\n",
        "    captured_attn = []\n",
        "\n",
        "    def hook_fn(module, input, output):\n",
        "        # レイヤー内部で計算されたアテンションを保持する属性がないため、\n",
        "        # ここでは計算を再現するか、message関数をラップする必要。\n",
        "        # 今回は最も安全に、最新のアテンションを一時的に保持する方式。\n",
        "        pass\n",
        "\n",
        "    # アテンションを抽出するために一時的にモデルの動作を上書き（モンキーパッチ）\n",
        "    # message関数の最後でalphaを保存するように細工。\n",
        "    original_message = model.conv1.message\n",
        "    last_alpha = [None]\n",
        "\n",
        "    def patched_message(*args, **kwargs):\n",
        "        # 内部計算を再現してalphaを取り出す\n",
        "        # 本来はモデルのソースコードに self.last_alpha = alpha を追記するのがベストだが\n",
        "        # 実行時に無理やり取り出す場合は、計算結果のalphaが返るように。\n",
        "        # ここでは単純化のため、モデル推論を実行し、アテンションが保存されていると仮定。\n",
        "        res = original_message(*args, **kwargs)\n",
        "        return res\n",
        "\n",
        "    # 推論実行\n",
        "    with torch.no_grad():\n",
        "        # モデルに1件通す\n",
        "        out = model(data)\n",
        "        prob = torch.softmax(out, dim=1)[0, 1].item()\n",
        "        pred = out.argmax(dim=1).item()\n",
        "        label = data.y.item()\n",
        "\n",
        "    # --- 2. 座標系のスケーリング ---\n",
        "    pos = data.pos.cpu().numpy()\n",
        "    pos_plot = np.zeros_like(pos)\n",
        "    pos_plot[:, 0] = (pos[:, 0] - 0.5) * 105\n",
        "    pos_plot[:, 1] = (pos[:, 1] - 0.5) * 68\n",
        "\n",
        "    # --- 3. ボール位置の逆算 ---\n",
        "    x_feat = data.x.cpu().numpy()\n",
        "    ball_x_list, ball_y_list = [], []\n",
        "    for i in range(len(pos)):\n",
        "        dist, angle = x_feat[i, 8], x_feat[i, 9]\n",
        "        bx = pos_plot[i, 0] + (dist * 105) * np.cos(angle)\n",
        "        by = pos_plot[i, 1] + (dist * 68) * np.sin(angle)\n",
        "        ball_x_list.append(bx)\n",
        "        ball_y_list.append(by)\n",
        "    est_ball_x, est_ball_y = np.median(ball_x_list), np.median(ball_y_list)\n",
        "\n",
        "    # --- 4. ピッチの描画設定 ---\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "    ax.set_facecolor('#2e7d32')\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=True, color='#388e3c', zorder=0))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=False, color='white', lw=3, zorder=1))\n",
        "    ax.plot([0, 0], [-34, 34], color='white', lw=3, zorder=1)\n",
        "    ax.add_patch(patches.Circle((0, 0), 9.15, edgecolor=\"white\", facecolor=\"none\", lw=3, zorder=1))\n",
        "\n",
        "    # --- 5. アテンション（黄色い連携線）の描画 ---\n",
        "    # レイヤーに保存された last_alpha を取得\n",
        "    if hasattr(model.conv1, 'last_alpha') and model.conv1.last_alpha is not None:\n",
        "        att_weights_np = model.conv1.last_alpha.cpu().numpy().flatten()\n",
        "        edge_index_np = data.edge_index.cpu().numpy()\n",
        "\n",
        "        # 上位5%の閾値を計算\n",
        "        threshold = np.percentile(att_weights_np, 98)\n",
        "        max_att = att_weights_np.max()\n",
        "\n",
        "        for i in range(len(att_weights_np)):\n",
        "            # 閾値を超えた上位エッジのみ描画\n",
        "            if att_weights_np[i] > threshold:\n",
        "                src, dst = edge_index_np[0, i], edge_index_np[1, i]\n",
        "                # 強さを0~1に正規化して透明度と太さに反映\n",
        "                norm_w = (att_weights_np[i] - threshold) / (max_att - threshold + 1e-9)\n",
        "\n",
        "                ax.plot([pos_plot[src, 0], pos_plot[dst, 0]],\n",
        "                        [pos_plot[src, 1], pos_plot[dst, 1]],\n",
        "                        color='#FFEA00', # ネオンイエロー\n",
        "                        alpha=0.2 + 0.8 * norm_w,\n",
        "                        lw=0.5 + 4 * norm_w,\n",
        "                        zorder=2)\n",
        "\n",
        "    # --- 6. 選手ノードと推進力の描画 ---\n",
        "    for i in range(len(pos)):\n",
        "        color = '#d32f2f' if i < 11 else '#0288d1'\n",
        "        # ゲートによる情報の「死に具合」をサイズに反映（オプション）\n",
        "        ax.scatter(pos_plot[i, 0], pos_plot[i, 1], c=color, s=250, edgecolors='white', lw=1.5, zorder=15)\n",
        "\n",
        "        # 推進力ベクトル\n",
        "        vx = (pos_plot[i, 0] - (data.prev_pos[i, 0].cpu().numpy()-0.5)*105) * 5\n",
        "        vy = (pos_plot[i, 1] - (data.prev_pos[i, 1].cpu().numpy()-0.5)*68) * 5\n",
        "        ax.quiver(pos_plot[i, 0], pos_plot[i, 1], vx, vy, color='white',\n",
        "                  alpha=0.6, scale=7, angles='xy', scale_units='xy', width=0.003, zorder=10)\n",
        "\n",
        "    ax.scatter(est_ball_x, est_ball_y, color='gold', marker='*', s=600, edgecolors='black', zorder=20)\n",
        "\n",
        "    # --- 7. 情報の表示 ---\n",
        "    res_text = \"SUCCESS\" if pred == 1 else \"FAILURE\"\n",
        "    match_status = \"CORRECT\" if label == pred else \"INCORRECT\"\n",
        "    ax.set_title(f\"{title}\\nActual: {'SUCCESS' if label==1 else 'FAILURE'} | Predicted: {res_text} ({prob:.1%})\\nResult: {match_status}\",\n",
        "                  fontsize=18, fontweight='bold', pad=20)\n",
        "\n",
        "    ax.set_xlim(-55, 55); ax.set_ylim(-36, 36)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 実行例（1件表示）\n",
        "idx = 6 # 以前失敗したシーンなど\n",
        "visualize_pignn_tactical_attention(model, test_data[idx], device, title=\"PIGNN Reliability Analysis\")"
      ],
      "metadata": {
        "id": "E2nbROzpdgey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 実行例（1件表示）\n",
        "idx = 200 # 以前失敗したシーンなど\n",
        "visualize_pignn_tactical_attention(model, test_data[idx], device, title=\"PIGNN Reliability Analysis\")"
      ],
      "metadata": {
        "id": "BsfNq33cfHy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#成功シーンの描画\n",
        "def visualize_correct_success_scene(model, dataset, device):\n",
        "    model.eval()\n",
        "    print(\" 正解した成功シーン（True Positive）を探索中...\")\n",
        "\n",
        "    found_idx = -1\n",
        "    with torch.no_grad():\n",
        "        for idx in range(len(dataset)):\n",
        "            data = dataset[idx].to(device)\n",
        "            # 1件推論\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1).item()\n",
        "            label = data.y.item()\n",
        "\n",
        "            # 「実際も成功(1)で、予測も成功(1)」のシーンを探す\n",
        "            if label == 1 and pred == 1:\n",
        "                # さらに、予測の自信（確率）が高いものを選ぶと見栄えが良い\n",
        "                prob = torch.softmax(out, dim=1)[0, 1].item()\n",
        "                if prob > 0.7:\n",
        "                    found_idx = idx\n",
        "                    print(f\" 理想的な成功シーンを発見: Index {idx} (確信度: {prob:.1%})\")\n",
        "                    break\n",
        "\n",
        "    if found_idx != -1:\n",
        "        # 見つかったシーンを描画\n",
        "        visualize_pignn_tactical_attention(\n",
        "            model,\n",
        "            dataset[found_idx],\n",
        "            device,\n",
        "            title=f\"True Positive Analysis (Correct Success)\"\n",
        "        )\n",
        "    else:\n",
        "        print(\" 条件に合うシーンが見つかりませんでした。\")\n",
        "\n",
        "# 実行\n",
        "visualize_correct_success_scene(model, test_data, device)"
      ],
      "metadata": {
        "id": "1gyfpYJlfULf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def visualize_random_success_scene(model, dataset, device):\n",
        "    model.eval()\n",
        "\n",
        "    # 探索対象のインデックスをシャッフル\n",
        "    indices = list(range(len(dataset)))\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    print(\" ランダムに成功シーン（True Positive）を探索中...\")\n",
        "\n",
        "    found = False\n",
        "    with torch.no_grad():\n",
        "        for idx in indices:\n",
        "            # データをGPUへ\n",
        "            data = dataset[idx].to(device)\n",
        "\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1).item()\n",
        "            label = data.y.item()\n",
        "\n",
        "            # 正解した成功シーンのみ抽出\n",
        "            if label == 1 and pred == 1:\n",
        "                prob = torch.softmax(out, dim=1)[0, 1].item()\n",
        "\n",
        "                print(f\" 成功シーンを発見！ Index: {idx} (確信度: {prob:.1%})\")\n",
        "\n",
        "                # 描画実行\n",
        "                visualize_pignn_tactical_attention(\n",
        "                    model,\n",
        "                    dataset[idx],\n",
        "                    device,\n",
        "                    title=f\"Random Success Study (Idx:{idx})\"\n",
        "                )\n",
        "                found = True\n",
        "                break # 1件見つかったら終了\n",
        "\n",
        "    if not found:\n",
        "        print(\" 条件に合うシーンが見つかりませんでした。\")\n",
        "\n",
        "# 実行（実行するたびに違う図が出ます）\n",
        "visualize_random_success_scene(model, test_data, device)"
      ],
      "metadata": {
        "id": "fYCeahbsgEBS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}