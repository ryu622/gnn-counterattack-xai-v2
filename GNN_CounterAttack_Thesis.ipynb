{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMtRXg1hLUbk67J8CRGpEtA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryu622/gnn-counterattack-xai-v2/blob/fix%2Ffile-clean/GNN_CounterAttack_Thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GNNのカウンターアタックの論文のデータを用いて、CrystalConvではなくGATを用いてやってみる。"
      ],
      "metadata": {
        "id": "gk8vvjrhrL4A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLpA8SqfpY5l"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Google Driveをマウント\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "# ファイルのパスを定義 (RAW_DATAフォルダ内の women.pkl)\n",
        "file_path = '/content/drive/MyDrive/GNN_Football_Analysis/Raw_Data/women.pkl'\n",
        "\n",
        "print(f\"データファイルパス: {file_path}\")\n",
        "\n",
        "# ファイルが存在するか確認（任意）\n",
        "if os.path.exists(file_path):\n",
        "    print(\"ファイルは存在します。ロードに進めます。\")\n",
        "else:\n",
        "    print(\"エラー: ファイルが見つかりません。パスを確認してください。\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipywidgets matplotlib pandas scikit-learn spektral tensorflow progressbar ml-insights"
      ],
      "metadata": {
        "id": "3JBzoCzEsf6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch_geometric numpy pandas scikit-learn"
      ],
      "metadata": {
        "id": "_fgyzEe7DkDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import Checkbox, Dropdown, Accordion, VBox\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.calibration import calibration_curve\n",
        "from spektral.data import Dataset, Graph, DisjointLoader\n",
        "from spektral.layers import CrystalConv, GlobalAvgPool\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import AUC\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import random\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import requests\n",
        "import progressbar\n",
        "from os.path import isfile\n",
        "import time\n",
        "\n",
        "# Setting up the logger\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.DEBUG)\n",
        "stdout_handler = logging.StreamHandler(sys.stdout)\n",
        "logger.addHandler(stdout_handler)"
      ],
      "metadata": {
        "id": "UWj_poEQrneo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# PyGのインポートも確認\n",
        "from torch_geometric.nn import GATConv, global_mean_pool"
      ],
      "metadata": {
        "id": "T7VhW0NeGGhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データのロード\n",
        "logger.info(f\"Opening {file_path}...\")\n",
        "try:\n",
        "    with open(file_path, 'rb') as handle:\n",
        "        og_data = pickle.load(handle)\n",
        "\n",
        "    logger.info(\"データが正常にロードされ、変数 'og_data' に格納されました。\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"ロードされたデータのキー: {og_data.keys()}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    logger.error(f\"エラー: ファイル '{file_path}' が見つかりません。パスを確認してください。\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"データのロード中にエラーが発生しました: {e}\")"
      ],
      "metadata": {
        "id": "p51dK3EdtG2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# SpektralとTensorFlowは既にインストール済みと仮定\n",
        "\n",
        "# ipywidgetsのモックオブジェクトを定義（ウィジェットを使わないための代替手段）\n",
        "class MockDropdown:\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "class MockVBox:\n",
        "    def __init__(self, children):\n",
        "        self.children = children\n",
        "class Checkbox:\n",
        "    def __init__(self, value, description):\n",
        "        self.value = value\n",
        "        self.description = description\n",
        "\n",
        "# 2.2 隣接行列の選択: 'normal' を選択\n",
        "adj_matrix_value = 'normal'\n",
        "adj_matrix = MockDropdown(adj_matrix_value)\n",
        "\n",
        "\n",
        "# 2.3 エッジ特徴量の選択: すべてにチェックを入れる\n",
        "edge_f_box_children = [\n",
        "    Checkbox(value=True, description='Player Distance'),\n",
        "    Checkbox(value=True, description='Speed Difference'),\n",
        "    Checkbox(value=True, description='Positional Sine angle'),\n",
        "    Checkbox(value=True, description='Positional Cosine angle'),\n",
        "    Checkbox(value=True, description='Velocity Sine angle'),\n",
        "    Checkbox(value=True, description='Velocity Cosine angle'),\n",
        "]\n",
        "edge_f_box = MockVBox(edge_f_box_children)\n",
        "\n",
        "\n",
        "# 2.4 ノード特徴量の選択: すべてにチェックを入れる\n",
        "node_f_box_children = [\n",
        "    Checkbox(value=True, description='x coordinate'),\n",
        "    Checkbox(value=True, description='y coordinate'),\n",
        "    Checkbox(value=True, description='vX'),\n",
        "    Checkbox(value=True, description='vY'),\n",
        "    Checkbox(value=True, description='Velocity'),\n",
        "    Checkbox(value=True, description='Velocity Angle'),\n",
        "    Checkbox(value=True, description='Distance to Goal'),\n",
        "    Checkbox(value=True, description='Angle with Goal'),\n",
        "    Checkbox(value=True, description='Distance to Ball'),\n",
        "    Checkbox(value=True, description='Angle with Ball'),\n",
        "    Checkbox(value=True, description='Attacking Team Flag'),\n",
        "    Checkbox(value=True, description='Potential Receiver'),\n",
        "]\n",
        "node_f_box = MockVBox(node_f_box_children)\n",
        "\n",
        "print(f\"隣接行列: '{adj_matrix.value}' に設定\")\n",
        "print(\"エッジ特徴量とノード特徴量はすべて選択されました。\")"
      ],
      "metadata": {
        "id": "aQDw9RzB-5HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from spektral.data import Dataset, Graph, DisjointLoader\n",
        "import tensorflow as tf\n",
        "\n",
        "# セクション 2.6: filter_features 関数の定義と実行 (修正済み)\n",
        "def filter_features(data, gender=None):\n",
        "    \"\"\"\n",
        "    データから選択されたノード特徴量とエッジ特徴量のみをフィルタリングする。\n",
        "    \"\"\"\n",
        "    # 選択された特徴量のインデックスを取得\n",
        "    edge_feature_idxs = [idx for idx, x in enumerate(edge_f_box.children) if x.value]\n",
        "    node_feature_idxs = [idx for idx, x in enumerate(node_f_box.children) if x.value]\n",
        "\n",
        "    # ノード特徴量の名前をグローバル変数として取得 (セクション 5 のために必要)\n",
        "    global node_features\n",
        "    node_features = [x.description for x in node_f_box.children if x.value]\n",
        "\n",
        "    if not any(edge_feature_idxs) and not any(node_feature_idxs):\n",
        "        print(\"\\nCannot have zero edge features and zero node features. Defaulting to the previous configuration.\")\n",
        "    else:\n",
        "        mat_type = adj_matrix.value\n",
        "\n",
        "        # データのフィルタリングを適用 (ここを修正: 'normal_e' -> 'e', 'normal_x' -> 'x')\n",
        "        # 隣接行列のデータセット内にある 'e' と 'x' にアクセス\n",
        "        data[mat_type]['e'] = [x[:, edge_feature_idxs] for x in data[mat_type]['e']]\n",
        "        data[mat_type]['x'] = [x[:, node_feature_idxs] for x in data[mat_type]['x']]\n",
        "\n",
        "    return data\n",
        "\n",
        "# データのフィルタリングを実行\n",
        "data = filter_features(og_data.copy())\n",
        "print(\"特徴量のフィルタリングが完了しました。\")\n",
        "\n",
        "\n",
        "# セクション 2.7: CounterDataset クラスの定義とデータセットのロード (修正済み)\n",
        "class CounterDataset(Dataset):\n",
        "    \"\"\"\n",
        "    SpektralのDatasetクラスを継承し、ノード特徴量、エッジ特徴量、隣接行列を含むGraphオブジェクトのリストを返す。\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        self.data = kwargs['data']\n",
        "        self.matrix_type = kwargs['matrix_type']\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def read(self):\n",
        "        logger.info(\"Loading Pass Dataset.\")\n",
        "        data_mat = self.data[self.matrix_type]\n",
        "\n",
        "        # Print Graph information (ここも修正: 'normal_x' -> 'x', 'normal_e' -> 'e', 'normal_a' -> 'a')\n",
        "        logger.info(\n",
        "            f\"\"\"node_features (x): {data_mat['x'][0].shape}\n",
        "             \\n adj_matrix (a): {data_mat['a'][0].shape}\n",
        "             \\n edge_features (e): {data_mat['e'][0].shape}\n",
        "             \"\"\"\n",
        "        )\n",
        "\n",
        "        # Graphオブジェクトを生成 (ここも修正: 'normal_x' -> 'x', 'normal_a' -> 'a', 'normal_e' -> 'e')\n",
        "        return [\n",
        "            Graph(x=x, a=a, e=e, y=y) for x, a, e, y in zip(\n",
        "                data_mat['x'], data_mat['a'], data_mat['e'], self.data['binary']\n",
        "            )\n",
        "        ]\n",
        "\n",
        "# データセットのロードとGraphオブジェクトへの変換を実行\n",
        "dataset = CounterDataset(data = data, matrix_type = adj_matrix.value)\n",
        "print(f\"データセットのロードと変換が完了しました。全サンプル数: {len(dataset)}\")"
      ],
      "metadata": {
        "id": "eicMQLAS_NTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "互換性の都合上Pytorchへ書き換え"
      ],
      "metadata": {
        "id": "uKTk0gszEq4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data, Dataset, DataLoader\n",
        "from torch_geometric.utils import dense_to_sparse # 隣接行列をedge_indexに変換\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 以前のデータロードで得た 'data' (フィルタリング済み) を使用します\n",
        "\n",
        "# PyGデータセットの定義 (修正版)\n",
        "\n",
        "class PyG_CounterDataset(Dataset):\n",
        "    def __init__(self, data, matrix_type, root=None, transform=None, pre_transform=None):\n",
        "        self.raw_data = data\n",
        "        self.matrix_type = matrix_type\n",
        "        self._data_list = self.process_data()\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "\n",
        "    def process_data(self):\n",
        "        data_mat = self.raw_data[self.matrix_type]\n",
        "        data_list = []\n",
        "\n",
        "        # NumPy配列のリストをPyG Dataオブジェクトのリストに変換\n",
        "        for x_np, a_np, e_np, y_np in tqdm(zip(\n",
        "            data_mat['x'], data_mat['a'], data_mat['e'], self.raw_data['binary']\n",
        "        ), total=len(data_mat['x']), desc=\"Converting to PyG Data\"):\n",
        "\n",
        "            try:\n",
        "                # 破損チェック 1: ノード特徴量が空ではないか確認\n",
        "                if x_np.shape[0] == 0:\n",
        "                    continue # ノードがなければスキップ\n",
        "\n",
        "                # 疎行列の修正: SciPyの疎行列を密行列に変換\n",
        "                if hasattr(a_np, 'todense'):\n",
        "                    a_np = a_np.todense()\n",
        "\n",
        "                # テンソルへの変換\n",
        "                x = torch.tensor(x_np, dtype=torch.float)\n",
        "                a = torch.tensor(a_np, dtype=torch.float)\n",
        "                e = torch.tensor(e_np, dtype=torch.float)\n",
        "                y = torch.tensor(y_np, dtype=torch.float).view(-1, 1)\n",
        "\n",
        "                # 隣接行列 (A) をエッジインデックス (edge_index) に変換\n",
        "                edge_index, _ = dense_to_sparse(a)\n",
        "\n",
        "                # 破損チェック 2: エッジがない場合はスキップ (GNN層は通常エッジを期待するため)\n",
        "                if edge_index.numel() == 0:\n",
        "                     continue\n",
        "\n",
        "                # エッジ特徴量の整合性チェック\n",
        "                if e.size(0) != edge_index.size(1):\n",
        "                    edge_attr = None\n",
        "                else:\n",
        "                    edge_attr = e\n",
        "\n",
        "                data_list.append(Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y))\n",
        "\n",
        "            except Exception as e:\n",
        "                # 変換中に予期せぬエラーが発生した場合も、そのサンプルをスキップし、ログ出力\n",
        "                # print(f\"破損したサンプルをスキップしました: {e}\")\n",
        "                continue # 続行\n",
        "\n",
        "        return data_list\n",
        "\n",
        "    def len(self):\n",
        "        return len(self._data_list)\n",
        "\n",
        "    def get(self, idx):\n",
        "        return self._data_list[idx]\n",
        "\n",
        "# データセットのロードと変換を再実行\n",
        "dataset_pyg = PyG_CounterDataset(data=data, matrix_type='normal')\n",
        "print(f\"PyTorch Geometricデータセットの変換が完了しました。有効なサンプル数: {len(dataset_pyg)}\")"
      ],
      "metadata": {
        "id": "ZMbBOhpqEn7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以上でデータの準備完了"
      ],
      "metadata": {
        "id": "QuTH3MX3AkS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ハイパーパラメータとデータローダーの準備\n",
        "\n",
        "# 3.1 Configurations\n",
        "learning_rate = 1e-3  # 学習率\n",
        "epochs = 50          # 訓練エポック数 (最初は少なめに設定)\n",
        "batch_size = 32      # バッチサイズ (元の16からGPU用に32に変更)\n",
        "channels = 32        # GNN隠れ層の出力次元 (元の128から32に変更し、計算負荷を軽減)\n",
        "layers = 3           # CrystalConv/GATConvの層の数\n",
        "ATTN_HEADS = 4       # GATモデルで使用するアテンションヘッド数\n",
        "\n",
        "# 3.2 Parameters and DataLoader Setup\n",
        "# データセットのメタ情報を取得 (前のステップで計算済み)\n",
        "N = max(g.n_nodes for g in dataset) # 最大ノード数 (23)\n",
        "F = dataset.n_node_features         # ノード特徴量の次元 (12)\n",
        "S = dataset.n_edge_features         # エッジ特徴量の次元 (6)\n",
        "n_out = dataset.n_labels            # ターゲットの次元 (1)\n",
        "n = len(dataset)                    # サンプル数 (4348)\n",
        "\n",
        "# 訓練/テスト分割 (SpektralのDisjointLoaderで使用するインデックスを生成)\n",
        "# データセットの分割比率: 訓練(70%)、テスト(30%)\n",
        "split_tr = int(0.7 * n)\n",
        "idxs = np.random.RandomState(seed=42).permutation(n)\n",
        "idx_tr, idx_te = np.split(idxs, [split_tr])\n",
        "\n",
        "dataset_tr = dataset[idx_tr]\n",
        "dataset_te = dataset[idx_te]\n",
        "\n",
        "# DataLoaderの構築 (SpektralのDisjointLoaderを使用)\n",
        "# DisjointLoaderは、GNNのバッチ処理に適した形式にデータを自動で結合・分割します\n",
        "loader_tr = DisjointLoader(dataset_tr, batch_size=batch_size, epochs=epochs)\n",
        "loader_te = DisjointLoader(dataset_te, batch_size=batch_size, epochs=1, shuffle=False)\n",
        "\n",
        "logger.info(f\"全サンプル数: {n}\")\n",
        "logger.info(f\"訓練サンプル数: {len(dataset_tr)}\")\n",
        "logger.info(f\"テストサンプル数: {len(dataset_te)}\")\n",
        "print(\"DataLoaderの構築が完了しました。\")"
      ],
      "metadata": {
        "id": "kglsEi57AnHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#モデルの定義\n",
        "# --- 4. GATモデルの再構築 (セクション 3.3の代替) ---\n",
        "# ハイパーパラメータの設定 (前のセルで定義された変数を使用)\n",
        "CHANNELS = 32\n",
        "LAYERS = 3\n",
        "ATTN_HEADS = 4\n",
        "N_OUT = 1\n",
        "\n",
        "class PyG_GNN(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_layers, num_heads, out_channels):\n",
        "        super(PyG_GNN, self).__init__()\n",
        "\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels,\n",
        "                             heads=num_heads, dropout=0.5, concat=True)\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(\n",
        "                GATConv(hidden_channels * num_heads, hidden_channels,\n",
        "                        heads=num_heads, dropout=0.5, concat=True)\n",
        "            )\n",
        "\n",
        "        # 最終GAT層: 出力次元を揃えるために heads=1, concat=False\n",
        "        self.conv_out = GATConv(hidden_channels * num_heads, hidden_channels,\n",
        "                                heads=1, dropout=0.5, concat=False)\n",
        "\n",
        "        # MLP層\n",
        "        self.dense1 = nn.Linear(hidden_channels, hidden_channels)\n",
        "        self.dense_out = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, data):\n",
        "        # PyGでは Dataオブジェクトから x, edge_index, batch を取得\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # GAT層の実行\n",
        "        x = F.elu(self.conv1(x, edge_index))\n",
        "        for conv in self.convs:\n",
        "            x = F.elu(conv(x, edge_index))\n",
        "        x = F.elu(self.conv_out(x, edge_index))\n",
        "\n",
        "        # Global Pooling (グラフ分類のため)\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # MLP実行\n",
        "        x = F.relu(self.dense1(x))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "\n",
        "        # 最終出力 (シグモイド)\n",
        "        return torch.sigmoid(self.dense_out(x))\n",
        "\n",
        "# モデルのインスタンス化\n",
        "model_pyg = PyG_GNN(\n",
        "    in_channels=dataset_pyg.num_node_features,\n",
        "    hidden_channels=CHANNELS,\n",
        "    num_layers=LAYERS,\n",
        "    num_heads=ATTN_HEADS,\n",
        "    out_channels=N_OUT\n",
        ")\n",
        "print(\"PyTorch GATモデルの再構築完了。\")"
      ],
      "metadata": {
        "id": "efzHVzkjF67B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoaderの準備 (有効なサンプルリストを使用)\n",
        "final_data_list = dataset_pyg._data_list # 破損していない有効なデータリスト\n",
        "indices = np.arange(len(final_data_list))\n",
        "\n",
        "# 訓練/テスト分割 (30% テスト)\n",
        "idx_tr, idx_te = train_test_split(indices, test_size=0.3, random_state=42)\n",
        "\n",
        "dataset_tr_pyg = [final_data_list[i] for i in idx_tr]\n",
        "dataset_te_pyg = [final_data_list[i] for i in idx_te]\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "loader_tr_pyg = PyGDataLoader(dataset_tr_pyg, batch_size=BATCH_SIZE, shuffle=True)\n",
        "loader_te_pyg = PyGDataLoader(dataset_te_pyg, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# オプティマイザと損失関数の設定\n",
        "optimizer_pyg = torch.optim.Adam(model_pyg.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.BCELoss()"
      ],
      "metadata": {
        "id": "Xp71qj2bWIVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#実行\n",
        "\n",
        "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- 5. 訓練ループの実行 ---\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 50\n",
        "\n",
        "# 訓練関数の定義\n",
        "def train_pyg(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "# メイン訓練ループの実行\n",
        "print(\"\\n PyTorch GATモデルの訓練を開始します...\")\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    loss = train_pyg(model_pyg, loader_tr_pyg, optimizer_pyg, criterion)\n",
        "    print(f\"--- Epoch {epoch}/{EPOCHS} --- Loss: {loss:.4f}\")\n",
        "\n",
        "print(\"PyTorch GATモデルの訓練が完了しました。\")"
      ],
      "metadata": {
        "id": "Iz_Y8zanGPZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#評価\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#評価関数の定義\n",
        "def evaluate_pyg(model, loader):\n",
        "    \"\"\"\n",
        "    テストデータローダーを使用してモデルを評価し、AUCとAccuracyを計算する。\n",
        "    \"\"\"\n",
        "    # 1. モデルを評価モードに設定\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    # 2. 勾配計算を無効化 (推論時のメモリ効率化)\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            # 順伝播\n",
        "            out = model(data)\n",
        "\n",
        "            # 予測確率と正解ラベルを収集\n",
        "            all_preds.append(out.cpu().numpy())\n",
        "            all_labels.append(data.y.cpu().numpy())\n",
        "\n",
        "    # 3. すべてのバッチの結果を結合\n",
        "    preds = np.concatenate(all_preds).flatten()\n",
        "    labels = np.concatenate(all_labels).flatten()\n",
        "\n",
        "    # 4. AUC (ROC曲線下面積) の計算\n",
        "    try:\n",
        "        # AUCは確率値 (0.0～1.0) を入力とする\n",
        "        auc = roc_auc_score(labels, preds)\n",
        "    except ValueError:\n",
        "        # テストセットに片方のクラスしかない場合、AUCは計算できない\n",
        "        auc = np.nan\n",
        "        print(\"Warning: テストセットに単一クラスのみ含まれるため、AUCは未定義です。\")\n",
        "\n",
        "    # 5. Accuracy (正答率) の計算\n",
        "    # 予測確率を0.5を閾値として二値化\n",
        "    predicted_classes = (preds > 0.5).astype(int)\n",
        "    accuracy = accuracy_score(labels, predicted_classes)\n",
        "\n",
        "    return auc, accuracy"
      ],
      "metadata": {
        "id": "vNnQx4H0H22b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 訓練ループの後に実行\n",
        "\n",
        "# 以前のデータ分割処理で得た、有効なデータリスト\n",
        "final_data_list = dataset_pyg._data_list\n",
        "\n",
        "# テストデータリストからNoneを明示的に除去する（安全策）\n",
        "dataset_te_pyg_clean = [data for data in dataset_te_pyg if data is not None]\n",
        "\n",
        "# DataLoaderを再構築 (念のため、エラーを引き起こした可能性のある古いローダーを使わない)\n",
        "# BATCH_SIZEは前のセルで定義済み\n",
        "loader_te_pyg_clean = PyGDataLoader(dataset_te_pyg_clean, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f\"クリーンなテストサンプル数: {len(dataset_te_pyg_clean)}\")"
      ],
      "metadata": {
        "id": "q5tifn4lIxLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 評価の実行\n",
        "\n",
        "# 評価の実行\n",
        "auc_te, acc_te = evaluate_pyg(model_pyg, loader_te_pyg_clean)\n",
        "\n",
        "print(f\"\\n--- 最終テスト結果 (PyTorch GAT) ---\")\n",
        "print(f\"Test AUC: {auc_te:.4f}\")\n",
        "print(f\"Test Accuracy: {acc_te:.4f}\")"
      ],
      "metadata": {
        "id": "sKTe6NzsH-YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#キャリブレーション\n",
        "from sklearn.calibration import calibration_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- キャリブレーションデータの取得 ---\n",
        "def get_calibration_data(model, loader, n_bins=10):\n",
        "    \"\"\"モデルの予測確率と真のラベルを取得し、キャリブレーション曲線のデータを計算する\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            out = model(data)\n",
        "            all_preds.append(out.cpu().numpy())\n",
        "            all_labels.append(data.y.cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(all_preds).flatten()\n",
        "    labels = np.concatenate(all_labels).flatten()\n",
        "\n",
        "    # scikit-learnのcalibration_curveを使用してデータを計算\n",
        "    # prob_true: 各ビンの実際の正答率 (y軸)\n",
        "    # prob_pred: 各ビンの平均予測確率 (x軸)\n",
        "    prob_true, prob_pred = calibration_curve(\n",
        "        y_true=labels,\n",
        "        y_prob=preds,\n",
        "        n_bins=n_bins,\n",
        "        strategy='uniform' # 均一な幅のビンを使用\n",
        "    )\n",
        "\n",
        "    return prob_true, prob_pred, preds, labels\n",
        "\n",
        "# --- 実行と描画 ---\n",
        "prob_true, prob_pred, preds_te, labels_te = get_calibration_data(model_pyg, loader_te_pyg_clean)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "# モデルのキャリブレーション曲線\n",
        "plt.plot(prob_pred, prob_true, marker='o', label='PyTorch GAT Model')\n",
        "# 理想的なキャリブレーション (対角線)\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n",
        "\n",
        "plt.xlabel('Average Predicted Probability (Confidence)')\n",
        "plt.ylabel('Fraction of Positives (Accuracy)')\n",
        "plt.title('Calibration Curve on Test Set')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"キャリブレーション曲線の描画が完了しました。\")\n",
        "# 理想的な対角線に近ければ近いほど、モデルの予測は「正直」であると評価できます。"
      ],
      "metadata": {
        "id": "5d2nekWpJWOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#PFI\n",
        "# --- 特徴量重要度の計算 ---\n",
        "# 特徴量の名前は filter_features 関数で定義されたグローバル変数 node_features を使用します\n",
        "global node_features # 以前のステップで定義されています\n",
        "\n",
        "def calculate_permutation_importance(model, loader, features_list, n_repeats=5):\n",
        "    \"\"\"テストデータセット上で、各特徴量のパーミュテーション重要度 (AUCの低下) を計算する\"\"\"\n",
        "\n",
        "    # 1. ベースラインAUCの計算\n",
        "    auc, _ = evaluate_pyg(model, loader)\n",
        "    baseline_auc = auc\n",
        "\n",
        "    importance = {}\n",
        "\n",
        "    # 2. 各特徴量に対して重要度を測定\n",
        "    for idx, feature_name in enumerate(features_list):\n",
        "        auc_decreases = []\n",
        "\n",
        "        for _ in range(n_repeats):\n",
        "            # 2.1. テストローダー内の全グラフをパーミュテーション処理\n",
        "            permuted_data_list = []\n",
        "            for batch_data in loader:\n",
        "                # バッチ内のノード特徴量 x のコピーを作成\n",
        "                x_permuted = batch_data.x.clone()\n",
        "\n",
        "                # 特定の特徴量列 (idx) をランダムに入れ替え (パーミュテーション)\n",
        "                # バッチ全体でランダムに行うのが一般的\n",
        "                perm_indices = torch.randperm(x_permuted.size(0))\n",
        "                x_permuted[:, idx] = x_permuted[perm_indices, idx]\n",
        "\n",
        "                # 新しいxでデータオブジェクトを再構築 (バッチ構造を維持)\n",
        "                permuted_batch = batch_data.clone()\n",
        "                permuted_batch.x = x_permuted\n",
        "                permuted_data_list.append(permuted_batch)\n",
        "\n",
        "            # 2.2. パーミュテーション後のAUCを評価\n",
        "            # Note: PyGDataLoaderはリストを受け取るため、ここで新しいDataLoaderを一時的に使用します\n",
        "            permuted_loader = PyGDataLoader(permuted_data_list, batch_size=loader.batch_size, shuffle=False)\n",
        "            permuted_auc, _ = evaluate_pyg(model, permuted_loader)\n",
        "            auc_decreases.append(baseline_auc - permuted_auc)\n",
        "\n",
        "        # 2.3. 平均低下量を格納\n",
        "        importance[feature_name] = np.mean(auc_decreases)\n",
        "\n",
        "    return importance\n",
        "\n",
        "# --- 実行と描画 ---\n",
        "\n",
        "# 注意: n_repeatsが多いほど精度が上がりますが、計算時間がかかります。\n",
        "importance_results = calculate_permutation_importance(model_pyg, loader_te_pyg_clean, node_features, n_repeats=1) # まずは n_repeats=1 で試します\n",
        "\n",
        "# 結果をソートして表示\n",
        "sorted_importance = sorted(importance_results.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "print(\"\\n--- 特徴量重要度 (AUC低下量) ---\")\n",
        "for feature, decrease in sorted_importance:\n",
        "    print(f\"[{feature: <20}] : {decrease:.5f}\")\n",
        "\n",
        "# 棒グラフで視覚化\n",
        "features = [item[0] for item in sorted_importance]\n",
        "decreases = [item[1] for item in sorted_importance]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(features, decreases, color='skyblue')\n",
        "plt.xlabel('AUC Decrease after Permutation')\n",
        "plt.title('Permutation Feature Importance (PyTorch GAT)')\n",
        "plt.gca().invert_yaxis() # 最も重要な特徴量を上に表示\n",
        "plt.grid(axis='x', linestyle='--')\n",
        "plt.show()\n",
        "\n",
        "print(\"特徴量重要度の計算と描画が完了しました。\")"
      ],
      "metadata": {
        "id": "pRqxLXJoJcnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "精度は元論文に及んでいないが、ある程度モデルとして使える精度が出た。また、PFIでも元論文と完全に一致はしていないが重要視した特徴量はほぼ一致した。\n",
        "このコードではアテンションの可視化ができないので、別ファイルで可視化ありのコードを実行する。"
      ],
      "metadata": {
        "id": "4msvJMWbMYfN"
      }
    }
  ]
}