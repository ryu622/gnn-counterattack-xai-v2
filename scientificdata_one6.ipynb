{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryu622/gnn-counterattack-xai-v2/blob/feat%2Fnew-file/scientificdata_one6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mWxW2igr3ZI"
      },
      "source": [
        "ï¼”ã‹ã‚‰ã‚°ãƒ©ãƒ•æ§‹ç¯‰ã®æ–¹æ³•ã‚’å…¨çµåˆå‹ã‹ã‚‰è·é›¢é–¾å€¤ã«å¤‰æ›´"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltlj4x8Z31pP"
      },
      "source": [
        "å¤‰æ›´ç‚¹ï¼šã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®æˆåŠŸã®å®šç¾©ã‚’ï¼“ï¼•ï½åœ°ç‚¹åˆ°é”ã®ã¿â†’ï¼’ï¼•ï½åˆ°é”ã‹ã¤ï¼•ï½å‰é€²ã«"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxLtiVge7-0f",
        "outputId": "1a468def-3f1c-46ee-a1e2-50718d9da26e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnkvNbXz8YoL",
        "outputId": "b10a0d11-8e39-4fd8-853d-922e521f5274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch_geometric in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.4.1)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch_geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2026.1.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHZPKF7685s3",
        "outputId": "f1c9429f-1130-43d7-92f9-06016e37351b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: floodlight in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: h5py<4.0.0,>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from floodlight) (3.15.1)\n",
            "Requirement already satisfied: iso8601<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from floodlight) (2.1.0)\n",
            "Requirement already satisfied: lxml<6.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from floodlight) (5.4.0)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.9.2 in /usr/local/lib/python3.12/dist-packages (from floodlight) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from floodlight) (2.4.1)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.2.2 in /usr/local/lib/python3.12/dist-packages (from floodlight) (2.2.2)\n",
            "Requirement already satisfied: pytz<2025.0,>=2024.1 in /usr/local/lib/python3.12/dist-packages (from floodlight) (2024.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from floodlight) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0.0,>=3.9.2->floodlight) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0.0,>=3.9.2->floodlight) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0.0,>=3.9.2->floodlight) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0.0,>=3.9.2->floodlight) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0.0,>=3.9.2->floodlight) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0.0,>=3.9.2->floodlight) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0.0,>=3.9.2->floodlight) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4.0.0,>=3.9.2->floodlight) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.2.2->floodlight) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.9.2->floodlight) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install floodlight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0mCPYwN8XKO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "from collections import defaultdict\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import radius_graph\n",
        "import floodlight.io.dfl as dfl_io\n",
        "\n",
        "# ==========================================\n",
        "# 1. GNNDataBuilder ã‚¯ãƒ©ã‚¹ (å…¨ãƒ¡ã‚½ãƒƒãƒ‰å®Œå…¨ãƒ»è·é›¢é–¾å€¤ã‚°ãƒ©ãƒ•ç‰ˆ)\n",
        "# ==========================================\n",
        "class GNNDataBuilder:\n",
        "    def __init__(self):\n",
        "        self.OBSERVATION_WINDOW = 25   # 1ç§’é–“ã®è¦³æ¸¬ï¼ˆé€Ÿåº¦ç®—å‡ºç”¨ï¼‰\n",
        "        self.PREDICTION_TARGET = 125   # 5ç§’å¾Œã®åˆ°é”åœ°ç‚¹ã‚’ç¢ºèª\n",
        "        self.dt_velocity = 0.20        # 5ãƒ•ãƒ¬ãƒ¼ãƒ å·®ã§ã®é€Ÿåº¦è¨ˆç®—\n",
        "        self.pitch_length_half = 52.5\n",
        "        self.pitch_width_half = 34.0\n",
        "        self.SUCCESS_X_THRESHOLD = 25.0\n",
        "\n",
        "    def extract_sequences(self, df_pos, df_event, match_id_idx, manual_flip, left_team_id):\n",
        "        recovery_types = ['TacklingGame', 'BallClaiming', 'BallDeflection']\n",
        "        recovery_events = df_event[df_event['eID'].isin(recovery_types)]\n",
        "        sequences = []\n",
        "\n",
        "        is_cm = df_pos['Ball_x'].abs().max() > 500\n",
        "        scale = 100.0 if is_cm else 1.0\n",
        "\n",
        "        # Offsetè£œæ­£\n",
        "        max_x_raw = df_pos['Ball_x'].max() / scale\n",
        "        min_x_raw = df_pos['Ball_x'].min() / scale\n",
        "        # æœ€å°å€¤ãŒ-30ä»¥ä¸‹ãªã‚‰ã‚»ãƒ³ã‚¿ãƒ¼0ç³»ã¨åˆ¤æ–­ï¼ˆMatch 4å¯¾ç­–ï¼‰\n",
        "        offset_x = 0.0 if min_x_raw < -30 else 52.5\n",
        "\n",
        "        stadium_flip = float(manual_flip)\n",
        "\n",
        "        for idx, event in recovery_events.iterrows():\n",
        "            period = str(event['period'])\n",
        "            striking_team = str(event['tID']).strip()\n",
        "\n",
        "            # --- ç‰©ç†æ•´åˆæ€§ã®ãŸã‚ã®Flipãƒ­ã‚¸ãƒƒã‚¯ ---\n",
        "            if period in ['1', 'firstHalf']:\n",
        "                raw_is_attacking_right = (striking_team == left_team_id)\n",
        "            else:\n",
        "                raw_is_attacking_right = (striking_team != left_team_id)\n",
        "\n",
        "            dynamic_flip = (1.0 if raw_is_attacking_right else -1.0) * stadium_flip\n",
        "\n",
        "            # ãƒ•ãƒ¬ãƒ¼ãƒ ç‰¹å®š\n",
        "            start_f_raw = int(event['gameclock'] * 25)\n",
        "            potential_start = df_pos[df_pos['frame_idx'] >= start_f_raw].index\n",
        "            if len(potential_start) == 0: continue\n",
        "            start_idx = potential_start[0]\n",
        "            target_idx = start_idx + self.PREDICTION_TARGET\n",
        "            if target_idx >= len(df_pos): continue\n",
        "\n",
        "            # --- ãƒ©ãƒ™ãƒ«åˆ¤å®š ---\n",
        "            start_x_flipped = ((df_pos.iloc[start_idx]['Ball_x'] / scale) - offset_x) * dynamic_flip\n",
        "            target_x_flipped = ((df_pos.iloc[target_idx]['Ball_x'] / scale) - offset_x) * dynamic_flip\n",
        "\n",
        "            is_in_deep_area = target_x_flipped > self.SUCCESS_X_THRESHOLD\n",
        "            is_progressing = (target_x_flipped - start_x_flipped) > 5.0\n",
        "\n",
        "            label = 1 if (is_in_deep_area and is_progressing) else 0\n",
        "\n",
        "            # è¦³æ¸¬ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼ˆ1ç§’åˆ†ï¼‰\n",
        "            obs_frames = df_pos.iloc[start_idx : start_idx + self.OBSERVATION_WINDOW].copy()\n",
        "            obs_frames.loc[:, 'offset_x_val'] = float(offset_x)\n",
        "            obs_frames.loc[:, 'flip_factor'] = float(dynamic_flip)\n",
        "            obs_frames.loc[:, 'label'] = int(label)\n",
        "            obs_frames.loc[:, 'SequenceID'] = int(idx + (match_id_idx * 1000))\n",
        "            sequences.append(obs_frames)\n",
        "\n",
        "        return sequences\n",
        "\n",
        "    def to_pyg_data(self, sequences, team_map):\n",
        "        pyg_list = []\n",
        "        if not sequences: return []\n",
        "\n",
        "        is_cm = sequences[0]['Ball_x'].abs().max() > 500\n",
        "        scale = 100.0 if is_cm else 1.0\n",
        "\n",
        "        for seq in sequences:\n",
        "            # ç‰©ç†ã‚²ãƒ¼ãƒˆç”¨ã«3åœ°ç‚¹ã‚’å–å¾—\n",
        "            frame_pprev = seq.iloc[-11] # 10æšå‰ (pprev)\n",
        "            frame_prev  = seq.iloc[-6]  # 5æšå‰ (prev)\n",
        "            frame_curr  = seq.iloc[-1]  # ç¾åœ¨ (pos)\n",
        "\n",
        "            off_x = frame_curr['offset_x_val']\n",
        "            flip = frame_curr['flip_factor']\n",
        "            label = int(frame_curr['label'])\n",
        "            sid = int(frame_curr['SequenceID'])\n",
        "\n",
        "            def transform_x(raw_val):\n",
        "                return (((raw_val / scale) - off_x) * flip) / self.pitch_length_half\n",
        "            def transform_y(raw_val, y_aug=1.0):\n",
        "                return (((raw_val / scale) * flip) * y_aug) / self.pitch_width_half\n",
        "\n",
        "            for y_aug in [1.0, -1.0]:\n",
        "                node_features, pos_list, prev_pos_list, pprev_pos_list = [], [], [], []\n",
        "\n",
        "                entities = []\n",
        "                for team_prefix in ['Home', 'Away']:\n",
        "                    team_val = 0.0 if team_prefix == 'Home' else 1.0\n",
        "                    for p_id in team_map[team_prefix]:\n",
        "                        entities.append((f\"{p_id}_x\", f\"{p_id}_y\", team_val))\n",
        "                entities.append(('Ball_x', 'Ball_y', 2.0))\n",
        "\n",
        "                for col_x, col_y, t_val in entities:\n",
        "                    if col_x not in frame_curr or pd.isna(frame_curr[col_x]) or frame_curr[col_x] == 0:\n",
        "                        continue\n",
        "\n",
        "                    px, py = transform_x(frame_curr[col_x]), transform_y(frame_curr[col_y], y_aug)\n",
        "                    px_p, py_p = transform_x(frame_prev[col_x]), transform_y(frame_prev[col_y], y_aug)\n",
        "                    px_pp, py_pp = transform_x(frame_pprev[col_x]), transform_y(frame_pprev[col_y], y_aug)\n",
        "\n",
        "                    vx = (px - px_p) / self.dt_velocity\n",
        "                    vy = (py - py_p) / self.dt_velocity\n",
        "\n",
        "                    node_features.append([px, py, vx, vy, 0.0, 0.0, t_val])\n",
        "                    pos_list.append([px, py])\n",
        "                    prev_pos_list.append([px_p, py_p])\n",
        "                    pprev_pos_list.append([px_pp, py_pp])\n",
        "\n",
        "                # ãƒ†ãƒ³ã‚½ãƒ«åŒ–\n",
        "                x_tensor = torch.tensor(node_features, dtype=torch.float)\n",
        "                pos_tensor = torch.tensor(pos_list, dtype=torch.float)\n",
        "\n",
        "                # --- ğŸŒŸ æ‰‹å‹•è·é›¢é–¾å€¤ã‚°ãƒ©ãƒ•æ§‹ç¯‰ (torch-clusterä¸è¦) ğŸŒŸ ---\n",
        "                # r=0.5 (ç´„26m) ã‚’é–¾å€¤ã¨ã™ã‚‹\n",
        "                r = 0.6\n",
        "                # å…¨ãƒãƒ¼ãƒ‰é–“ã®è·é›¢è¡Œåˆ—ã‚’è¨ˆç®— (å½¢çŠ¶: [N, N])\n",
        "                dist_matrix = torch.cdist(pos_tensor, pos_tensor, p=2)\n",
        "                # è·é›¢ãŒ r ä»¥ä¸‹ã‹ã¤è‡ªåˆ†è‡ªèº«ã§ãªã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—\n",
        "                edge_index = (dist_matrix <= r).nonzero(as_tuple=False).t()\n",
        "                # è‡ªå·±ãƒ«ãƒ¼ãƒ— (i==j) ã‚’é™¤å»\n",
        "                mask = edge_index[0] != edge_index[1]\n",
        "                edge_index = edge_index[:, mask]\n",
        "\n",
        "                pyg_list.append(Data(\n",
        "                    x=x_tensor,\n",
        "                    edge_index=edge_index,\n",
        "                    y=torch.tensor([label], dtype=torch.long),\n",
        "                    pos=pos_tensor,\n",
        "                    prev_pos=torch.tensor(prev_pos_list, dtype=torch.float),\n",
        "                    pprev_pos=torch.tensor(pprev_pos_list, dtype=torch.float),\n",
        "                    sequence_id=torch.tensor([sid if y_aug == 1.0 else sid + 5000], dtype=torch.long)\n",
        "                ))\n",
        "        return pyg_list\n",
        "\n",
        "# ==========================================\n",
        "# 2. è£œåŠ©é–¢æ•°ã®å®šç¾©\n",
        "# ==========================================\n",
        "def get_match_direction_map(e_path):\n",
        "    tree = ET.parse(e_path)\n",
        "    root = tree.getroot()\n",
        "    d_map = {}\n",
        "    for event in root.findall('.//Event'):\n",
        "        ko = event.find('KickOff')\n",
        "        if ko is not None:\n",
        "            period = ko.get('GameSection')\n",
        "            d_map[period] = {'Left': ko.get('TeamLeft'), 'Right': ko.get('TeamRight')}\n",
        "    return d_map\n",
        "\n",
        "def parse_dfl_positions_to_wide(p_path):\n",
        "    tree = ET.parse(p_path)\n",
        "    root = tree.getroot()\n",
        "    data_dict = defaultdict(dict)\n",
        "    all_pids = set()\n",
        "    for frameset in root.findall('.//FrameSet'):\n",
        "        pID, period = frameset.get('PersonId'), frameset.get('GameSection')\n",
        "        all_pids.add(pID)\n",
        "        for frame in frameset.findall('Frame'):\n",
        "            n = int(frame.get('N'))\n",
        "            data_dict[n][pID] = [float(frame.get('X')), float(frame.get('Y')), float(frame.get('S'))]\n",
        "            data_dict[n]['period'] = period\n",
        "    sorted_frames = sorted(data_dict.keys())\n",
        "    sorted_pIDs = sorted(list(all_pids))\n",
        "    final_data = []\n",
        "    for n in sorted_frames:\n",
        "        row = {'frame_idx': n, 'period': data_dict[n].get('period')}\n",
        "        for pID in sorted_pIDs:\n",
        "            vals = data_dict[n].get(pID, [np.nan, np.nan, np.nan])\n",
        "            name = 'Ball' if '0000XT' in pID else pID\n",
        "            row[f'{name}_x'], row[f'{name}_y'], row[f'{name}_s'] = vals\n",
        "        final_data.append(row)\n",
        "    return pd.DataFrame(final_data).ffill()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6Dnzw2x8kND",
        "outputId": "76fef8b5-5c7a-43df-e683-091618ef02fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== ã€å®Ÿè¡Œã€‘ è©¦åˆ 4: DFL-MAT-J03WOY =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/floodlight/core/events.py:80: UserWarning: The 'gameclock' column does not match the defined value range (from floodlight.core.definitions). This may lead to unexpected behavior of methods using this column.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> âœ… æˆåŠŸ: 438 ã‚·ãƒ¼ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 3. 1è©¦åˆãšã¤æ¤œè¨¼ãƒ»å®Ÿè¡Œãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—\n",
        "# ==========================================\n",
        "raw_data_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Raw_Data\"\n",
        "save_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/matches_v17\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "builder = GNNDataBuilder()\n",
        "\n",
        "# --- ã€ç¢ºå®šè¨­å®šã€‘Match 4 æ•‘æ¸ˆè¨­å®š ---\n",
        "TARGET_MATCH_IDX = 3\n",
        "SET_MANUAL_FLIP = 1.0 # ãƒœãƒ¼ãƒ«æ¨é€²ç‡ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«åè»¢\n",
        "SET_LEFT_TEAM = \"00000Q\"\n",
        "\n",
        "info_files = sorted([f for f in os.listdir(raw_data_path) if \"matchinformation\" in f])\n",
        "i_f = info_files[TARGET_MATCH_IDX]\n",
        "match_id_str = i_f.split('_')[-1].replace('.xml', '')\n",
        "\n",
        "print(f\"\\n===== ã€å®Ÿè¡Œã€‘ è©¦åˆ {TARGET_MATCH_IDX + 1}: {match_id_str} =====\")\n",
        "\n",
        "i_path = os.path.join(raw_data_path, i_f)\n",
        "p_path = next(os.path.join(raw_data_path, f) for f in os.listdir(raw_data_path) if match_id_str in f and \"positions_raw\" in f)\n",
        "e_path = next(os.path.join(raw_data_path, f) for f in os.listdir(raw_data_path) if match_id_str in f and \"events_raw\" in f)\n",
        "\n",
        "try:\n",
        "    # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
        "    sheets = dfl_io.read_teamsheets_from_mat_info_xml(i_path)\n",
        "    df_pos = parse_dfl_positions_to_wide(p_path)\n",
        "    events, _, _ = dfl_io.read_event_data_xml(e_path, i_path)\n",
        "\n",
        "    all_events_list = []\n",
        "    for half in events:\n",
        "        for team_label in events[half]:\n",
        "            df_ev = events[half][team_label].events.copy()\n",
        "            df_ev['tID'] = str(sheets[team_label].teamsheet['tID'].iloc[0]).strip()\n",
        "            df_ev['period'] = half\n",
        "            if 'type' in df_ev.columns: df_ev = df_ev.rename(columns={'type': 'eID'})\n",
        "            all_events_list.append(df_ev)\n",
        "    df_event_all = pd.concat(all_events_list)\n",
        "\n",
        "    # 2. å¤‰æ›å®Ÿè¡Œ\n",
        "    team_map = {'Home': list(sheets['Home'].teamsheet['pID']), 'Away': list(sheets['Away'].teamsheet['pID'])}\n",
        "\n",
        "    match_sequences = builder.extract_sequences(\n",
        "        df_pos=df_pos,\n",
        "        df_event=df_event_all,\n",
        "        match_id_idx=TARGET_MATCH_IDX + 1,\n",
        "        manual_flip=SET_MANUAL_FLIP,\n",
        "        left_team_id=SET_LEFT_TEAM\n",
        "    )\n",
        "\n",
        "    pyg_data = builder.to_pyg_data(match_sequences, team_map)\n",
        "\n",
        "    # 3. ä¿å­˜\n",
        "    if pyg_data:\n",
        "        save_path = os.path.join(save_dir, f\"match_{TARGET_MATCH_IDX + 1}.pt\")\n",
        "        torch.save(pyg_data, save_path)\n",
        "        print(f\"-> âœ… æˆåŠŸ: {len(pyg_data)} ã‚·ãƒ¼ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\")\n",
        "    else:\n",
        "        print(\"-> âš  æˆåŠŸã‚·ãƒ¼ãƒ³ãŒ0ä»¶ã§ã—ãŸã€‚\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"-> âŒ ã‚¨ãƒ©ãƒ¼: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4k9V1ukr0rp",
        "outputId": "70468fb1-284a-4d80-a858-ae25baacd401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== ã€å®Ÿè¡Œã€‘ è©¦åˆ 7: DFL-MAT-J03WR9 =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/floodlight/core/events.py:80: UserWarning: The 'gameclock' column does not match the defined value range (from floodlight.core.definitions). This may lead to unexpected behavior of methods using this column.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> âœ… æˆåŠŸ: 432 ã‚·ãƒ¼ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 3. 1è©¦åˆãšã¤æ¤œè¨¼ãƒ»å®Ÿè¡Œãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—\n",
        "# ==========================================\n",
        "raw_data_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Raw_Data\"\n",
        "save_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/matches_v17\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "builder = GNNDataBuilder()\n",
        "\n",
        "# --- ã€ç¢ºå®šè¨­å®šã€‘Match 4 æ•‘æ¸ˆè¨­å®š ---\n",
        "TARGET_MATCH_IDX = 6\n",
        "SET_MANUAL_FLIP = -1.0 # ãƒœãƒ¼ãƒ«æ¨é€²ç‡ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«åè»¢\n",
        "SET_LEFT_TEAM = \"00000P\"\n",
        "\n",
        "info_files = sorted([f for f in os.listdir(raw_data_path) if \"matchinformation\" in f])\n",
        "i_f = info_files[TARGET_MATCH_IDX]\n",
        "match_id_str = i_f.split('_')[-1].replace('.xml', '')\n",
        "\n",
        "print(f\"\\n===== ã€å®Ÿè¡Œã€‘ è©¦åˆ {TARGET_MATCH_IDX + 1}: {match_id_str} =====\")\n",
        "\n",
        "i_path = os.path.join(raw_data_path, i_f)\n",
        "p_path = next(os.path.join(raw_data_path, f) for f in os.listdir(raw_data_path) if match_id_str in f and \"positions_raw\" in f)\n",
        "e_path = next(os.path.join(raw_data_path, f) for f in os.listdir(raw_data_path) if match_id_str in f and \"events_raw\" in f)\n",
        "\n",
        "try:\n",
        "    # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
        "    sheets = dfl_io.read_teamsheets_from_mat_info_xml(i_path)\n",
        "    df_pos = parse_dfl_positions_to_wide(p_path)\n",
        "    events, _, _ = dfl_io.read_event_data_xml(e_path, i_path)\n",
        "\n",
        "    all_events_list = []\n",
        "    for half in events:\n",
        "        for team_label in events[half]:\n",
        "            df_ev = events[half][team_label].events.copy()\n",
        "            df_ev['tID'] = str(sheets[team_label].teamsheet['tID'].iloc[0]).strip()\n",
        "            df_ev['period'] = half\n",
        "            if 'type' in df_ev.columns: df_ev = df_ev.rename(columns={'type': 'eID'})\n",
        "            all_events_list.append(df_ev)\n",
        "    df_event_all = pd.concat(all_events_list)\n",
        "\n",
        "    # 2. å¤‰æ›å®Ÿè¡Œ\n",
        "    team_map = {'Home': list(sheets['Home'].teamsheet['pID']), 'Away': list(sheets['Away'].teamsheet['pID'])}\n",
        "\n",
        "    match_sequences = builder.extract_sequences(\n",
        "        df_pos=df_pos,\n",
        "        df_event=df_event_all,\n",
        "        match_id_idx=TARGET_MATCH_IDX + 1,\n",
        "        manual_flip=SET_MANUAL_FLIP,\n",
        "        left_team_id=SET_LEFT_TEAM\n",
        "    )\n",
        "\n",
        "    pyg_data = builder.to_pyg_data(match_sequences, team_map)\n",
        "\n",
        "    # 3. ä¿å­˜\n",
        "    if pyg_data:\n",
        "        save_path = os.path.join(save_dir, f\"match_{TARGET_MATCH_IDX + 1}.pt\")\n",
        "        torch.save(pyg_data, save_path)\n",
        "        print(f\"-> âœ… æˆåŠŸ: {len(pyg_data)} ã‚·ãƒ¼ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\")\n",
        "    else:\n",
        "        print(\"-> âš  æˆåŠŸã‚·ãƒ¼ãƒ³ãŒ0ä»¶ã§ã—ãŸã€‚\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"-> âŒ ã‚¨ãƒ©ãƒ¼: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFGDSPV2vSwn",
        "outputId": "a691cdfb-294a-49ee-9f2a-0031586fcf6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== ã€å®Ÿè¡Œã€‘ è©¦åˆ 6: DFL-MAT-J03WQQ =====\n",
            "-> âœ… æˆåŠŸ: 488 ã‚·ãƒ¼ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 3. 1è©¦åˆãšã¤æ¤œè¨¼ãƒ»å®Ÿè¡Œãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—\n",
        "# ==========================================\n",
        "raw_data_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Raw_Data\"\n",
        "save_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/matches_v17\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "builder = GNNDataBuilder()\n",
        "\n",
        "# --- ã€ç¢ºå®šè¨­å®šã€‘Match 4 æ•‘æ¸ˆè¨­å®š ---\n",
        "TARGET_MATCH_IDX = 5\n",
        "SET_MANUAL_FLIP = 1.0 # ãƒœãƒ¼ãƒ«æ¨é€²ç‡ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«åè»¢\n",
        "SET_LEFT_TEAM = \"00000H\"\n",
        "\n",
        "info_files = sorted([f for f in os.listdir(raw_data_path) if \"matchinformation\" in f])\n",
        "i_f = info_files[TARGET_MATCH_IDX]\n",
        "match_id_str = i_f.split('_')[-1].replace('.xml', '')\n",
        "\n",
        "print(f\"\\n===== ã€å®Ÿè¡Œã€‘ è©¦åˆ {TARGET_MATCH_IDX + 1}: {match_id_str} =====\")\n",
        "\n",
        "i_path = os.path.join(raw_data_path, i_f)\n",
        "p_path = next(os.path.join(raw_data_path, f) for f in os.listdir(raw_data_path) if match_id_str in f and \"positions_raw\" in f)\n",
        "e_path = next(os.path.join(raw_data_path, f) for f in os.listdir(raw_data_path) if match_id_str in f and \"events_raw\" in f)\n",
        "\n",
        "try:\n",
        "    # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
        "    sheets = dfl_io.read_teamsheets_from_mat_info_xml(i_path)\n",
        "    df_pos = parse_dfl_positions_to_wide(p_path)\n",
        "    events, _, _ = dfl_io.read_event_data_xml(e_path, i_path)\n",
        "\n",
        "    all_events_list = []\n",
        "    for half in events:\n",
        "        for team_label in events[half]:\n",
        "            df_ev = events[half][team_label].events.copy()\n",
        "            df_ev['tID'] = str(sheets[team_label].teamsheet['tID'].iloc[0]).strip()\n",
        "            df_ev['period'] = half\n",
        "            if 'type' in df_ev.columns: df_ev = df_ev.rename(columns={'type': 'eID'})\n",
        "            all_events_list.append(df_ev)\n",
        "    df_event_all = pd.concat(all_events_list)\n",
        "\n",
        "    # 2. å¤‰æ›å®Ÿè¡Œ\n",
        "    team_map = {'Home': list(sheets['Home'].teamsheet['pID']), 'Away': list(sheets['Away'].teamsheet['pID'])}\n",
        "\n",
        "    match_sequences = builder.extract_sequences(\n",
        "        df_pos=df_pos,\n",
        "        df_event=df_event_all,\n",
        "        match_id_idx=TARGET_MATCH_IDX + 1,\n",
        "        manual_flip=SET_MANUAL_FLIP,\n",
        "        left_team_id=SET_LEFT_TEAM\n",
        "    )\n",
        "\n",
        "    pyg_data = builder.to_pyg_data(match_sequences, team_map)\n",
        "\n",
        "    # 3. ä¿å­˜\n",
        "    if pyg_data:\n",
        "        save_path = os.path.join(save_dir, f\"match_{TARGET_MATCH_IDX + 1}.pt\")\n",
        "        torch.save(pyg_data, save_path)\n",
        "        print(f\"-> âœ… æˆåŠŸ: {len(pyg_data)} ã‚·ãƒ¼ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\")\n",
        "    else:\n",
        "        print(\"-> âš  æˆåŠŸã‚·ãƒ¼ãƒ³ãŒ0ä»¶ã§ã—ãŸã€‚\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"-> âŒ ã‚¨ãƒ©ãƒ¼: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHS4m6US3ccv",
        "outputId": "bcf8dbbb-8e72-41eb-d58f-f0ad866b1552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== ã€å®Ÿè¡Œã€‘ è©¦åˆ 5: DFL-MAT-J03WPY =====\n",
            "-> âœ… æˆåŠŸ: 442 ã‚·ãƒ¼ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 3. 1è©¦åˆãšã¤æ¤œè¨¼ãƒ»å®Ÿè¡Œãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—\n",
        "# ==========================================\n",
        "raw_data_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Raw_Data\"\n",
        "save_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/matches_v17\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "builder = GNNDataBuilder()\n",
        "\n",
        "# --- ã€ç¢ºå®šè¨­å®šã€‘Match 4 æ•‘æ¸ˆè¨­å®š ---\n",
        "TARGET_MATCH_IDX = 4\n",
        "SET_MANUAL_FLIP = -1.0 # ãƒœãƒ¼ãƒ«æ¨é€²ç‡ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«åè»¢\n",
        "SET_LEFT_TEAM = \"000005\"\n",
        "\n",
        "info_files = sorted([f for f in os.listdir(raw_data_path) if \"matchinformation\" in f])\n",
        "i_f = info_files[TARGET_MATCH_IDX]\n",
        "match_id_str = i_f.split('_')[-1].replace('.xml', '')\n",
        "\n",
        "print(f\"\\n===== ã€å®Ÿè¡Œã€‘ è©¦åˆ {TARGET_MATCH_IDX + 1}: {match_id_str} =====\")\n",
        "\n",
        "i_path = os.path.join(raw_data_path, i_f)\n",
        "p_path = next(os.path.join(raw_data_path, f) for f in os.listdir(raw_data_path) if match_id_str in f and \"positions_raw\" in f)\n",
        "e_path = next(os.path.join(raw_data_path, f) for f in os.listdir(raw_data_path) if match_id_str in f and \"events_raw\" in f)\n",
        "\n",
        "try:\n",
        "    # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
        "    sheets = dfl_io.read_teamsheets_from_mat_info_xml(i_path)\n",
        "    df_pos = parse_dfl_positions_to_wide(p_path)\n",
        "    events, _, _ = dfl_io.read_event_data_xml(e_path, i_path)\n",
        "\n",
        "    all_events_list = []\n",
        "    for half in events:\n",
        "        for team_label in events[half]:\n",
        "            df_ev = events[half][team_label].events.copy()\n",
        "            df_ev['tID'] = str(sheets[team_label].teamsheet['tID'].iloc[0]).strip()\n",
        "            df_ev['period'] = half\n",
        "            if 'type' in df_ev.columns: df_ev = df_ev.rename(columns={'type': 'eID'})\n",
        "            all_events_list.append(df_ev)\n",
        "    df_event_all = pd.concat(all_events_list)\n",
        "\n",
        "    # 2. å¤‰æ›å®Ÿè¡Œ\n",
        "    team_map = {'Home': list(sheets['Home'].teamsheet['pID']), 'Away': list(sheets['Away'].teamsheet['pID'])}\n",
        "\n",
        "    match_sequences = builder.extract_sequences(\n",
        "        df_pos=df_pos,\n",
        "        df_event=df_event_all,\n",
        "        match_id_idx=TARGET_MATCH_IDX + 1,\n",
        "        manual_flip=SET_MANUAL_FLIP,\n",
        "        left_team_id=SET_LEFT_TEAM\n",
        "    )\n",
        "\n",
        "    pyg_data = builder.to_pyg_data(match_sequences, team_map)\n",
        "\n",
        "    # 3. ä¿å­˜\n",
        "    if pyg_data:\n",
        "        save_path = os.path.join(save_dir, f\"match_{TARGET_MATCH_IDX + 1}.pt\")\n",
        "        torch.save(pyg_data, save_path)\n",
        "        print(f\"-> âœ… æˆåŠŸ: {len(pyg_data)} ã‚·ãƒ¼ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\")\n",
        "    else:\n",
        "        print(\"-> âš  æˆåŠŸã‚·ãƒ¼ãƒ³ãŒ0ä»¶ã§ã—ãŸã€‚\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"-> âŒ ã‚¨ãƒ©ãƒ¼: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-qeAbgU5Kwr",
        "outputId": "b3a72316-f73b-4223-ad7f-47999626cb9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== ã€å®Ÿè¡Œã€‘ è©¦åˆ 3: DFL-MAT-J03WOH =====\n",
            "-> âœ… æˆåŠŸ: 492 ã‚·ãƒ¼ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 3. 1è©¦åˆãšã¤æ¤œè¨¼ãƒ»å®Ÿè¡Œãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—\n",
        "# ==========================================\n",
        "raw_data_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Raw_Data\"\n",
        "save_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/matches_v17\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "builder = GNNDataBuilder()\n",
        "\n",
        "# --- ã€ç¢ºå®šè¨­å®šã€‘Match 4 æ•‘æ¸ˆè¨­å®š ---\n",
        "TARGET_MATCH_IDX = 2\n",
        "SET_MANUAL_FLIP = 1.0 # ãƒœãƒ¼ãƒ«æ¨é€²ç‡ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«åè»¢\n",
        "SET_LEFT_TEAM = \"000011\"\n",
        "\n",
        "info_files = sorted([f for f in os.listdir(raw_data_path) if \"matchinformation\" in f])\n",
        "i_f = info_files[TARGET_MATCH_IDX]\n",
        "match_id_str = i_f.split('_')[-1].replace('.xml', '')\n",
        "\n",
        "print(f\"\\n===== ã€å®Ÿè¡Œã€‘ è©¦åˆ {TARGET_MATCH_IDX + 1}: {match_id_str} =====\")\n",
        "\n",
        "i_path = os.path.join(raw_data_path, i_f)\n",
        "p_path = next(os.path.join(raw_data_path, f) for f in os.listdir(raw_data_path) if match_id_str in f and \"positions_raw\" in f)\n",
        "e_path = next(os.path.join(raw_data_path, f) for f in os.listdir(raw_data_path) if match_id_str in f and \"events_raw\" in f)\n",
        "\n",
        "try:\n",
        "    # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
        "    sheets = dfl_io.read_teamsheets_from_mat_info_xml(i_path)\n",
        "    df_pos = parse_dfl_positions_to_wide(p_path)\n",
        "    events, _, _ = dfl_io.read_event_data_xml(e_path, i_path)\n",
        "\n",
        "    all_events_list = []\n",
        "    for half in events:\n",
        "        for team_label in events[half]:\n",
        "            df_ev = events[half][team_label].events.copy()\n",
        "            df_ev['tID'] = str(sheets[team_label].teamsheet['tID'].iloc[0]).strip()\n",
        "            df_ev['period'] = half\n",
        "            if 'type' in df_ev.columns: df_ev = df_ev.rename(columns={'type': 'eID'})\n",
        "            all_events_list.append(df_ev)\n",
        "    df_event_all = pd.concat(all_events_list)\n",
        "\n",
        "    # 2. å¤‰æ›å®Ÿè¡Œ\n",
        "    team_map = {'Home': list(sheets['Home'].teamsheet['pID']), 'Away': list(sheets['Away'].teamsheet['pID'])}\n",
        "\n",
        "    match_sequences = builder.extract_sequences(\n",
        "        df_pos=df_pos,\n",
        "        df_event=df_event_all,\n",
        "        match_id_idx=TARGET_MATCH_IDX + 1,\n",
        "        manual_flip=SET_MANUAL_FLIP,\n",
        "        left_team_id=SET_LEFT_TEAM\n",
        "    )\n",
        "\n",
        "    pyg_data = builder.to_pyg_data(match_sequences, team_map)\n",
        "\n",
        "    # 3. ä¿å­˜\n",
        "    if pyg_data:\n",
        "        save_path = os.path.join(save_dir, f\"match_{TARGET_MATCH_IDX + 1}.pt\")\n",
        "        torch.save(pyg_data, save_path)\n",
        "        print(f\"-> âœ… æˆåŠŸ: {len(pyg_data)} ã‚·ãƒ¼ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\")\n",
        "    else:\n",
        "        print(\"-> âš  æˆåŠŸã‚·ãƒ¼ãƒ³ãŒ0ä»¶ã§ã—ãŸã€‚\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"-> âŒ ã‚¨ãƒ©ãƒ¼: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFaosvvG61Tz",
        "outputId": "52762f16-2151-41e2-9044-a900af9ffe57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== ã€å®Ÿè¡Œã€‘ è©¦åˆ 2: DFL-MAT-J03WN1 =====\n",
            "-> âœ… æˆåŠŸ: 434 ã‚·ãƒ¼ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 3. 1è©¦åˆãšã¤æ¤œè¨¼ãƒ»å®Ÿè¡Œãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—\n",
        "# ==========================================\n",
        "raw_data_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Raw_Data\"\n",
        "save_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/matches_v17\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "builder = GNNDataBuilder()\n",
        "\n",
        "# --- ã€ç¢ºå®šè¨­å®šã€‘Match 4 æ•‘æ¸ˆè¨­å®š ---\n",
        "TARGET_MATCH_IDX = 1\n",
        "SET_MANUAL_FLIP = 1.0 # ãƒœãƒ¼ãƒ«æ¨é€²ç‡ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«åè»¢\n",
        "SET_LEFT_TEAM = \"00000B\"\n",
        "\n",
        "info_files = sorted([f for f in os.listdir(raw_data_path) if \"matchinformation\" in f])\n",
        "i_f = info_files[TARGET_MATCH_IDX]\n",
        "match_id_str = i_f.split('_')[-1].replace('.xml', '')\n",
        "\n",
        "print(f\"\\n===== ã€å®Ÿè¡Œã€‘ è©¦åˆ {TARGET_MATCH_IDX + 1}: {match_id_str} =====\")\n",
        "\n",
        "i_path = os.path.join(raw_data_path, i_f)\n",
        "p_path = next(os.path.join(raw_data_path, f) for f in os.listdir(raw_data_path) if match_id_str in f and \"positions_raw\" in f)\n",
        "e_path = next(os.path.join(raw_data_path, f) for f in os.listdir(raw_data_path) if match_id_str in f and \"events_raw\" in f)\n",
        "\n",
        "try:\n",
        "    # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
        "    sheets = dfl_io.read_teamsheets_from_mat_info_xml(i_path)\n",
        "    df_pos = parse_dfl_positions_to_wide(p_path)\n",
        "    events, _, _ = dfl_io.read_event_data_xml(e_path, i_path)\n",
        "\n",
        "    all_events_list = []\n",
        "    for half in events:\n",
        "        for team_label in events[half]:\n",
        "            df_ev = events[half][team_label].events.copy()\n",
        "            df_ev['tID'] = str(sheets[team_label].teamsheet['tID'].iloc[0]).strip()\n",
        "            df_ev['period'] = half\n",
        "            if 'type' in df_ev.columns: df_ev = df_ev.rename(columns={'type': 'eID'})\n",
        "            all_events_list.append(df_ev)\n",
        "    df_event_all = pd.concat(all_events_list)\n",
        "\n",
        "    # 2. å¤‰æ›å®Ÿè¡Œ\n",
        "    team_map = {'Home': list(sheets['Home'].teamsheet['pID']), 'Away': list(sheets['Away'].teamsheet['pID'])}\n",
        "\n",
        "    match_sequences = builder.extract_sequences(\n",
        "        df_pos=df_pos,\n",
        "        df_event=df_event_all,\n",
        "        match_id_idx=TARGET_MATCH_IDX + 1,\n",
        "        manual_flip=SET_MANUAL_FLIP,\n",
        "        left_team_id=SET_LEFT_TEAM\n",
        "    )\n",
        "\n",
        "    pyg_data = builder.to_pyg_data(match_sequences, team_map)\n",
        "\n",
        "    # 3. ä¿å­˜\n",
        "    if pyg_data:\n",
        "        save_path = os.path.join(save_dir, f\"match_{TARGET_MATCH_IDX + 1}.pt\")\n",
        "        torch.save(pyg_data, save_path)\n",
        "        print(f\"-> âœ… æˆåŠŸ: {len(pyg_data)} ã‚·ãƒ¼ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\")\n",
        "    else:\n",
        "        print(\"-> âš  æˆåŠŸã‚·ãƒ¼ãƒ³ãŒ0ä»¶ã§ã—ãŸã€‚\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"-> âŒ ã‚¨ãƒ©ãƒ¼: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka-O1jtv9MIZ",
        "outputId": "d0f56e39-8163-45d6-fd9b-4538041e574c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== ã€å®Ÿè¡Œã€‘ è©¦åˆ 1: DFL-MAT-J03WMX =====\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/floodlight/core/events.py:80: UserWarning: The 'gameclock' column does not match the defined value range (from floodlight.core.definitions). This may lead to unexpected behavior of methods using this column.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> âœ… æˆåŠŸ: 518 ã‚·ãƒ¼ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 3. 1è©¦åˆãšã¤æ¤œè¨¼ãƒ»å®Ÿè¡Œãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—\n",
        "# ==========================================\n",
        "raw_data_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Raw_Data\"\n",
        "save_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/matches_v17\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "builder = GNNDataBuilder()\n",
        "\n",
        "# --- ã€ç¢ºå®šè¨­å®šã€‘Match 4 æ•‘æ¸ˆè¨­å®š ---\n",
        "TARGET_MATCH_IDX = 0\n",
        "SET_MANUAL_FLIP = 1.0 # ãƒœãƒ¼ãƒ«æ¨é€²ç‡ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«åè»¢\n",
        "SET_LEFT_TEAM = \"00000G\"\n",
        "\n",
        "info_files = sorted([f for f in os.listdir(raw_data_path) if \"matchinformation\" in f])\n",
        "i_f = info_files[TARGET_MATCH_IDX]\n",
        "match_id_str = i_f.split('_')[-1].replace('.xml', '')\n",
        "\n",
        "print(f\"\\n===== ã€å®Ÿè¡Œã€‘ è©¦åˆ {TARGET_MATCH_IDX + 1}: {match_id_str} =====\")\n",
        "\n",
        "i_path = os.path.join(raw_data_path, i_f)\n",
        "p_path = next(os.path.join(raw_data_path, f) for f in os.listdir(raw_data_path) if match_id_str in f and \"positions_raw\" in f)\n",
        "e_path = next(os.path.join(raw_data_path, f) for f in os.listdir(raw_data_path) if match_id_str in f and \"events_raw\" in f)\n",
        "\n",
        "try:\n",
        "    # 1. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿\n",
        "    sheets = dfl_io.read_teamsheets_from_mat_info_xml(i_path)\n",
        "    df_pos = parse_dfl_positions_to_wide(p_path)\n",
        "    events, _, _ = dfl_io.read_event_data_xml(e_path, i_path)\n",
        "\n",
        "    all_events_list = []\n",
        "    for half in events:\n",
        "        for team_label in events[half]:\n",
        "            df_ev = events[half][team_label].events.copy()\n",
        "            df_ev['tID'] = str(sheets[team_label].teamsheet['tID'].iloc[0]).strip()\n",
        "            df_ev['period'] = half\n",
        "            if 'type' in df_ev.columns: df_ev = df_ev.rename(columns={'type': 'eID'})\n",
        "            all_events_list.append(df_ev)\n",
        "    df_event_all = pd.concat(all_events_list)\n",
        "\n",
        "    # 2. å¤‰æ›å®Ÿè¡Œ\n",
        "    team_map = {'Home': list(sheets['Home'].teamsheet['pID']), 'Away': list(sheets['Away'].teamsheet['pID'])}\n",
        "\n",
        "    match_sequences = builder.extract_sequences(\n",
        "        df_pos=df_pos,\n",
        "        df_event=df_event_all,\n",
        "        match_id_idx=TARGET_MATCH_IDX + 1,\n",
        "        manual_flip=SET_MANUAL_FLIP,\n",
        "        left_team_id=SET_LEFT_TEAM\n",
        "    )\n",
        "\n",
        "    pyg_data = builder.to_pyg_data(match_sequences, team_map)\n",
        "\n",
        "    # 3. ä¿å­˜\n",
        "    if pyg_data:\n",
        "        save_path = os.path.join(save_dir, f\"match_{TARGET_MATCH_IDX + 1}.pt\")\n",
        "        torch.save(pyg_data, save_path)\n",
        "        print(f\"-> âœ… æˆåŠŸ: {len(pyg_data)} ã‚·ãƒ¼ãƒ³ã‚’ä¿å­˜ã—ã¾ã—ãŸã€‚\")\n",
        "    else:\n",
        "        print(\"-> âš  æˆåŠŸã‚·ãƒ¼ãƒ³ãŒ0ä»¶ã§ã—ãŸã€‚\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"-> âŒ ã‚¨ãƒ©ãƒ¼: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulv3y1QC6zhD"
      },
      "source": [
        "ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã«ã€ãã®ã¾ã¾ä¿å­˜"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuyT715utreo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc199ec-e6f3-4367-ece6-6930d2b39b51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7 è©¦åˆåˆ†ã®çµ±åˆé–‹å§‹...\n",
            " -> match_1.pt: 518 frames loaded. (Marked as Match 1)\n",
            " -> match_2.pt: 434 frames loaded. (Marked as Match 2)\n",
            " -> match_3.pt: 492 frames loaded. (Marked as Match 3)\n",
            " -> match_4.pt: 438 frames loaded. (Marked as Match 4)\n",
            " -> match_5.pt: 442 frames loaded. (Marked as Match 5)\n",
            " -> match_6.pt: 488 frames loaded. (Marked as Match 6)\n",
            " -> match_7.pt: 432 frames loaded. (Marked as Match 7)\n",
            "\n",
            "--- æœ€çµ‚ãƒ‡ãƒ¼ã‚¿æ§‹æˆï¼ˆCVç”¨ãƒ»IDåˆ»å°æ¸ˆã¿ï¼‰ ---\n",
            "ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: 3244\n",
            "å…¨ãƒ‡ãƒ¼ã‚¿å†…è¨³: æˆåŠŸ 264 æš / å¤±æ•— 2980 æš\n",
            "è©¦åˆåˆ¥ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {1: 518, 2: 434, 3: 492, 4: 438, 5: 442, 6: 488, 7: 432}\n",
            "\n",
            "âœ… ä¿å­˜å®Œäº†: /content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_v18_final.pt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "# ==========================================\n",
        "# ã‚»ã‚¯ã‚·ãƒ§ãƒ³2: ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆmatch_idã‚’æ˜ç¤ºçš„ã«ä»˜ä¸ï¼‰\n",
        "# ==========================================\n",
        "save_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/matches_v17\"\n",
        "final_output_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_v18_final.pt\"\n",
        "\n",
        "match_files = sorted([os.path.join(save_dir, f) for f in os.listdir(save_dir) if f.startswith('match_') and f.endswith('.pt')])\n",
        "all_data = []\n",
        "\n",
        "print(f\"{len(match_files)} è©¦åˆåˆ†ã®çµ±åˆé–‹å§‹...\")\n",
        "\n",
        "for i, f in enumerate(match_files):\n",
        "    # ãƒ•ã‚¡ã‚¤ãƒ«åã«é–¢ã‚ã‚‰ãšã€1ã‹ã‚‰å§‹ã¾ã‚‹é€£ç•ªã‚’è©¦åˆIDã¨ã—ã¦ç¢ºå®šã•ã›ã‚‹\n",
        "    current_match_id = i + 1\n",
        "\n",
        "    m_data = torch.load(f, weights_only=False)\n",
        "\n",
        "    # ã€ã“ã“ãŒé‡è¦ï¼ã€‘\n",
        "    # å„ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã« match_id å±æ€§ã‚’è¿½åŠ ã™ã‚‹\n",
        "    for d in m_data:\n",
        "        d.match_id = torch.tensor([current_match_id])\n",
        "\n",
        "    all_data.extend(m_data)\n",
        "    print(f\" -> {os.path.basename(f)}: {len(m_data)} frames loaded. (Marked as Match {current_match_id})\")\n",
        "\n",
        "# ==========================================\n",
        "# ã‚»ã‚¯ã‚·ãƒ§ãƒ³3: ä¿å­˜\n",
        "# ==========================================\n",
        "print(f\"\\n--- æœ€çµ‚ãƒ‡ãƒ¼ã‚¿æ§‹æˆï¼ˆCVç”¨ãƒ»IDåˆ»å°æ¸ˆã¿ï¼‰ ---\")\n",
        "print(f\"ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {len(all_data)}\")\n",
        "\n",
        "all_lbls = Counter([int(d.y.item()) for d in all_data])\n",
        "print(f\"å…¨ãƒ‡ãƒ¼ã‚¿å†…è¨³: æˆåŠŸ {all_lbls[1]} æš / å¤±æ•— {all_lbls[0]} æš\")\n",
        "\n",
        "# è©¦åˆã”ã¨ã®å†…è¨³ã‚‚ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰\n",
        "match_counts = Counter([int(d.match_id.item()) for d in all_data])\n",
        "print(f\"è©¦åˆåˆ¥ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {dict(sorted(match_counts.items()))}\")\n",
        "\n",
        "save_obj = {\n",
        "    'all_data': all_data,\n",
        "    'description': 'v16 integrated data with explicit match_id'\n",
        "}\n",
        "\n",
        "torch.save(save_obj, final_output_path)\n",
        "print(f\"\\nâœ… ä¿å­˜å®Œäº†: {final_output_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP40HZASfAohfipWnZRYPg+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}