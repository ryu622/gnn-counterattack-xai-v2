{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNdCs2mUAPoEM8Z8fcd05sP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryu622/gnn-counterattack-xai-v2/blob/fix%2Ffile-clean/GNN_CounterAttack_Thesis_show_PIGNN_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "論文データでPIGNNを使用して予測してみる"
      ],
      "metadata": {
        "id": "PIaP1CsgB7pp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GNN_CounterAttack_Thesis_show_PIGNN.ipynbにアテンション係数の可視化などを追加"
      ],
      "metadata": {
        "id": "h45zAhVz26cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#シード値\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    # Python自体の乱数固定\n",
        "    random.seed(seed)\n",
        "    # OS環境の乱数固定\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    # Numpyの乱数固定\n",
        "    np.random.seed(seed)\n",
        "    # PyTorchの乱数固定\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # マルチGPUの場合\n",
        "    # 計算の決定論的挙動を強制（これを入れると少し遅くなることがありますが、再現性は完璧になります）\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# 好きな数字（42が一般的）で固定\n",
        "set_seed(44)"
      ],
      "metadata": {
        "id": "_mEGWM5rW5St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdMoTADGeH1c"
      },
      "outputs": [],
      "source": [
        "# --- 1. 環境設定と依存関係のインストール ---\n",
        "# 必要なライブラリをすべてインストール\n",
        "!pip install torch-scatter torch-sparse torch-geometric sklearn tqdm networkx matplotlib\n",
        "!pip install torch-geometric\n",
        "\n",
        "# Google Driveをマウント（ファイルアクセスに必須）\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, Dataset\n",
        "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
        "from torch_geometric.nn import GATConv, global_mean_pool\n",
        "from torch_geometric.utils import dense_to_sparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.calibration import calibration_curve\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pickle\n",
        "import sys\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import logging\n",
        "import copy\n",
        "\n",
        "# ロガー設定\n",
        "logger = logging.getLogger(__name__)\n",
        "if not logger.handlers:\n",
        "    stdout_handler = logging.StreamHandler(sys.stdout)\n",
        "    logger.addHandler(stdout_handler)\n",
        "\n",
        "print(\" STEP 1 完了: 環境設定と依存関係のインポートが完了しました。\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_geometric.utils import softmax, dense_to_sparse\n",
        "from torch_geometric.data import Data, DataLoader as PyGDataLoader\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, recall_score\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================\n",
        "# 1. PIGNNモデルの定義 (以前のミスを修正した完成版)\n",
        "# ==========================================\n",
        "class PIGNNLayer(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, tau=0.015):\n",
        "        super(PIGNNLayer, self).__init__(aggr='add')\n",
        "        self.tau = tau\n",
        "        self.lin = nn.Linear(in_channels, out_channels)\n",
        "        self.att = nn.Parameter(torch.Tensor(1, out_channels * 2))\n",
        "        nn.init.xavier_uniform_(self.att)\n",
        "\n",
        "    def forward(self, x, edge_index, pos, vel, return_attention=False):\n",
        "        h = self.lin(x)\n",
        "        out, alpha = self.propagate(edge_index, x=h, pos=pos, vel=vel)\n",
        "        if return_attention: return out, (edge_index, alpha)\n",
        "        return out\n",
        "\n",
        "    def message(self, x_i, x_j, pos_i, pos_j, vel_i, vel_j, edge_index_i):\n",
        "        # 物理バイアス：0-1スケールの座標系に合わせ、分母を0.05(実寸約5m)に設定\n",
        "        pos_i_pred = pos_i + vel_i * self.tau\n",
        "        pos_j_pred = pos_j + vel_j * self.tau\n",
        "        dist_future = torch.norm(pos_i_pred - pos_j_pred, dim=-1, keepdim=True)\n",
        "        physics_bias = torch.exp(-dist_future / 0.05)\n",
        "\n",
        "        alpha = torch.cat([x_i, x_j], dim=-1)\n",
        "        alpha = (alpha * self.att).sum(dim=-1, keepdim=True)\n",
        "        alpha = F.leaky_relu(alpha) + physics_bias\n",
        "        alpha = softmax(alpha, edge_index_i)\n",
        "        return alpha * x_j, alpha\n",
        "\n",
        "    def aggregate(self, inputs, index, ptr=None, dim_size=None):\n",
        "        out = super().aggregate(inputs[0], index, ptr, dim_size)\n",
        "        return out, inputs[1]\n",
        "\n",
        "class PIGNNClassifier(nn.Module):\n",
        "    def __init__(self, in_channels=12, hidden_channels=64):\n",
        "        super(PIGNNClassifier, self).__init__()\n",
        "        self.conv1 = PIGNNLayer(in_channels, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data, return_attention=False):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "        if return_attention:\n",
        "            x, att_weights = self.conv1(x, edge_index, pos, vel, return_attention=True)\n",
        "        else:\n",
        "            x = self.conv1(x, edge_index, pos, vel)\n",
        "        x = F.elu(x)\n",
        "        x = self.conv2(x, edge_index, pos, vel)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        out = F.log_softmax(self.lin(x), dim=1)\n",
        "        return (out, att_weights) if return_attention else out\n"
      ],
      "metadata": {
        "id": "PeZVmxW4HfSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. データロードと変換 (KeyErrorを完全回避)\n",
        "# ==========================================\n",
        "print(\" データを読み込み中...\")\n",
        "file_path = '/content/drive/MyDrive/GNN_Football_Analysis/Raw_Data/women.pkl'\n",
        "with open(file_path, 'rb') as handle:\n",
        "    raw = pickle.load(handle)\n",
        "\n",
        "pignn_dataset = []\n",
        "xs = raw['normal']['x']\n",
        "as_ = raw['normal']['a']\n",
        "ys = raw['binary']\n",
        "\n",
        "for i in tqdm(range(len(xs)), desc=\"PyGデータへ変換\"):\n",
        "    try:\n",
        "        x_np = xs[i][:, :12] # 12次元を確実に抽出\n",
        "        if hasattr(as_[i], 'todense'): a_np = as_[i].todense()\n",
        "        else: a_np = as_[i]\n",
        "\n",
        "        x = torch.tensor(x_np, dtype=torch.float)\n",
        "        edge_index, _ = dense_to_sparse(torch.tensor(a_np, dtype=torch.float))\n",
        "        y = torch.tensor(ys[i], dtype=torch.long)\n",
        "\n",
        "        # PIGNN専用：0,1をpos、2,3をvelとして保存\n",
        "        pos, vel = x[:, 0:2].clone(), x[:, 2:4].clone()\n",
        "        pignn_dataset.append(Data(x=x, edge_index=edge_index, y=y, pos=pos, vel=vel))\n",
        "    except: continue\n",
        "\n",
        "# 分割 (シャッフル & 層化抽出でSuccess 0を防止)\n",
        "train_data, test_data = train_test_split(\n",
        "    pignn_dataset, test_size=0.3, random_state=42, stratify=[int(d.y) for d in pignn_dataset]\n",
        ")\n",
        "\n",
        "train_loader = PyGDataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = PyGDataLoader(test_data, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\" 準備完了。訓練成功数: {sum([int(d.y) for d in train_data])}, テスト成功数: {sum([int(d.y) for d in test_data])}\")"
      ],
      "metadata": {
        "id": "tS-sh95cHjnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. 学習の実行 (物理損失 Phys_L 適用)\n",
        "# ==========================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = PIGNNClassifier(in_channels=12).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "alpha_p = 0.05\n",
        "\n",
        "print(\"\\n PIGNN学習開始...\")\n",
        "for epoch in range(1,101):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for d in train_loader:\n",
        "        d = d.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(d)\n",
        "\n",
        "        # 分類損失\n",
        "        ce_loss = F.nll_loss(out, d.y)\n",
        "        # 物理損失 (成功予測 × 後退速度)\n",
        "        probs = torch.exp(out)[:, 1]\n",
        "        atk_mask = (d.x[:, 10] == 1.0) # Attacking Team Flag\n",
        "        vx = d.vel[:, 0]\n",
        "        phys_loss = torch.mean(probs[d.batch[atk_mask]] * F.relu(-vx[atk_mask]))\n",
        "\n",
        "        loss = ce_loss + alpha_p * phys_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:02d} | Loss: {total_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "lAPsHNMqHoJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. 最終評価 (Classification Report)\n",
        "# ==========================================\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for d in test_loader:\n",
        "        d = d.to(device)\n",
        "        out = model(d)\n",
        "        y_true.extend(d.y.cpu().numpy())\n",
        "        y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "print(\"\\n --- Final Report ---\")\n",
        "print(classification_report(y_true, y_pred, target_names=['Failure', 'Success']))"
      ],
      "metadata": {
        "id": "c4pfwQnmHrjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = PIGNNClassifier(in_channels=12).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 記録用のリスト\n",
        "history = {'total_loss': [], 'ce_loss': [], 'phys_loss': []}\n",
        "\n",
        "print(\"\\n 物理損失を記録しながら学習開始...\")\n",
        "epochs = 50 # 数値を伸ばすために50回に増やします\n",
        "\n",
        "\n",
        "alpha_p = 100.0\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    epoch_total = 0\n",
        "    epoch_ce = 0\n",
        "    epoch_phys = 0\n",
        "\n",
        "    for d in train_loader:\n",
        "        d = d.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(d)\n",
        "\n",
        "        # 1. 分類損失\n",
        "        ce_loss = F.nll_loss(out, d.y)\n",
        "\n",
        "        # 2. 物理損失 (成功予測 × 後退速度)\n",
        "        probs = torch.exp(out)[:, 1]\n",
        "        atk_mask = (d.x[:, 10] == 1.0)\n",
        "        vx = d.vel[:, 0]\n",
        "        # 物理的に「ありえない（後ろに走っているのに成功）」と予測すると大きくなる損失\n",
        "        phys_loss = torch.mean(probs[d.batch[atk_mask]] * F.relu(-vx[atk_mask]))\n",
        "\n",
        "        # トータル\n",
        "        loss = ce_loss + (alpha_p * phys_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_total += loss.item()\n",
        "        epoch_ce += ce_loss.item()\n",
        "        epoch_phys += phys_loss.item()\n",
        "\n",
        "    # 平均を保存\n",
        "    history['total_loss'].append(epoch_total / len(train_loader))\n",
        "    history['ce_loss'].append(epoch_ce / len(train_loader))\n",
        "    history['phys_loss'].append(epoch_phys / len(train_loader))\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:02d} | Total: {history['total_loss'][-1]:.4f} | Phys: {history['phys_loss'][-1]:.4f}\")\n",
        "\n",
        "# --- 可視化コード ---\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# グラフ1: 分類損失とトータル損失\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['total_loss'], label='Total Loss')\n",
        "plt.plot(history['ce_loss'], label='CE Loss (Classification)')\n",
        "plt.title('Learning Curve (Classification)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# グラフ2: 物理損失（ここが卒論のポイント！）\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['phys_loss'], color='red', label='Physics Loss (Phys_L)')\n",
        "plt.title('Physics-Informed Loss Consistency')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Phys_L Value')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9M5alHtrIsOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def final_evaluate_pignn(model, loader, device):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    all_probs = []\n",
        "    all_vxs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in loader:\n",
        "            d = d.to(device)\n",
        "            out = model(d)\n",
        "            # 成功確率（expしてlog_softmaxを確率に戻す）\n",
        "            probs = torch.exp(out)[:, 1]\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            y_true.extend(d.y.cpu().numpy())\n",
        "            y_pred.extend(pred.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "            # 各グラフの攻撃側(AtkFlag)の代表的な速度（平均など）を記録\n",
        "            # 物理制約が効いているか確認するため\n",
        "            for i in range(len(d.ptr)-1):\n",
        "                mask = (d.batch == i) & (d.x[:, 10] == 1.0)\n",
        "                if mask.any():\n",
        "                    avg_vx = d.vel[mask, 0].mean().item()\n",
        "                    all_vxs.append(avg_vx)\n",
        "                else:\n",
        "                    all_vxs.append(0)\n",
        "\n",
        "    # 1. クラシフィケーションレポート\n",
        "    print(\"\\n --- PIGNN Final Classification Report ---\")\n",
        "    print(classification_report(y_true, y_pred, target_names=['Failure', 'Success'], zero_division=0))\n",
        "\n",
        "    # 2. 混同行列\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Pred Failure', 'Pred Success'],\n",
        "                yticklabels=['Actual Failure', 'Actual Success'])\n",
        "    plt.title('Confusion Matrix')\n",
        "\n",
        "    # 3. 物理的整合性の可視化（卒論の目玉）\n",
        "    # 成功と予測したシーンの「速度」がどうなっているか\n",
        "    plt.subplot(1, 2, 2)\n",
        "    pred_success_vxs = [v for p, v in zip(y_pred, all_vxs) if p == 1]\n",
        "    plt.hist(pred_success_vxs, bins=30, color='red', alpha=0.7)\n",
        "    plt.axvline(x=0.038, color='black', linestyle='--', label='Threshold')\n",
        "    plt.title('Velocity Distribution of \"Pred Success\"')\n",
        "    plt.xlabel('Average Forward Velocity (vx)')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 実行\n",
        "final_evaluate_pignn(model, test_loader, device)"
      ],
      "metadata": {
        "id": "SNgdhEOKTfpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = PIGNNClassifier(in_channels=12).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 記録用のリスト\n",
        "history = {'total_loss': [], 'ce_loss': [], 'phys_loss': []}\n",
        "\n",
        "print(\"\\n 物理損失を記録しながら学習開始...\")\n",
        "epochs = 50 # 数値を伸ばすために50回に増やします\n",
        "\n",
        "\n",
        "alpha_p = 0\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    epoch_total = 0\n",
        "    epoch_ce = 0\n",
        "    epoch_phys = 0\n",
        "\n",
        "    for d in train_loader:\n",
        "        d = d.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(d)\n",
        "\n",
        "        # 1. 分類損失\n",
        "        ce_loss = F.nll_loss(out, d.y)\n",
        "\n",
        "        # 2. 物理損失 (成功予測 × 後退速度)\n",
        "        probs = torch.exp(out)[:, 1]\n",
        "        atk_mask = (d.x[:, 10] == 1.0)\n",
        "        vx = d.vel[:, 0]\n",
        "        # 物理的に「ありえない（後ろに走っているのに成功）」と予測すると大きくなる損失\n",
        "        phys_loss = torch.mean(probs[d.batch[atk_mask]] * F.relu(-vx[atk_mask]))\n",
        "\n",
        "        # トータル\n",
        "        loss = ce_loss + (alpha_p * phys_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_total += loss.item()\n",
        "        epoch_ce += ce_loss.item()\n",
        "        epoch_phys += phys_loss.item()\n",
        "\n",
        "    # 平均を保存\n",
        "    history['total_loss'].append(epoch_total / len(train_loader))\n",
        "    history['ce_loss'].append(epoch_ce / len(train_loader))\n",
        "    history['phys_loss'].append(epoch_phys / len(train_loader))\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:02d} | Total: {history['total_loss'][-1]:.4f} | Phys: {history['phys_loss'][-1]:.4f}\")\n",
        "\n",
        "# --- 可視化コード ---\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# グラフ1: 分類損失とトータル損失\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['total_loss'], label='Total Loss')\n",
        "plt.plot(history['ce_loss'], label='CE Loss (Classification)')\n",
        "plt.title('Learning Curve (Classification)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# グラフ2: 物理損失（ここが卒論のポイント！）\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['phys_loss'], color='red', label='Physics Loss (Phys_L)')\n",
        "plt.title('Physics-Informed Loss Consistency')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Phys_L Value')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4y4FmIlETynw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def final_evaluate_pignn(model, loader, device):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    all_probs = []\n",
        "    all_vxs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in loader:\n",
        "            d = d.to(device)\n",
        "            out = model(d)\n",
        "            # 成功確率（expしてlog_softmaxを確率に戻す）\n",
        "            probs = torch.exp(out)[:, 1]\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            y_true.extend(d.y.cpu().numpy())\n",
        "            y_pred.extend(pred.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "            # 各グラフの攻撃側(AtkFlag)の代表的な速度（平均など）を記録\n",
        "            # 物理制約が効いているか確認するため\n",
        "            for i in range(len(d.ptr)-1):\n",
        "                mask = (d.batch == i) & (d.x[:, 10] == 1.0)\n",
        "                if mask.any():\n",
        "                    avg_vx = d.vel[mask, 0].mean().item()\n",
        "                    all_vxs.append(avg_vx)\n",
        "                else:\n",
        "                    all_vxs.append(0)\n",
        "\n",
        "    # 1. クラシフィケーションレポート\n",
        "    print(\"\\n --- PIGNN Final Classification Report ---\")\n",
        "    print(classification_report(y_true, y_pred, target_names=['Failure', 'Success'], zero_division=0))\n",
        "\n",
        "    # 2. 混同行列\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Pred Failure', 'Pred Success'],\n",
        "                yticklabels=['Actual Failure', 'Actual Success'])\n",
        "    plt.title('Confusion Matrix')\n",
        "\n",
        "    # 3. 物理的整合性の可視化（卒論の目玉）\n",
        "    # 成功と予測したシーンの「速度」がどうなっているか\n",
        "    plt.subplot(1, 2, 2)\n",
        "    pred_success_vxs = [v for p, v in zip(y_pred, all_vxs) if p == 1]\n",
        "    plt.hist(pred_success_vxs, bins=30, color='red', alpha=0.7)\n",
        "    plt.axvline(x=0.038, color='black', linestyle='--', label='Threshold')\n",
        "    plt.title('Velocity Distribution of \"Pred Success\"')\n",
        "    plt.xlabel('Average Forward Velocity (vx)')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 実行\n",
        "final_evaluate_pignn(model, test_loader, device)"
      ],
      "metadata": {
        "id": "jYJedKsYT22E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = PIGNNClassifier(in_channels=12).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 記録用のリスト\n",
        "history = {'total_loss': [], 'ce_loss': [], 'phys_loss': []}\n",
        "\n",
        "print(\"\\n 物理損失を記録しながら学習開始...\")\n",
        "epochs = 50 # 数値を伸ばすために50回に増やします\n",
        "\n",
        "\n",
        "alpha_p = 10.0\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    epoch_total = 0\n",
        "    epoch_ce = 0\n",
        "    epoch_phys = 0\n",
        "\n",
        "    for d in train_loader:\n",
        "        d = d.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(d)\n",
        "\n",
        "        # 1. 分類損失\n",
        "        ce_loss = F.nll_loss(out, d.y)\n",
        "\n",
        "        # 2. 物理損失 (成功予測 × 後退速度)\n",
        "        probs = torch.exp(out)[:, 1]\n",
        "        atk_mask = (d.x[:, 10] == 1.0)\n",
        "        vx = d.vel[:, 0]\n",
        "        # 物理的に「ありえない（後ろに走っているのに成功）」と予測すると大きくなる損失\n",
        "        phys_loss = torch.mean(probs[d.batch[atk_mask]] * F.relu(-vx[atk_mask]))\n",
        "\n",
        "        # トータル\n",
        "        loss = ce_loss + (alpha_p * phys_loss)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_total += loss.item()\n",
        "        epoch_ce += ce_loss.item()\n",
        "        epoch_phys += phys_loss.item()\n",
        "\n",
        "    # 平均を保存\n",
        "    history['total_loss'].append(epoch_total / len(train_loader))\n",
        "    history['ce_loss'].append(epoch_ce / len(train_loader))\n",
        "    history['phys_loss'].append(epoch_phys / len(train_loader))\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:02d} | Total: {history['total_loss'][-1]:.4f} | Phys: {history['phys_loss'][-1]:.4f}\")\n",
        "\n",
        "# --- 可視化コード ---\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# グラフ1: 分類損失とトータル損失\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history['total_loss'], label='Total Loss')\n",
        "plt.plot(history['ce_loss'], label='CE Loss (Classification)')\n",
        "plt.title('Learning Curve (Classification)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# グラフ2: 物理損失（ここが卒論のポイント！）\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history['phys_loss'], color='red', label='Physics Loss (Phys_L)')\n",
        "plt.title('Physics-Informed Loss Consistency')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Phys_L Value')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4KsaPTk9T-ZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def final_evaluate_pignn(model, loader, device):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    all_probs = []\n",
        "    all_vxs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for d in loader:\n",
        "            d = d.to(device)\n",
        "            out = model(d)\n",
        "            # 成功確率（expしてlog_softmaxを確率に戻す）\n",
        "            probs = torch.exp(out)[:, 1]\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            y_true.extend(d.y.cpu().numpy())\n",
        "            y_pred.extend(pred.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "            # 各グラフの攻撃側(AtkFlag)の代表的な速度（平均など）を記録\n",
        "            # 物理制約が効いているか確認するため\n",
        "            for i in range(len(d.ptr)-1):\n",
        "                mask = (d.batch == i) & (d.x[:, 10] == 1.0)\n",
        "                if mask.any():\n",
        "                    avg_vx = d.vel[mask, 0].mean().item()\n",
        "                    all_vxs.append(avg_vx)\n",
        "                else:\n",
        "                    all_vxs.append(0)\n",
        "\n",
        "    # 1. クラシフィケーションレポート\n",
        "    print(\"\\n --- PIGNN Final Classification Report ---\")\n",
        "    print(classification_report(y_true, y_pred, target_names=['Failure', 'Success'], zero_division=0))\n",
        "\n",
        "    # 2. 混同行列\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Pred Failure', 'Pred Success'],\n",
        "                yticklabels=['Actual Failure', 'Actual Success'])\n",
        "    plt.title('Confusion Matrix')\n",
        "\n",
        "    # 3. 物理的整合性の可視化（卒論の目玉）\n",
        "    # 成功と予測したシーンの「速度」がどうなっているか\n",
        "    plt.subplot(1, 2, 2)\n",
        "    pred_success_vxs = [v for p, v in zip(y_pred, all_vxs) if p == 1]\n",
        "    plt.hist(pred_success_vxs, bins=30, color='red', alpha=0.7)\n",
        "    plt.axvline(x=0.038, color='black', linestyle='--', label='Threshold')\n",
        "    plt.title('Velocity Distribution of \"Pred Success\"')\n",
        "    plt.xlabel('Average Forward Velocity (vx)')\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 実行\n",
        "final_evaluate_pignn(model, test_loader, device)"
      ],
      "metadata": {
        "id": "FFKN9upGUEnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "# 1. 初期状態の重みを保存（全実験を同じスタート地点にするため）\n",
        "save_dir = '/content/drive/MyDrive/GNN_Football_Analysis/Models'\n",
        "initial_model = PIGNNClassifier(in_channels=12).to(device)\n",
        "torch.save(initial_model.state_dict(), os.path.join(save_dir, 'initial_weights.pth'))\n",
        "print(f\" 初期重みを保存しました: {save_dir}/initial_weights.pth\")\n",
        "\n",
        "# 実験したい alpha_p のリスト\n",
        "alpha_list = [0.0, 10.0, 100.0]\n",
        "# 各実験の結果を格納する辞書\n",
        "all_histories = {}\n",
        "\n",
        "for ap in alpha_list:\n",
        "    print(f\"\\n基準物理重み alpha_p = {ap} で学習開始...\")\n",
        "\n",
        "    # モデルを初期状態に戻す\n",
        "    model = PIGNNClassifier(in_channels=12).to(device)\n",
        "    #model.load_state_dict(torch.load('initial_weights.pth'))\n",
        "    load_path = os.path.join(save_dir, 'initial_weights.pth') # さっき定義した save_dir を使う\n",
        "    model.load_state_dict(torch.load(load_path))\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    history = {'total_loss': [], 'ce_loss': [], 'phys_loss': []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        epoch_total, epoch_ce, epoch_phys = 0, 0, 0\n",
        "\n",
        "        for d in train_loader:\n",
        "            d = d.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(d)\n",
        "\n",
        "            # 損失計算\n",
        "            ce_loss = F.nll_loss(out, d.y)\n",
        "            probs = torch.exp(out)[:, 1]\n",
        "            atk_mask = (d.x[:, 10] == 1.0)\n",
        "            vx = d.vel[:, 0]\n",
        "\n",
        "            # 物理損失（※前進を促すため 0.038 のしきい値と20倍ブーストを推奨）\n",
        "            phys_val = torch.mean(probs[d.batch[atk_mask]] * F.relu(0.038 - vx[atk_mask]))\n",
        "\n",
        "            loss = ce_loss + (ap * phys_val * 20) # ap=0の時はここが0になる\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_total += loss.item()\n",
        "            epoch_ce += ce_loss.item()\n",
        "            epoch_phys += phys_val.item()\n",
        "\n",
        "        history['total_loss'].append(epoch_total / len(train_loader))\n",
        "        history['ce_loss'].append(epoch_ce / len(train_loader))\n",
        "        history['phys_loss'].append(epoch_phys / len(train_loader))\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f\"Epoch {epoch:02d} | Total: {history['total_loss'][-1]:.4f}\")\n",
        "\n",
        "    # --- 学習済みモデルと履歴を保存 ---\n",
        "    model_save_path = os.path.join(save_dir, f'model_alpha_{ap}.pth')\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\" alpha_p={ap} のモデルを保存しました: {model_save_path}\")\n"
      ],
      "metadata": {
        "id": "hM8RO_lnY6B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. 評価用関数の定義\n",
        "def evaluate_saved_models(model_paths, loader, device):\n",
        "    results = []\n",
        "    for ap, path in model_paths.items():\n",
        "        print(f\" alpha_p={ap} のモデルを読み込み中...\")\n",
        "        m = PIGNNClassifier(in_channels=12).to(device)\n",
        "        m.load_state_dict(torch.load(path))\n",
        "        m.eval()\n",
        "\n",
        "        y_true, y_pred = [], []\n",
        "        with torch.no_grad():\n",
        "            for d in loader:\n",
        "                d = d.to(device)\n",
        "                out = m(d)\n",
        "                y_true.extend(d.y.cpu().numpy())\n",
        "                y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "        results.append({'Alpha_p': ap, 'Test_Accuracy': acc})\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# 2. Drive内のモデルパスを指定（さっき保存した場所）\n",
        "model_paths = {\n",
        "    0.0: os.path.join(save_dir, 'model_alpha_0.0.pth'),\n",
        "    10.0: os.path.join(save_dir, 'model_alpha_10.0.pth'),\n",
        "    100.0: os.path.join(save_dir, 'model_alpha_100.0.pth')\n",
        "}\n",
        "\n",
        "# 3. 実行して表を表示\n",
        "df_results = evaluate_saved_models(model_paths, test_loader, device)\n",
        "\n",
        "print(\"\\n 【卒論用：最終結果比較表】\")\n",
        "print(df_results)\n",
        "\n",
        "# 4. ついでにグラフも出す\n",
        "df_results.plot(x='Alpha_p', y='Test_Accuracy', kind='bar', color='skyblue', legend=False)\n",
        "plt.title('Final Accuracy Comparison')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1.0)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QfXmmecUa7bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 履歴格納用の辞書を「ループの外」で定義\n",
        "all_histories = {}\n",
        "\n",
        "for ap in alpha_list:\n",
        "    print(f\"\\n alpha_p = {ap} で学習開始（全 {epochs} エポック）...\")\n",
        "\n",
        "    # 初期状態に戻す\n",
        "    model = PIGNNClassifier(in_channels=12).to(device)\n",
        "    model.load_state_dict(torch.load(os.path.join(save_dir, 'initial_weights.pth')))\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # この alpha_p 専用の履歴リスト\n",
        "    history = {'total_loss': [], 'ce_loss': [], 'phys_loss': []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        epoch_phys = 0\n",
        "        for d in train_loader:\n",
        "            d = d.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(d)\n",
        "            ce_loss = F.nll_loss(out, d.y)\n",
        "\n",
        "            # 物理損失の計算\n",
        "            probs = torch.exp(out)[:, 1]\n",
        "            atk_mask = (d.x[:, 10] == 1.0)\n",
        "            vx = d.vel[:, 0]\n",
        "            phys_val = torch.mean(probs[d.batch[atk_mask]] * F.relu(0.038 - vx[atk_mask]))\n",
        "\n",
        "            loss = ce_loss + (ap * phys_val * 20)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_phys += phys_val.item()\n",
        "\n",
        "        # エポックごとの平均を記録\n",
        "        history['phys_loss'].append(epoch_phys / len(train_loader))\n",
        "        if epoch % 10 == 0: print(f\"Epoch {epoch} done.\")\n",
        "\n",
        "    #  重要：float(ap)をキーにして辞書に保存\n",
        "    all_histories[float(ap)] = history\n",
        "\n",
        "\n",
        "# --- 以前と同じ折れ線グラフの描画 ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for ap in alpha_list:\n",
        "    # 記録した履歴を取り出してプロット\n",
        "    h = all_histories.get(float(ap))\n",
        "    if h is not None:\n",
        "        plt.plot(h['phys_loss'], label=f'alpha_p={ap}')\n",
        "\n",
        "plt.title('Physics Loss (Phys_L) Transition during Training', fontsize=14)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Phys_L Value', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True, which='both', linestyle='--', alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8_rR2cMQbryV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_pignn_miracle_scenes(model_0, model_phys, loader, device):\n",
        "    \"\"\"\n",
        "    物理なし(model_0)が失敗し、物理あり(model_phys)が正解した「奇跡のシーン」を検索する。\n",
        "    \"\"\"\n",
        "    model_0.eval()\n",
        "    model_phys.eval()\n",
        "\n",
        "    miracle_results = []\n",
        "\n",
        "    print(\" テストデータをスキャンして、物理制約が予測を改善したシーンを探しています...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, d in enumerate(loader):\n",
        "            d = d.to(device)\n",
        "\n",
        "            # 各モデルの予測（対数確率）を取得\n",
        "            out_0 = model_0(d)\n",
        "            out_phys = model_phys(d)\n",
        "\n",
        "            # 予測クラス (0: Failure, 1: Success)\n",
        "            pred_0 = out_0.argmax(dim=1)\n",
        "            pred_phys = out_phys.argmax(dim=1)\n",
        "\n",
        "            # 成功確率（0.0 ~ 1.0）\n",
        "            prob_0 = torch.exp(out_0)[:, 1]\n",
        "            prob_phys = torch.exp(out_phys)[:, 1]\n",
        "\n",
        "            for i in range(len(d.y)):\n",
        "                label = d.y[i].item()\n",
        "                p0 = pred_0[i].item()\n",
        "                pp = pred_phys[i].item()\n",
        "\n",
        "                # --- 狙い撃ち条件 ---\n",
        "                # 実際は成功(1) であり、\n",
        "                # 物理なし(p0)は「失敗(0)」と予測し、\n",
        "                # 物理あり(pp)が「成功(1)」と正解したシーン\n",
        "                if label == 1 and p0 == 0 and pp == 1:\n",
        "                    # 改善度合い（確率の差）を計算\n",
        "                    improvement = prob_phys[i].item() - prob_0[i].item()\n",
        "\n",
        "                    miracle_results.append({\n",
        "                        'batch_idx': batch_idx,\n",
        "                        'item_idx': i,\n",
        "                        'improvement': improvement,\n",
        "                        'prob_baseline': prob_0[i].item(),\n",
        "                        'prob_proposed': prob_phys[i].item()\n",
        "                    })\n",
        "\n",
        "    # 改善幅が大きい順（よりドラマチックな変化）にソート\n",
        "    miracle_results = sorted(miracle_results, key=lambda x: x['improvement'], reverse=True)\n",
        "\n",
        "    return miracle_results\n"
      ],
      "metadata": {
        "id": "1Mu3X6jfdCUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 比較用の空のモデル器を作成\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_0 = PIGNNClassifier(in_channels=12).to(device)\n",
        "model_10 = PIGNNClassifier(in_channels=12).to(device)\n",
        "\n",
        "# 2. 正しいパス（save_dir）を指定してロード\n",
        "# すでに save_dir = '/content/drive/MyDrive/GNN_Football_Analysis/Models' と定義されている前提です\n",
        "model_0.load_state_dict(torch.load(os.path.join(save_dir, 'model_alpha_0.0.pth')))\n",
        "model_10.load_state_dict(torch.load(os.path.join(save_dir, 'model_alpha_10.0.pth')))\n",
        "\n",
        "print(\" Driveからモデルのロードに成功しました。\")\n",
        "\n",
        "# 3. その後で「狙い撃ち」を実行\n",
        "miracle_list = find_pignn_miracle_scenes(model_0, model_10,test_loader, device)\n",
        "print(f\"狙い撃ち可能なシーン数: {len(miracle_list)}\")"
      ],
      "metadata": {
        "id": "aFAKk4fsXdbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def visualize_at_miracle_scene(model, data_item, title=\"PIGNN Analysis\"):\n",
        "    model.eval()\n",
        "    data_item = data_item.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out, (edge_index, att_weights) = model(data_item, return_attention=True)\n",
        "        prob = torch.exp(out)[0, 1].item()\n",
        "\n",
        "    # 座標はデータそのもの（既に0~1）を使用\n",
        "    pos = data_item.pos.cpu().numpy()\n",
        "    vel = data_item.vel.cpu().numpy()\n",
        "    atk_flag = data_item.x[:, 10].cpu().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # --- 1. ピッチの描画 (完全に 0.0 ~ 1.0 の端から端まで) ---\n",
        "    # axis('off')対策：背景を緑の長方形で埋める\n",
        "    ax.add_patch(patches.Rectangle((-0.05, -0.05), 1.1, 1.1, facecolor='#2e7d32', zorder=0))\n",
        "\n",
        "    # コートの端を 0.0 と 1.0 に完全に一致させる\n",
        "    ax.add_patch(patches.Rectangle((0, 0), 1, 1, edgecolor=\"white\", facecolor=\"none\", linewidth=3, zorder=1))\n",
        "\n",
        "    # センターライン (0.5)\n",
        "    ax.plot([0.5, 0.5], [0, 1], color=\"white\", linewidth=3, zorder=1)\n",
        "\n",
        "    # センターサークル (半径はピッチサイズに対して 0.0915 が国際規格比)\n",
        "    ax.add_patch(patches.Circle((0.5, 0.5), 0.0915, edgecolor=\"white\", facecolor=\"none\", linewidth=3, zorder=1))\n",
        "\n",
        "    # ペナルティエリア (端は 0 と 1 に接する)\n",
        "    ax.add_patch(patches.Rectangle((0, 0.2), 0.165, 0.6, edgecolor=\"white\", facecolor=\"none\", linewidth=2, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((1-0.165, 0.2), 0.165, 0.6, edgecolor=\"white\", facecolor=\"none\", linewidth=2, zorder=1))\n",
        "\n",
        "    # --- 2. アテンション（黄色い線）の描画 ---\n",
        "    att_weights = att_weights.cpu().numpy().flatten()\n",
        "    edge_index = edge_index.cpu().numpy()\n",
        "\n",
        "    if len(att_weights) > 0:\n",
        "        threshold = np.percentile(att_weights, 90) # 上位10%\n",
        "        max_att = att_weights.max()\n",
        "        for i in range(edge_index.shape[1]):\n",
        "            if att_weights[i] > threshold:\n",
        "                src, dst = edge_index[0, i], edge_index[1, i]\n",
        "                if atk_flag[src] == 1:\n",
        "                    alpha = min(1.0, (att_weights[i] / max_att))\n",
        "                    ax.plot([pos[src, 0], pos[dst, 0]], [pos[src, 1], pos[dst, 1]],\n",
        "                            color=\"#FFFF00\", alpha=alpha, linewidth=4, zorder=2)\n",
        "\n",
        "    # --- 3. 選手と「黒」矢印の描画 ---\n",
        "    for k in range(len(pos)):\n",
        "        color = 'red' if atk_flag[k] == 1 else 'blue'\n",
        "        ax.scatter(pos[k, 0], pos[k, 1], c=color, s=250, edgecolors='white', zorder=4)\n",
        "        # 黒色の矢印\n",
        "        ax.quiver(pos[k, 0], pos[k, 1], vel[k, 0], vel[k, 1],\n",
        "                  color='black', angles='xy', scale_units='xy', scale=10, width=0.007, zorder=5)\n",
        "\n",
        "    ax.set_title(f\"{title}\\nSuccess Prob: {prob:.4f}\", fontsize=20, pad=20)\n",
        "    ax.set_xlim(-0.02, 1.02)\n",
        "    ax.set_ylim(-0.02, 1.02)\n",
        "    ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "icbXcuGug1vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def visualize_at_miracle_scene(model, data_item, title=\"PIGNN Analysis\"):\n",
        "    model.eval()\n",
        "    data_item = data_item.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out, (edge_index, att_weights) = model(data_item, return_attention=True)\n",
        "        prob = torch.exp(out)[0, 1].item()\n",
        "\n",
        "    # 座標は0~1正規化済みデータをそのまま使用\n",
        "    pos = data_item.pos.cpu().numpy()\n",
        "    vel = data_item.vel.cpu().numpy()\n",
        "    atk_flag = data_item.x[:, 10].cpu().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # --- 1. ピッチ背景と白線の描画 (0.0 ~ 1.0) ---\n",
        "    ax.add_patch(patches.Rectangle((-0.05, -0.05), 1.1, 1.1, facecolor='#2e7d32', zorder=0))\n",
        "    ax.add_patch(patches.Rectangle((0, 0), 1, 1, edgecolor=\"white\", facecolor=\"none\", linewidth=3, zorder=1))\n",
        "    ax.plot([0.5, 0.5], [0, 1], color=\"white\", linewidth=3, zorder=1)\n",
        "    ax.add_patch(patches.Circle((0.5, 0.5), 0.0915, edgecolor=\"white\", facecolor=\"none\", linewidth=3, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((0, 0.2), 0.165, 0.6, edgecolor=\"white\", facecolor=\"none\", linewidth=2, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((0.835, 0.2), 0.165, 0.6, edgecolor=\"white\", facecolor=\"none\", linewidth=2, zorder=1))\n",
        "\n",
        "    # --- 2. アテンション（黄色い線）の描画 ---\n",
        "    att_weights = att_weights.cpu().numpy().flatten()\n",
        "    edge_index = edge_index.cpu().numpy()\n",
        "    if len(att_weights) > 0:\n",
        "        threshold = np.percentile(att_weights, 90)\n",
        "        max_att = att_weights.max()\n",
        "        for i in range(edge_index.shape[1]):\n",
        "            if att_weights[i] > threshold:\n",
        "                src, dst = edge_index[0, i], edge_index[1, i]\n",
        "                # ボール(22)への注目、または攻撃側(atk_flag)からの注目を描画\n",
        "                if src < 22 and atk_flag[src] == 1:\n",
        "                    alpha = min(1.0, (att_weights[i] / max_att))\n",
        "                    ax.plot([pos[src, 0], pos[dst, 0]], [pos[src, 1], pos[dst, 1]],\n",
        "                            color=\"#FFFF00\", alpha=alpha, linewidth=4, zorder=2)\n",
        "\n",
        "    # --- 3. 選手とボールの描画 ---\n",
        "    for k in range(len(pos)):\n",
        "        if k < 22:  # 選手(0~21)\n",
        "            color = 'red' if k < 11 else 'blue' # 前半11人が赤、後半11人が青\n",
        "            ax.scatter(pos[k, 0], pos[k, 1], c=color, s=250, edgecolors='white', zorder=4)\n",
        "            # 黒色の速度ベクトル\n",
        "            ax.quiver(pos[k, 0], pos[k, 1], vel[k, 0], vel[k, 1],\n",
        "                      color='black', angles='xy', scale_units='xy', scale=10, width=0.007, zorder=5)\n",
        "        else:  # ボール(22)\n",
        "            # 白黒のサッカーボール風デザイン\n",
        "            ax.scatter(pos[k, 0], pos[k, 1], c='white', s=150, edgecolors='black', linewidth=1.5, zorder=6)\n",
        "            ax.scatter(pos[k, 0], pos[k, 1], c='black', s=30, marker='x', zorder=7)\n",
        "\n",
        "    ax.set_title(f\"{title}\\nSuccess Prob: {prob:.4f}\", fontsize=20, pad=20)\n",
        "    ax.set_xlim(-0.02, 1.02); ax.set_ylim(-0.02, 1.02); ax.axis('off')\n",
        "    plt.tight_layout(); plt.show()"
      ],
      "metadata": {
        "id": "zTsFL9hJiuFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 修正版：KeyError対策済み抽出コード ---\n",
        "\n",
        "if len(miracle_list) > 0:\n",
        "    first_miracle = miracle_list[0]\n",
        "\n",
        "    # KeyError: 0 が出たので、辞書のキー名で取得します\n",
        "    try:\n",
        "        batch_idx = first_miracle['batch_idx']\n",
        "        item_idx = first_miracle['item_idx']\n",
        "    except TypeError:\n",
        "        # 万が一タプルだった場合の予備処理\n",
        "        batch_idx = first_miracle[0]\n",
        "        item_idx = first_miracle[1]\n",
        "\n",
        "    print(f\"Targeting: Batch {batch_idx}, Item {item_idx}\")\n",
        "\n",
        "    example_data = None\n",
        "    # test_loader から対象のバッチを特定\n",
        "    for i, batch in enumerate(test_loader):\n",
        "        if i == batch_idx:\n",
        "            # PyGのバッチを個別のデータリストに変換\n",
        "            example_data = batch.to_data_list()[item_idx]\n",
        "            break\n",
        "\n",
        "    if example_data is not None:\n",
        "        # GPUへ転送\n",
        "        example_data = example_data.to(device)\n",
        "\n",
        "        # 可視化実行\n",
        "        # model_0 と model_100 (または model) が存在することを確認してください\n",
        "        print(\"Rendering Baseline...\")\n",
        "        visualize_at_miracle_scene(model_0, example_data, title=\"Baseline (alpha_p=0)\")\n",
        "\n",
        "        print(\"Rendering Proposed...\")\n",
        "        visualize_at_miracle_scene(model_10, example_data, title=\"Proposed (alpha_p=10)\")\n",
        "    else:\n",
        "        print(f\"Error: Batch {batch_idx} が見つかりませんでした。\")\n",
        "else:\n",
        "    print(\"miracle_list が空です。\")"
      ],
      "metadata": {
        "id": "a2P5syuMdvKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 修正版：KeyError対策済み抽出コード ---\n",
        "\n",
        "if len(miracle_list) > 0:\n",
        "    first_miracle = miracle_list[3]\n",
        "\n",
        "    # KeyError: 0 が出たので、辞書のキー名で取得します\n",
        "    try:\n",
        "        batch_idx = first_miracle['batch_idx']\n",
        "        item_idx = first_miracle['item_idx']\n",
        "    except TypeError:\n",
        "        # 万が一タプルだった場合の予備処理\n",
        "        batch_idx = first_miracle[0]\n",
        "        item_idx = first_miracle[1]\n",
        "\n",
        "    print(f\"Targeting: Batch {batch_idx}, Item {item_idx}\")\n",
        "\n",
        "    example_data = None\n",
        "    # test_loader から対象のバッチを特定\n",
        "    for i, batch in enumerate(test_loader):\n",
        "        if i == batch_idx:\n",
        "            # PyGのバッチを個別のデータリストに変換\n",
        "            example_data = batch.to_data_list()[item_idx]\n",
        "            break\n",
        "\n",
        "    if example_data is not None:\n",
        "        # GPUへ転送\n",
        "        example_data = example_data.to(device)\n",
        "\n",
        "        # 可視化実行\n",
        "        # model_0 と model_100 (または model) が存在することを確認してください\n",
        "        print(\"Rendering Baseline...\")\n",
        "        visualize_at_miracle_scene(model_0, example_data, title=\"Baseline (alpha_p=0)\")\n",
        "\n",
        "        print(\"Rendering Proposed...\")\n",
        "        visualize_at_miracle_scene(model_10, example_data, title=\"Proposed (alpha_p=10)\")\n",
        "    else:\n",
        "        print(f\"Error: Batch {batch_idx} が見つかりませんでした。\")\n",
        "else:\n",
        "    print(\"miracle_list が空です。\")"
      ],
      "metadata": {
        "id": "aqoc3NqLSDjc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}