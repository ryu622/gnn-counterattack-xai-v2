{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNZqNZaTy9lB3kZzuiqCrTw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryu622/gnn-counterattack-xai-v2/blob/feat%2Fdelete-show-and-add-show2/GNN_CounterAttack_Thesis_show2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdMoTADGeH1c"
      },
      "outputs": [],
      "source": [
        "# --- 1. ç’°å¢ƒè¨­å®šã¨ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« ---\n",
        "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!pip install torch-scatter torch-sparse torch-geometric sklearn tqdm networkx matplotlib\n",
        "!pip install torch-geometric\n",
        "\n",
        "# Google Driveã‚’ãƒã‚¦ãƒ³ãƒˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ã«å¿…é ˆï¼‰\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, Dataset\n",
        "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
        "from torch_geometric.nn import GATConv, global_mean_pool\n",
        "from torch_geometric.utils import dense_to_sparse\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.calibration import calibration_curve\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pickle\n",
        "import sys\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import logging\n",
        "import copy\n",
        "import json\n",
        "\n",
        "# ãƒ­ã‚¬ãƒ¼è¨­å®š\n",
        "logger = logging.getLogger(__name__)\n",
        "if not logger.handlers:\n",
        "    stdout_handler = logging.StreamHandler(sys.stdout)\n",
        "    logger.addHandler(stdout_handler)\n",
        "\n",
        "print(\" STEP 1 å®Œäº†: ç’°å¢ƒè¨­å®šã¨ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ã€ç‰¹å¾´é‡å®šç¾©ã€PyGãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¤‰æ› ---\n",
        "\n",
        "# --- ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ ---\n",
        "# --- ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å¿…è¦ã«å¿œã˜ã¦ä¿®æ­£ ---\n",
        "file_path = '/content/drive/MyDrive/GNN_Football_Analysis/Raw_Data/women.pkl'\n",
        "\n",
        "try:\n",
        "    with open(file_path, 'rb') as handle:\n",
        "        og_data = pickle.load(handle)\n",
        "    print(f\"ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰å®Œäº†: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ {file_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"é‡å¤§ãªã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ '{file_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹ã‚’ä¿®æ­£ã™ã‚‹ã‹ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Google Driveã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\")\n",
        "    sys.exit()\n",
        "\n",
        "# --- ç‰¹å¾´é‡å®šç¾©ã®ãŸã‚ã®ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆä»£æ›¿ ---\n",
        "class Checkbox:\n",
        "    def __init__(self, value, description): self.value = value\n",
        "class MockDropdown:\n",
        "    def __init__(self, value): self.value = value\n",
        "class MockVBox:\n",
        "    def __init__(self, children): self.children = children\n",
        "\n",
        "adj_matrix = MockDropdown('normal')\n",
        "edge_f_box = MockVBox([Checkbox(value=True, description='Player Distance')] * 6)\n",
        "node_f_box = MockVBox([Checkbox(value=True, description='x coordinate')] * 12)\n",
        "\n",
        "# --- ç‰¹å¾´é‡ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (æ­£ã—ã„ç‰¹å¾´é‡åã§å®šç¾©) ---\n",
        "def filter_features(data, gender=None):\n",
        "    CORRECT_NODE_FEATURES = [\n",
        "        \"X Coordinate\", \"Y Coordinate\", \"vX\", \"vY\", \"Speed\", \"Velocity Angle\",\n",
        "        \"Distance to Goal\", \"Angle with Goal\", \"Distance to Ball\", \"Angle with Ball\",\n",
        "        \"Time in Possession\", \"Is Ball Carrier\"\n",
        "    ]\n",
        "    edge_feature_idxs = [idx for idx, x in enumerate(edge_f_box.children) if x.value]\n",
        "    node_feature_idxs = [idx for idx, x in enumerate(node_f_box.children) if x.value]\n",
        "    global node_features\n",
        "    node_features = [CORRECT_NODE_FEATURES[i] for i in node_feature_idxs]\n",
        "    mat_type = adj_matrix.value\n",
        "    data[mat_type]['e'] = [x[:, edge_feature_idxs] for x in data[mat_type]['e']]\n",
        "    data[mat_type]['x'] = [x[:, node_feature_idxs] for x in data[mat_type]['x']]\n",
        "    return data\n",
        "\n",
        "data = filter_features(og_data.copy())\n",
        "\n",
        "# --- PyGãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã¨å¤‰æ› ---\n",
        "class PyG_CounterDataset(Dataset):\n",
        "    def __init__(self, data, matrix_type):\n",
        "        self.raw_data = data; self.matrix_type = matrix_type\n",
        "        self._data_list = self.process_data()\n",
        "        super().__init__(None, None, None)\n",
        "    def process_data(self):\n",
        "        data_mat = self.raw_data[self.matrix_type]; data_list = []\n",
        "        for x_np, a_np, e_np, y_np in tqdm(zip(data_mat['x'], data_mat['a'], data_mat['e'], self.raw_data['binary']), total=len(data_mat['x']), desc=\"Converting to PyG Data\"):\n",
        "            try:\n",
        "                if x_np.shape[0] == 0: continue\n",
        "                if hasattr(a_np, 'todense'): a_np = a_np.todense()\n",
        "                x = torch.tensor(x_np, dtype=torch.float); a = torch.tensor(a_np, dtype=torch.float)\n",
        "                y = torch.tensor(y_np, dtype=torch.float).view(-1, 1)\n",
        "                edge_index, _ = dense_to_sparse(a)\n",
        "                if edge_index.numel() == 0: continue\n",
        "                data_list.append(Data(x=x, edge_index=edge_index, y=y))\n",
        "            except Exception: continue\n",
        "        return data_list\n",
        "    def len(self): return len(self._data_list)\n",
        "    def get(self, idx): return self._data_list[idx]\n",
        "\n",
        "dataset_pyg = PyG_CounterDataset(data=data, matrix_type='normal')\n",
        "print(f\"STEP 2 å®Œäº†: PyTorch Geometricãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¤‰æ›å®Œäº†ã€‚å…¨ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(dataset_pyg)}\")"
      ],
      "metadata": {
        "id": "QzDPV8JQeOm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#çµæœã®ä¿å­˜å…ˆã®è¨­å®š\n",
        "\n",
        "# è¨­å®šï¼ˆå®Ÿè¡Œæ™‚ã«æ›¸ãæ›ãˆã‚‹ï¼‰\n",
        "exp_tag = \"1217_v1\"  # å®Ÿé¨“ã”ã¨ã« \"1217_v2\" ãªã©ã«å¤‰ãˆã‚‹(æ—¥ä»˜â†’ãƒãƒ¼ã‚¸ãƒ§ãƒ³)\n",
        "\n",
        "# ä¿å­˜å…ˆã®ãƒ‘ã‚¹ï¼ˆResultsãƒ•ã‚©ãƒ«ãƒ€ã®ä¸‹ã«ã‚¿ã‚°åã§ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œã‚‹ï¼‰\n",
        "save_dir = f'/content/drive/MyDrive/GNN_Football_Analysis/Results/{exp_tag}/'\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "UKTGDqF3GuJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. GATãƒ¢ãƒ‡ãƒ«ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€é–¢æ•°ã®å®šç¾© ---\n",
        "\n",
        "# --- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ---\n",
        "CHANNELS = 32; LAYERS = 3; ATTN_HEADS = 4; N_OUT = 1; LEARNING_RATE = 1e-3; EPOCHS = 50; BATCH_SIZE = 32\n",
        "\n",
        "# --- GATãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã®å®šç¾© (ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æŠ½å‡ºæ©Ÿèƒ½ä»˜ã) ---\n",
        "class PyG_GNN_Attn(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_layers, num_heads, out_channels):\n",
        "        super(PyG_GNN_Attn, self).__init__()\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads=num_heads, dropout=0.5, concat=True)\n",
        "        self.convs = nn.ModuleList([GATConv(hidden_channels * num_heads, hidden_channels, heads=num_heads, dropout=0.5, concat=True) for _ in range(num_layers - 1)])\n",
        "        self.conv_out = GATConv(hidden_channels * num_heads, hidden_channels, heads=1, dropout=0.5, concat=False)\n",
        "        self.dense1 = nn.Linear(hidden_channels, hidden_channels); self.dense_out = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, data, return_attn=False):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        attn_weights = None\n",
        "\n",
        "        if return_attn: x, (edge_index, attn_weights) = self.conv1(x, edge_index, return_attention_weights=True)\n",
        "        else: x = self.conv1(x, edge_index)\n",
        "\n",
        "        x = F.elu(x);\n",
        "        for conv in self.convs: x = F.elu(conv(x, edge_index))\n",
        "        x = F.elu(self.conv_out(x, edge_index))\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = F.relu(self.dense1(x))\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        output = torch.sigmoid(self.dense_out(x))\n",
        "\n",
        "        if return_attn: return output, edge_index, attn_weights.squeeze()\n",
        "        return output\n",
        "\n",
        "# --- è¨“ç·´/è©•ä¾¡é–¢æ•°ã®å®šç¾© ---\n",
        "def train_pyg(model, loader, optimizer, criterion):\n",
        "    model.train(); total_loss = 0; device = next(model.parameters()).device\n",
        "    for data in loader:\n",
        "        data = data.to(device); optimizer.zero_grad(); out = model(data)\n",
        "        loss = criterion(out, data.y); loss.backward(); optimizer.step()\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "def evaluate_pyg(model, loader):\n",
        "    model.eval(); preds = []; labels = []; device = next(model.parameters()).device\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device); out = model(data)\n",
        "            preds.append(out.cpu().numpy().flatten()); labels.append(data.y.cpu().numpy().flatten())\n",
        "    preds = np.concatenate(preds); labels = np.concatenate(labels)\n",
        "    if len(np.unique(labels)) > 1: auc = roc_auc_score(labels, preds)\n",
        "    else: auc = 0.0\n",
        "    return auc, preds\n",
        "\n",
        "print(\"STEP 3 å®Œäº†: ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã¨è¨“ç·´/è©•ä¾¡é–¢æ•°ã®å®šç¾©ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "6lyzOMh-eoZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. DataLoaderã®æº–å‚™ã¨ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ ---\n",
        "\n",
        "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†å‰²\n",
        "final_data_list = dataset_pyg._data_list; indices = np.arange(len(final_data_list))\n",
        "idx_tr, idx_te = train_test_split(indices, test_size=0.3, random_state=42) # 30%ã‚’ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨\n",
        "dataset_tr_pyg = [final_data_list[i] for i in idx_tr]; dataset_te_pyg = [final_data_list[i] for i in idx_te]\n",
        "\n",
        "loader_tr_pyg = PyGDataLoader(dataset_tr_pyg, batch_size=BATCH_SIZE, shuffle=True)\n",
        "loader_te_pyg_clean = PyGDataLoader(dataset_te_pyg, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–\n",
        "model_attn = PyG_GNN_Attn(in_channels=dataset_pyg.num_node_features, hidden_channels=CHANNELS, num_layers=LAYERS, num_heads=ATTN_HEADS, out_channels=N_OUT)\n",
        "optimizer_pyg = torch.optim.Adam(model_attn.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# è¨“ç·´ã®å®Ÿè¡Œ\n",
        "print(\"\\n PyTorch GATãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    loss = train_pyg(model_attn, loader_tr_pyg, optimizer_pyg, criterion)\n",
        "    if epoch % 10 == 0: print(f\"--- Epoch {epoch}/{EPOCHS} --- Loss: {loss:.4f}\")\n",
        "    print(f\"--- Epoch {epoch}/{EPOCHS} --- Training Loss: {loss:.4f}\")\n",
        "print(\"STEP 4 å®Œäº†: è¨“ç·´ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "5uItH98DeqcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. æœ€çµ‚è©•ä¾¡ã¨PFIã®è¨ˆç®— ---\n",
        "\n",
        "# è©•ä¾¡ã®å®Ÿè¡Œ\n",
        "auc_te, preds_te = evaluate_pyg(model_attn, loader_te_pyg_clean)\n",
        "labels_te = np.concatenate([data.y.numpy().flatten() for data in dataset_te_pyg])\n",
        "\n",
        "print(f\"\\n--- æœ€çµ‚ãƒ†ã‚¹ãƒˆçµæœ --- AUC: {auc_te:.4f}\")\n",
        "\n",
        "# PFIã®é–¢æ•°å®šç¾©\n",
        "def calculate_permutation_importance(model, loader, features_list, n_repeats=3):\n",
        "    \"\"\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸Šã§ã€å„ç‰¹å¾´é‡ã®ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é‡è¦åº¦ (AUCã®ä½ä¸‹) ã‚’è¨ˆç®—ã™ã‚‹\"\"\"\n",
        "    auc, _ = evaluate_pyg(model, loader)\n",
        "    baseline_auc = auc; importance = {}\n",
        "\n",
        "    for idx, feature_name in enumerate(features_list):\n",
        "        auc_decreases = []\n",
        "        for _ in range(n_repeats):\n",
        "            permuted_data_list = [];\n",
        "            for batch_data in loader:\n",
        "                x_permuted = batch_data.x.clone()\n",
        "                perm_indices = torch.randperm(x_permuted.size(0))\n",
        "                x_permuted[:, idx] = x_permuted[perm_indices, idx]\n",
        "                permuted_batch = batch_data.clone(); permuted_batch.x = x_permuted\n",
        "                permuted_data_list.append(permuted_batch)\n",
        "\n",
        "            permuted_loader = PyGDataLoader(permuted_data_list, batch_size=loader.batch_size, shuffle=False)\n",
        "            permuted_auc, _ = evaluate_pyg(model, permuted_loader)\n",
        "            auc_decreases.append(baseline_auc - permuted_auc)\n",
        "\n",
        "        importance[feature_name] = np.mean(auc_decreases)\n",
        "    return importance\n",
        "\n",
        "# PFIã®å®Ÿè¡Œ (è¨ˆç®—æ™‚é–“ã‚’è€ƒæ…®ã—ã€ã“ã“ã§ã¯n_repeats=3ã§å®Ÿè¡Œ)\n",
        "print(\"\\nPFI (ãƒ‘ãƒ¼ãƒŸãƒ¥ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç‰¹å¾´é‡é‡è¦åº¦) ã®è¨ˆç®—ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "importance_results = calculate_permutation_importance(model_attn, loader_te_pyg_clean, node_features, n_repeats=3)\n",
        "\n",
        "# PFIçµæœã®å¯è¦–åŒ–\n",
        "sorted_importance = sorted(importance_results.items(), key=lambda item: item[1], reverse=True)\n",
        "print(\"\\n--- ç‰¹å¾´é‡é‡è¦åº¦ (AUCä½ä¸‹é‡) ---\")\n",
        "for feature, decrease in sorted_importance:\n",
        "    print(f\"[{feature: <20}] : {decrease:.5f}\")\n",
        "\n",
        "features = [item[0] for item in sorted_importance]; decreases = [item[1] for item in sorted_importance]\n",
        "plt.figure(figsize=(10, 6)); plt.barh(features, decreases, color='darkorange')\n",
        "plt.xlabel('AUC Decrease after Permutation (PFI)'); plt.title('Permutation Feature Importance (PyTorch GAT)')\n",
        "plt.gca().invert_yaxis(); plt.grid(axis='x', linestyle='--')\n",
        "plt.tight_layout()\n",
        "\n",
        "# --- Google Driveã¸ä¿å­˜ ---\n",
        "# save_dir ã¯å…ˆã»ã©ã®ã‚³ãƒ¼ãƒ‰ã§ä½œæˆã—ãŸãƒ‘ã‚¹ã‚’ä½¿ã„ã¾ã™\n",
        "fig_path = os.path.join(save_dir, 'pfi.png')\n",
        "plt.savefig(fig_path)\n",
        "\n",
        "print(f\"ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {fig_path}\")\n",
        "\n",
        "plt.show()\n",
        "print(\"STEP 5 å®Œäº†: PFIã®è¨ˆç®—ã¨å¯è¦–åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "ooOSn-xQesKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆè¼ƒæ­£ï¼‰æ›²ç·šã®æç”» ---\n",
        "\n",
        "prob_true, prob_pred = calibration_curve(y_true=labels_te, y_prob=preds_te, n_bins=10, strategy='uniform')\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(prob_pred, prob_true, marker='o', label='GAT Model')\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly Calibrated')\n",
        "\n",
        "plt.xlabel(\"Average Predicted Probability (Confidence)\"); plt.ylabel(\"Fraction of Positives (Accuracy)\")\n",
        "plt.title(\"Model Calibration Curve on Test Set\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# --- Google Driveã¸ä¿å­˜ ---\n",
        "# save_dir ã¯å…ˆã»ã©ã®ã‚³ãƒ¼ãƒ‰ã§ä½œæˆã—ãŸãƒ‘ã‚¹ã‚’ä½¿ã„ã¾ã™\n",
        "fig_path = os.path.join(save_dir, 'Calibration.png')\n",
        "plt.savefig(fig_path)\n",
        "\n",
        "print(f\"ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {fig_path}\")\n",
        "\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"STEP 6 å®Œäº†: ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ›²ç·šã®æç”»ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "XP0OA5D9euBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã®å¯è¦–åŒ– ---\n",
        "\n",
        "# 1. æˆåŠŸäºˆæ¸¬ãƒ•ãƒ¬ãƒ¼ãƒ ã®ç‰¹å®š\n",
        "model_attn.eval(); max_pred = -1.0; best_index = 0\n",
        "device = next(model_attn.parameters()).device\n",
        "with torch.no_grad():\n",
        "    for idx, data_frame in enumerate(dataset_te_pyg):\n",
        "        data_frame = data_frame.to(device)\n",
        "        prediction_prob = model_attn(data_frame).item()\n",
        "        if prediction_prob > max_pred: max_pred = prediction_prob; best_index = idx\n",
        "\n",
        "# 2. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®æŠ½å‡º\n",
        "single_frame = dataset_te_pyg[best_index]; single_data = single_frame.to(device)\n",
        "with torch.no_grad():\n",
        "    prediction, edge_index_attn, weights_attn = model_attn(single_data, return_attn=True)\n",
        "\n",
        "# 3. NetworkXã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰\n",
        "x_coords = single_data.x[:, 0].cpu().numpy(); y_coords = single_data.x[:, 1].cpu().numpy()\n",
        "edge_list = edge_index_attn.cpu().numpy().T; attention_weights = weights_attn.cpu().numpy()\n",
        "G = nx.Graph(); num_nodes = len(x_coords); G.add_nodes_from(range(num_nodes)); pos = {i: (x_coords[i], y_coords[i]) for i in range(num_nodes)}\n",
        "min_w = attention_weights.min(); max_w = attention_weights.max()\n",
        "normalized_weights_np = 10 * (attention_weights - min_w) / (max_w - min_w + 1e-6) + 1\n",
        "\n",
        "for i, (u, v) in enumerate(edge_list):\n",
        "    try: weight_val = float(normalized_weights_np[i].flatten()[0])\n",
        "    except Exception: weight_val = float(normalized_weights_np[i])\n",
        "    G.add_edge(u, v, weight=weight_val)\n",
        "\n",
        "# 4. å¯è¦–åŒ–ã®å®Ÿè¡Œ\n",
        "plt.figure(figsize=(12, 8)); plt.title(f\"GAT Local Attention Visualization (Prediction: {prediction.item():.2f})\")\n",
        "num_players = 22; num_players_per_team = 11\n",
        "node_colors = ['red'] * num_players_per_team + ['blue'] * num_players_per_team + ['yellow']\n",
        "node_sizes = [300] * num_players + [500]\n",
        "edge_widths = [G[u][v]['weight'] for u, v in G.edges()]\n",
        "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, label=True)\n",
        "nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.7, edge_color='black')\n",
        "nx.draw_networkx_labels(G, pos, font_size=8)\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.xlabel(\"X Coordinate\")\n",
        "plt.ylabel(\"Y Coordinate\")\n",
        "\n",
        "\n",
        "# --- Google Driveã¸ä¿å­˜ ---\n",
        "# save_dir ã¯å…ˆã»ã©ã®ã‚³ãƒ¼ãƒ‰ã§ä½œæˆã—ãŸãƒ‘ã‚¹ã‚’ä½¿ã„ã¾ã™\n",
        "fig_path = os.path.join(save_dir, 'AttentionVisualization.png')\n",
        "plt.savefig(fig_path)\n",
        "\n",
        "print(f\"ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {fig_path}\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ STEP 7 å®Œäº†: ã™ã¹ã¦ã®å·¥ç¨‹ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "i_fHiryHev0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. æœ€ã‚‚é«˜ã„äºˆæ¸¬ç¢ºç‡ã‚’æŒã¤ãƒ•ãƒ¬ãƒ¼ãƒ ã®ç‰¹å®š ---\n",
        "model_attn.eval()\n",
        "max_pred = -1.0\n",
        "best_index = -1\n",
        "device = next(model_attn.parameters()).device\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, data_frame in enumerate(dataset_te_pyg):\n",
        "        # ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•\n",
        "        data_frame = data_frame.to(device)\n",
        "\n",
        "        # äºˆæ¸¬ç¢ºç‡ã®å–å¾—\n",
        "        prediction_prob = model_attn(data_frame).item()\n",
        "\n",
        "        if prediction_prob > max_pred:\n",
        "            max_pred = prediction_prob\n",
        "            best_index = idx\n",
        "\n",
        "print(f\"æœ€ã‚‚é«˜ã„äºˆæ¸¬ç¢ºç‡ {max_pred:.4f} ã‚’æŒã¤ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {best_index}\")"
      ],
      "metadata": {
        "id": "NxjfLuYFgPsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. ç‰¹å®šãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æŠ½å‡ºã¨ã‚°ãƒ©ãƒ•ã®å†æ§‹ç¯‰ï¼ˆåŸºæœ¬ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼‰ ---\n",
        "\n",
        "# æˆåŠŸãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ‡ãƒ¼ã‚¿å–å¾—\n",
        "single_frame = dataset_te_pyg[best_index]\n",
        "single_data = single_frame.to(next(model_attn.parameters()).device)\n",
        "\n",
        "# ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å€¤ã®æŠ½å‡º\n",
        "with torch.no_grad():\n",
        "    prediction, edge_index_attn, weights_attn = model_attn(single_data, return_attn=True)\n",
        "\n",
        "print(f\"äºˆæ¸¬ç¢ºç‡: {prediction.item():.2f}\")\n",
        "\n",
        "# 3. NetworkXã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰\n",
        "x_coords = single_data.x[:, 0].cpu().numpy()\n",
        "y_coords = single_data.x[:, 1].cpu().numpy()\n",
        "edge_list = edge_index_attn.cpu().numpy().T\n",
        "attention_weights = weights_attn.cpu().numpy()\n",
        "\n",
        "G = nx.Graph()\n",
        "num_nodes = len(x_coords)\n",
        "G.add_nodes_from(range(num_nodes))\n",
        "pos = {i: (x_coords[i], y_coords[i]) for i in range(num_nodes)}\n",
        "\n",
        "# ã‚¨ãƒƒã‚¸ã®é‡ã¿ä»˜ã‘ã¨æ­£è¦åŒ–\n",
        "min_w = attention_weights.min()\n",
        "max_w = attention_weights.max()\n",
        "# normalized_weights ãŒä»¥é™ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ä½¿ç”¨ã•ã‚Œã¾ã™\n",
        "normalized_weights = 10 * (attention_weights - min_w) / (max_w - min_w + 1e-6) + 1\n",
        "\n",
        "for i, (u, v) in enumerate(edge_list):\n",
        "    try:\n",
        "        # NumPyã®å‹ã‚’Pythonã®floatã«å¤‰æ›ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯\n",
        "        weight_val = float(normalized_weights[i].flatten()[0])\n",
        "    except Exception:\n",
        "        weight_val = float(normalized_weights[i])\n",
        "\n",
        "    G.add_edge(u, v, weight=weight_val)\n",
        "\n",
        "# ãƒãƒ¼ãƒ‰å®šç¾© (ã‚°ãƒ©ãƒ•å…¨ä½“ã§å…±é€š)\n",
        "num_players = 22\n",
        "num_players_per_team = 11\n",
        "node_colors = ['red'] * num_players_per_team + ['blue'] * num_players_per_team + ['yellow']\n",
        "node_sizes = [300] * num_players + [500]"
      ],
      "metadata": {
        "id": "gi2O9CBKgSFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. å…¨ä½“ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å¯è¦–åŒ– ---\n",
        "#æœ€ã‚‚é«˜ã„ç¢ºç‡ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å¯è¦–åŒ–\n",
        "\n",
        "edge_widths = [G[u][v]['weight'] for u, v in G.edges()]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title(f\"GAT Local Attention Visualization (Prediction: {prediction.item():.2f}) - ALL EDGES\")\n",
        "\n",
        "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, label=True)\n",
        "nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.7, edge_color='black')\n",
        "nx.draw_networkx_labels(G, pos, font_size=8)\n",
        "\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.xlabel(\"X Coordinate\")\n",
        "plt.ylabel(\"Y Coordinate\")\n",
        "\n",
        "# --- Google Driveã¸ä¿å­˜ ---\n",
        "# save_dir ã¯å…ˆã»ã©ã®ã‚³ãƒ¼ãƒ‰ã§ä½œæˆã—ãŸãƒ‘ã‚¹ã‚’ä½¿ã„ã¾ã™\n",
        "fig_path = os.path.join(save_dir, 'AttentionVisualization_max.png')\n",
        "plt.savefig(fig_path)\n",
        "\n",
        "print(f\"ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {fig_path}\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"å…¨ä½“ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å¯è¦–åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "IQRcVw2GgUqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. å®ˆå‚™å´ã®ã¿ã®å†…éƒ¨é€£æº (Blue-Blue) å¯è¦–åŒ– ---\n",
        "\n",
        "DEFENSE_START_ID = 11\n",
        "DEFENSE_END_ID = 21\n",
        "\n",
        "defense_edges = []\n",
        "defense_widths = []\n",
        "\n",
        "# Gã«æ ¼ç´ã•ã‚ŒãŸé‡ã¿ã‚’å‚ç…§ã—ã€å®ˆå‚™-å®ˆå‚™ã®ã‚¨ãƒƒã‚¸ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
        "for u, v, data in G.edges(data=True):\n",
        "    is_defense_u = (DEFENSE_START_ID <= u <= DEFENSE_END_ID)\n",
        "    is_defense_v = (DEFENSE_START_ID <= v <= DEFENSE_END_ID)\n",
        "\n",
        "    if is_defense_u and is_defense_v:\n",
        "        defense_edges.append((u, v))\n",
        "        defense_widths.append(data['weight'])\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ã®æç”»\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title(f\"GAT Attention: Defense Team Internal Collaboration (Blue-Blue Edges) - Prediction: {prediction.item():.2f}\")\n",
        "\n",
        "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, label=True)\n",
        "nx.draw_networkx_edges(G, pos, edgelist=defense_edges, width=defense_widths, alpha=0.9, edge_color='darkblue')\n",
        "nx.draw_networkx_labels(G, pos, font_size=8)\n",
        "\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "\n",
        "# --- Google Driveã¸ä¿å­˜ ---\n",
        "# save_dir ã¯å…ˆã»ã©ã®ã‚³ãƒ¼ãƒ‰ã§ä½œæˆã—ãŸãƒ‘ã‚¹ã‚’ä½¿ã„ã¾ã™\n",
        "fig_path = os.path.join(save_dir, 'AttentionVisualization_max_defence.png')\n",
        "plt.savefig(fig_path)\n",
        "\n",
        "print(f\"ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {fig_path}\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"å®ˆå‚™å´ã®ã¿ã®é€£æºã‚°ãƒ©ãƒ•æç”»ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "Ba30VOTggXhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. æ”»æ’ƒå´ã®ã¿ã®å†…éƒ¨é€£æº (Red-Red) å¯è¦–åŒ– ---\n",
        "\n",
        "ATTACK_START_ID = 0\n",
        "ATTACK_END_ID = 10\n",
        "\n",
        "attack_edges = []\n",
        "attack_widths = []\n",
        "\n",
        "# Gã«æ ¼ç´ã•ã‚ŒãŸé‡ã¿ã‚’å‚ç…§ã—ã€æ”»æ’ƒ-æ”»æ’ƒã®ã‚¨ãƒƒã‚¸ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
        "for u, v, data in G.edges(data=True):\n",
        "    is_attack_u = (ATTACK_START_ID <= u <= ATTACK_END_ID)\n",
        "    is_attack_v = (ATTACK_START_ID <= v <= ATTACK_END_ID)\n",
        "\n",
        "    if is_attack_u and is_attack_v:\n",
        "        attack_edges.append((u, v))\n",
        "        attack_widths.append(data['weight'])\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ã®æç”»\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title(f\"GAT Attention: Attack Team Internal Collaboration (Red-Red Edges) - Prediction: {prediction.item():.2f}\")\n",
        "\n",
        "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, label=True)\n",
        "nx.draw_networkx_edges(G, pos, edgelist=attack_edges, width=attack_widths, alpha=0.9, edge_color='darkred')\n",
        "nx.draw_networkx_labels(G, pos, font_size=8)\n",
        "\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.xlabel(\"X Coordinate\")\n",
        "plt.ylabel(\"Y Coordinate\")\n",
        "\n",
        "# --- Google Driveã¸ä¿å­˜ ---\n",
        "# save_dir ã¯å…ˆã»ã©ã®ã‚³ãƒ¼ãƒ‰ã§ä½œæˆã—ãŸãƒ‘ã‚¹ã‚’ä½¿ã„ã¾ã™\n",
        "fig_path = os.path.join(save_dir, 'AttentionVisualization_max_attack.png')\n",
        "plt.savefig(fig_path)\n",
        "\n",
        "print(f\"ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {fig_path}\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"æ”»æ’ƒå´ã®ã¿ã®é€£æºã‚°ãƒ©ãƒ•æç”»ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "lNyFNo4-ganu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "#æœ€ã‚‚ä½ã„ç¢ºç‡ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã®å¯è¦–åŒ–\n",
        "# --- 1. å¤±æ•—äºˆæ¸¬ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆç¢ºç‡ãŒæœ€å°ï¼‰ã‚’è¦‹ã¤ã‘ã‚‹ ---\n",
        "model_attn.eval()\n",
        "min_pred = 2.0  # äºˆæ¸¬ç¢ºç‡ã®åˆæœŸå€¤ã‚’æœ€å¤§å€¤ã‚ˆã‚Šã‚‚å¤§ããè¨­å®š (ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰å‡ºåŠ›ã¯0ã‹ã‚‰1)\n",
        "best_index_failure = -1\n",
        "device = next(model_attn.parameters()).device\n",
        "\n",
        "with torch.no_grad():\n",
        "    for idx, data_frame in enumerate(dataset_te_pyg):\n",
        "        data_frame = data_frame.to(device)\n",
        "        prediction_prob = model_attn(data_frame).item()\n",
        "\n",
        "        # äºˆæ¸¬ç¢ºç‡ãŒç¾åœ¨ã®æœ€å°å€¤ (min_pred) ã‚ˆã‚Šã‚‚ä½ã„å ´åˆã€æ›´æ–°ã™ã‚‹\n",
        "        if prediction_prob < min_pred:\n",
        "            min_pred = prediction_prob\n",
        "            best_index_failure = idx\n",
        "\n",
        "print(f\"æœ€ã‚‚ä½ã„äºˆæ¸¬ç¢ºç‡ {min_pred:.4f} ã‚’æŒã¤å¤±æ•—ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {best_index_failure}\")\n",
        "\n",
        "# --- 2. NetworkXã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰ ---\n",
        "\n",
        "# å¤±æ•—ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ‡ãƒ¼ã‚¿å–å¾—\n",
        "single_frame = dataset_te_pyg[best_index_failure]\n",
        "single_data = single_frame.to(device)\n",
        "\n",
        "# ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å€¤ã®æŠ½å‡º\n",
        "with torch.no_grad():\n",
        "    prediction, edge_index_attn, weights_attn = model_attn(single_data, return_attn=True)\n",
        "\n",
        "print(f\"å¤±æ•—äºˆæ¸¬ç¢ºç‡: {prediction.item():.2f}\")\n",
        "\n",
        "# ãƒ†ãƒ³ã‚½ãƒ«ã‚’CPUä¸Šã®NumPyé…åˆ—ã«å¤‰æ›\n",
        "x_coords = single_data.x[:, 0].cpu().numpy()\n",
        "y_coords = single_data.x[:, 1].cpu().numpy()\n",
        "edge_list = edge_index_attn.cpu().numpy().T\n",
        "attention_weights = weights_attn.cpu().numpy()\n",
        "\n",
        "# NetworkXã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰\n",
        "G = nx.Graph()\n",
        "num_nodes = len(x_coords)\n",
        "G.add_nodes_from(range(num_nodes))\n",
        "pos = {i: (x_coords[i], y_coords[i]) for i in range(num_nodes)}\n",
        "\n",
        "# ã‚¨ãƒƒã‚¸ã®é‡ã¿ä»˜ã‘ã¨æ­£è¦åŒ–\n",
        "min_w = attention_weights.min()\n",
        "max_w = attention_weights.max()\n",
        "normalized_weights_np = 10 * (attention_weights - min_w) / (max_w - min_w + 1e-6) + 1\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ã«ã‚¨ãƒƒã‚¸ã¨é‡ã¿ã‚’è¿½åŠ  (å‹å¤‰æ›ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒ)\n",
        "for i, (u, v) in enumerate(edge_list):\n",
        "    try:\n",
        "        weight_val = float(np.float64(normalized_weights_np[i]))\n",
        "    except Exception:\n",
        "        weight_val = float(normalized_weights_np[i].flatten()[0])\n",
        "    G.add_edge(u, v, weight=weight_val)\n",
        "\n",
        "# ãƒãƒ¼ãƒ‰æƒ…å ± (æç”»å…±é€š)\n",
        "num_players = 22\n",
        "num_players_per_team = 11\n",
        "node_colors = ['red'] * num_players_per_team + ['blue'] * num_players_per_team + ['yellow']\n",
        "node_sizes = [300] * num_players + [500]"
      ],
      "metadata": {
        "id": "2D-cliNMhXOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. å¤±æ•—ãƒ•ãƒ¬ãƒ¼ãƒ å…¨ä½“ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å¯è¦–åŒ– ---\n",
        "\n",
        "edge_widths = [G[u][v]['weight'] for u, v in G.edges()]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title(f\"GAT Local Attention Visualization (FAILURE Prediction: {prediction.item():.2f}) - ALL EDGES\")\n",
        "\n",
        "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, label=True)\n",
        "nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.7, edge_color='black')\n",
        "nx.draw_networkx_labels(G, pos, font_size=8)\n",
        "\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.xlabel(\"X Coordinate\")\n",
        "plt.ylabel(\"Y Coordinate\")\n",
        "\n",
        "# --- Google Driveã¸ä¿å­˜ ---\n",
        "# save_dir ã¯å…ˆã»ã©ã®ã‚³ãƒ¼ãƒ‰ã§ä½œæˆã—ãŸãƒ‘ã‚¹ã‚’ä½¿ã„ã¾ã™\n",
        "fig_path = os.path.join(save_dir, 'AttentionVisualization_min.png')\n",
        "plt.savefig(fig_path)\n",
        "\n",
        "print(f\"ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {fig_path}\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"å¤±æ•—ãƒ•ãƒ¬ãƒ¼ãƒ å…¨ä½“ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å¯è¦–åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "lveQ6lwShZde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. å¤±æ•—ãƒ•ãƒ¬ãƒ¼ãƒ å®ˆå‚™å´ã®ã¿ã®å†…éƒ¨é€£æº (Blue-Blue) å¯è¦–åŒ– ---\n",
        "\n",
        "DEFENSE_START_ID = 11\n",
        "DEFENSE_END_ID = 21\n",
        "\n",
        "defense_edges = []\n",
        "defense_widths = []\n",
        "\n",
        "for u, v, data in G.edges(data=True):\n",
        "    is_defense_u = (DEFENSE_START_ID <= u <= DEFENSE_END_ID)\n",
        "    is_defense_v = (DEFENSE_START_ID <= v <= DEFENSE_END_ID)\n",
        "\n",
        "    if is_defense_u and is_defense_v:\n",
        "        defense_edges.append((u, v))\n",
        "        defense_widths.append(data['weight'])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title(f\"GAT Attention: Defense Team Internal Collaboration (Blue-Blue Edges) - FAILURE Prediction: {prediction.item():.2f}\")\n",
        "\n",
        "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, label=True)\n",
        "nx.draw_networkx_edges(G, pos, edgelist=defense_edges, width=defense_widths, alpha=0.9, edge_color='darkblue')\n",
        "nx.draw_networkx_labels(G, pos, font_size=8)\n",
        "\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "\n",
        "# --- Google Driveã¸ä¿å­˜ ---\n",
        "# save_dir ã¯å…ˆã»ã©ã®ã‚³ãƒ¼ãƒ‰ã§ä½œæˆã—ãŸãƒ‘ã‚¹ã‚’ä½¿ã„ã¾ã™\n",
        "fig_path = os.path.join(save_dir, 'AttentionVisualization_min_defence.png')\n",
        "plt.savefig(fig_path)\n",
        "\n",
        "print(f\"ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {fig_path}\")\n",
        "\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… å¤±æ•—ãƒ•ãƒ¬ãƒ¼ãƒ å®ˆå‚™å´ã®ã¿ã®é€£æºã‚°ãƒ©ãƒ•æç”»ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "QEYNyimUhgA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. å¤±æ•—ãƒ•ãƒ¬ãƒ¼ãƒ æ”»æ’ƒå´ã®ã¿ã®å†…éƒ¨é€£æº (Red-Red) å¯è¦–åŒ– ---\n",
        "\n",
        "ATTACK_START_ID = 0\n",
        "ATTACK_END_ID = 10\n",
        "\n",
        "attack_edges = []\n",
        "attack_widths = []\n",
        "\n",
        "for u, v, data in G.edges(data=True):\n",
        "    is_attack_u = (ATTACK_START_ID <= u <= ATTACK_END_ID)\n",
        "    is_attack_v = (ATTACK_START_ID <= v <= ATTACK_END_ID)\n",
        "\n",
        "    if is_attack_u and is_attack_v:\n",
        "        attack_edges.append((u, v))\n",
        "        attack_widths.append(data['weight'])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title(f\"GAT Attention: Attack Team Internal Collaboration (Red-Red Edges) - FAILURE Prediction: {prediction.item():.2f}\")\n",
        "\n",
        "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, label=True)\n",
        "nx.draw_networkx_edges(G, pos, edgelist=attack_edges, width=attack_widths, alpha=0.9, edge_color='darkred')\n",
        "nx.draw_networkx_labels(G, pos, font_size=8)\n",
        "\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.xlabel(\"X Coordinate\")\n",
        "plt.ylabel(\"Y Coordinate\")\n",
        "\n",
        "# --- Google Driveã¸ä¿å­˜ ---\n",
        "# save_dir ã¯å…ˆã»ã©ã®ã‚³ãƒ¼ãƒ‰ã§ä½œæˆã—ãŸãƒ‘ã‚¹ã‚’ä½¿ã„ã¾ã™\n",
        "fig_path = os.path.join(save_dir, 'AttentionVisualization_attack.png')\n",
        "plt.savefig(fig_path)\n",
        "\n",
        "print(f\"ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {fig_path}\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"å¤±æ•—ãƒ•ãƒ¬ãƒ¼ãƒ æ”»æ’ƒå´ã®ã¿ã®é€£æºã‚°ãƒ©ãƒ•æç”»ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "v8W16sTwhjDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#çµæœã®ä¿å­˜\n",
        "\n",
        "\n",
        "\n",
        "# 1. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
        "torch.save(model_attn.state_dict(), os.path.join(save_dir, 'model_weight.pth'))\n",
        "\n",
        "# 2. å­¦ç¿’å±¥æ­´ï¼ˆLossãªã©ï¼‰ã®ä¿å­˜\n",
        "# â€»å®Ÿéš›ã«ã¯å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã§æºœã‚ãŸhistoryå¤‰æ•°ã‚’ä½¿ã„ã¾ã™\n",
        "history = {'train_loss': loss}\n",
        "with open(os.path.join(save_dir, 'history.json'), 'w') as f:\n",
        "    json.dump(history, f)\n",
        "\n",
        "# 3. è¨­å®šå€¤ã®ä¿å­˜\n",
        "config = {\n",
        "    'learning_rate': 0.001,\n",
        "    'batch_size': 32,\n",
        "    'model_type': 'GAT',\n",
        "    'tag': exp_tag\n",
        "}\n",
        "with open(os.path.join(save_dir, 'config.json'), 'w') as f:\n",
        "    json.dump(config, f)\n",
        "\n",
        "print(f\"ä¿å­˜å®Œäº†: {save_dir}\")"
      ],
      "metadata": {
        "id": "HrfkdbX9Cjgv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}