{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOO57UK74F4hHQh4AjXY8hO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryu622/gnn-counterattack-xai-v2/blob/fix%2Ffile-clean/GAT_CounterAttack_Prediction_Train_Scientific7_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ã‚·ãƒ¼ãƒ‰å€¤\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    # Pythonè‡ªä½“ã®ä¹±æ•°å›ºå®š\n",
        "    random.seed(seed)\n",
        "    # OSç’°å¢ƒã®ä¹±æ•°å›ºå®š\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    # Numpyã®ä¹±æ•°å›ºå®š\n",
        "    np.random.seed(seed)\n",
        "    # PyTorchã®ä¹±æ•°å›ºå®š\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # ãƒãƒ«ãƒGPUã®å ´åˆ\n",
        "    # è¨ˆç®—ã®æ±ºå®šè«–çš„æŒ™å‹•ã‚’å¼·åˆ¶ï¼ˆã“ã‚Œã‚’å…¥ã‚Œã‚‹ã¨å°‘ã—é…ããªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ãŒã€å†ç¾æ€§ã¯å®Œç’§ã«ãªã‚Šã¾ã™ï¼‰\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# å¥½ããªæ•°å­—ï¼ˆ42ãŒä¸€èˆ¬çš„ï¼‰ã§å›ºå®š\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "QKyaHmiRGtjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcw4b0y1F0PK"
      },
      "outputs": [],
      "source": [
        "#GoogleDriveã‚’ãƒã‚¦ãƒ³ãƒˆ\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Driveã‚’ä»®æƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ãƒã‚¦ãƒ³ãƒˆ\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å¿…é ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "!pip install torch_geometric\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch import optim\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import re\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# è¡¨ç¤ºè¨­å®š\n",
        "np.set_printoptions(suppress=True, precision=3)\n",
        "pd.set_option('display.precision', 3)    # å°æ•°ç‚¹ä»¥ä¸‹ã®è¡¨ç¤ºæ¡\n",
        "pd.set_option('display.max_rows', 50)   # è¡¨ç¤ºã™ã‚‹è¡Œæ•°ã®ä¸Šé™\n",
        "pd.set_option('display.max_columns', 15)  # è¡¨ç¤ºã™ã‚‹åˆ—æ•°ã®ä¸Šé™\n",
        "%precision 3"
      ],
      "metadata": {
        "id": "K7DHFGzCGYlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ï¼ˆPIGNNã®ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ãƒ©ã‚¹ï¼‰"
      ],
      "metadata": {
        "id": "ocDgeAXM6Ifi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_geometric.utils import softmax\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# 1. ãƒ¢ãƒ‡ãƒ«å®šç¾© (PIGNN: æ”¹è‰¯ç‰ˆ 7æ¬¡å…ƒå…¥åŠ›)\n",
        "# ==========================================\n",
        "class PIGNNLayer(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, tau=1.5):\n",
        "        super(PIGNNLayer, self).__init__(aggr='add')\n",
        "        self.tau = tau\n",
        "        self.lin = nn.Linear(in_channels, out_channels)\n",
        "        self.att = nn.Parameter(torch.Tensor(1, out_channels * 2))\n",
        "        nn.init.xavier_uniform_(self.att)\n",
        "\n",
        "    def forward(self, x, edge_index, pos, vel):\n",
        "        h = self.lin(x)\n",
        "        return self.propagate(edge_index, x=h, pos=pos, vel=vel)\n",
        "\n",
        "    def message(self, x_i, x_j, pos_i, pos_j, vel_i, vel_j, edge_index_i):\n",
        "        # r(t + tau) = r(t) + v(t)tau\n",
        "        pos_i_pred = pos_i + vel_i * self.tau\n",
        "        pos_j_pred = pos_j + vel_j * self.tau\n",
        "\n",
        "        # å°†æ¥ã®è¿‘æ¥åº¦ã«åŸºã¥ãç‰©ç†ãƒã‚¤ã‚¢ã‚¹\n",
        "        dist_future = torch.norm(pos_i_pred - pos_j_pred, dim=-1, keepdim=True)\n",
        "        physics_bias = torch.exp(-dist_future / 5.0)\n",
        "\n",
        "        alpha = torch.cat([x_i, x_j], dim=-1)\n",
        "        alpha = (alpha * self.att).sum(dim=-1, keepdim=True)\n",
        "        alpha = F.leaky_relu(alpha) + physics_bias\n",
        "\n",
        "        alpha = softmax(alpha, edge_index_i)\n",
        "        return alpha * x_j\n",
        "\n",
        "class PIGNNClassifier(nn.Module):\n",
        "    def __init__(self, hidden_channels=64):\n",
        "        super(PIGNNClassifier, self).__init__()\n",
        "        # ä¿®æ­£: 7æ¬¡å…ƒ [x, y, vx, vy, d_goal, d_ball, team_id]\n",
        "        self.conv1 = PIGNNLayer(7, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        x = F.elu(self.conv1(x, edge_index, pos, vel))\n",
        "        x = self.conv2(x, edge_index, pos, vel)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return F.log_softmax(self.lin(x), dim=1)\n",
        "\n",
        "# ==========================================\n",
        "# 2. å‰å‡¦ç†ãƒ»æå¤±é–¢æ•°ã®ä¿®æ­£\n",
        "# ==========================================\n",
        "def preprocess_batch(data, device):\n",
        "    # vx, vy ã®æ­£è¦åŒ–\n",
        "    #data.x[:, 2:4] = data.x[:, 2:4] / 10.0\n",
        "    data.x[:, 2:4] = data.x[:, 2:4]\n",
        "    # d_goal, d_ball ã®æ­£è¦åŒ–\n",
        "    data.x[:, 4:6] = data.x[:, 4:6] / 100.0\n",
        "\n",
        "    # ã€è§£èª¬ã€‘data.x[:, 6] (team_id) ã¯ 0, 1, 2 ãªã®ã§æ­£è¦åŒ–ã›ãšãã®ã¾ã¾ä½¿ã†\n",
        "\n",
        "    data.x = data.x.float()\n",
        "    data.pos = data.pos.float()\n",
        "    data.vel = data.vel.float()\n",
        "    return data.to(device)\n",
        "\n",
        "def pignn_loss_function(out, target_y, vel, alpha_p=0.01):\n",
        "    classification_loss = F.nll_loss(out, target_y)\n",
        "    # é‹å‹•å­¦çš„ãƒšãƒŠãƒ«ãƒ†ã‚£: 12m/sè¶…ãˆã¸ã®åˆ¶ç´„\n",
        "    v_magnitude = torch.norm(vel, dim=-1)\n",
        "    l_motion = torch.mean(F.relu(v_magnitude - 12.0)**2)\n",
        "    return classification_loss, l_motion\n",
        "\n",
        "def train_pignn_epoch(model, loader, optimizer, alpha_p, device):\n",
        "    model.train()\n",
        "    total_loss, total_phys = 0, 0\n",
        "    for data in loader:\n",
        "        # ãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒæ•°ã‚„ãƒ‡ãƒã‚¤ã‚¹è»¢é€ã‚’è¡Œã†å‰å‡¦ç†\n",
        "        data = preprocess_batch(data, device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "\n",
        "        # æå¤±è¨ˆç®—\n",
        "        c_loss, p_loss = pignn_loss_function(out, data.y.view(-1), data.vel, alpha_p)\n",
        "        loss = c_loss + alpha_p * p_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "        total_phys += p_loss.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(loader.dataset), total_phys / len(loader.dataset)\n",
        "\n",
        "def test_pignn(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = preprocess_batch(data, device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == data.y.view(-1)).sum().item()\n",
        "    return correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "hIyIVn-LHCPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "def balance_dataset_by_undersampling(data_list):\n",
        "    \"\"\"\n",
        "    ã‚·ãƒ¼ã‚¯ã‚¨ãƒ³ã‚¹ï¼ˆä¸€é€£ã®ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰å˜ä½ã§ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’è¡Œã„ã€\n",
        "    æˆåŠŸ(1)ã¨å¤±æ•—(0)ã®æ¯”ç‡ã‚’1:1ã«æ•´ãˆã‚‹é–¢æ•°ã€‚\n",
        "    \"\"\"\n",
        "    # 1. sequence_id ã”ã¨ã«ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–\n",
        "    seq_groups = defaultdict(list)\n",
        "    for d in data_list:\n",
        "        # sequence_id ãŒ Tensor ã®å ´åˆã¯ .item() ã§æ•°å€¤åŒ–\n",
        "        sid = int(d.sequence_id.item()) if torch.is_tensor(d.sequence_id) else int(d.sequence_id)\n",
        "        seq_groups[sid].append(d)\n",
        "\n",
        "    # 2. æˆåŠŸã‚°ãƒ«ãƒ¼ãƒ—ã¨å¤±æ•—ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†ã‘ã‚‹\n",
        "    l0_groups = []\n",
        "    l1_groups = []\n",
        "    for sid, frames in seq_groups.items():\n",
        "        label = int(frames[0].y.item())\n",
        "        if label == 0:\n",
        "            l0_groups.append(frames)\n",
        "        else:\n",
        "            l1_groups.append(frames)\n",
        "\n",
        "    # 3. æ•°ã®å°‘ãªã„æ–¹ï¼ˆé€šå¸¸ã¯æˆåŠŸã‚°ãƒ«ãƒ¼ãƒ—ï¼‰ã«åˆã‚ã›ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
        "    min_size = min(len(l0_groups), len(l1_groups))\n",
        "\n",
        "    # å¤±æ•—ã‚°ãƒ«ãƒ¼ãƒ—ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«æŠ½å‡º\n",
        "    sampled_l0 = random.sample(l0_groups, min_size)\n",
        "    # æˆåŠŸã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆå…¨æ•°ã€ã‚ã‚‹ã„ã¯åŒæ•°æŠ½å‡ºï¼‰\n",
        "    sampled_l1 = random.sample(l1_groups, min_size)\n",
        "\n",
        "    # 4. ãƒªã‚¹ãƒˆã«æˆ»ã—ã¦ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
        "    balanced_list = [frame for group in (sampled_l0 + sampled_l1) for frame in group]\n",
        "    random.shuffle(balanced_list)\n",
        "\n",
        "    print(f\"    [Sampling] Success Seqs: {len(sampled_l1)} | Failure Seqs: {len(sampled_l0)} | Total Frames: {len(balanced_list)}\")\n",
        "    return balanced_list"
      ],
      "metadata": {
        "id": "xLJSMj6S8Wvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ==========================================\n",
        "# 4. ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ (CV) å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³ (ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆ)\n",
        "# ==========================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 1. ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰\n",
        "v15_load_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_v15_final.pt\"\n",
        "print(f\"CVç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {v15_load_path}\")\n",
        "checkpoint = torch.load(v15_load_path, weights_only=False)\n",
        "all_data_list = checkpoint['all_data']\n",
        "\n",
        "# --- ã€ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆã€‘match_idãŒãªã„å ´åˆã€sequence_idã‹ã‚‰å–å¾— ---\n",
        "# sequence_idãŒ 1001 ãªã‚‰ 1è©¦åˆç›®ã€2005 ãªã‚‰ 2è©¦åˆç›® ã¨åˆ¤å®šï¼ˆv14ä»¥å‰ã®ä»•æ§˜ï¼‰\n",
        "# ã‚‚ã—ã‚ãªãŸã® sequence_id ãŒå˜ãªã‚‹é€£ç•ª(0, 1, 2...)ãªã‚‰ã€ã“ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯ä½¿ãˆã¾ã›ã‚“ã€‚\n",
        "for d in all_data_list:\n",
        "    if not hasattr(d, 'match_id'):\n",
        "        # sequence_id ã‚’ 1000 ã§å‰²ã£ã¦è©¦åˆIDã‚’ä½œã‚‹ (1001 // 1000 = 1)\n",
        "        sid = int(d.sequence_id.item()) if torch.is_tensor(d.sequence_id) else int(d.sequence_id)\n",
        "        d.match_id = torch.tensor([sid // 1000])\n",
        "\n",
        "# å…¨è©¦åˆIDã‚’è‡ªå‹•å–å¾—\n",
        "match_ids = sorted(list(set([int(d.match_id.item()) for d in all_data_list])))\n",
        "print(f\"æ¤œå‡ºã•ã‚ŒãŸè©¦åˆID: {match_ids}\")\n",
        "\n",
        "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "EPOCHS_CV = 30\n",
        "LR = 0.0005\n",
        "ALPHA_P = 0.01\n",
        "cv_final_reports = []\n",
        "\n",
        "print(f\"PIGNN ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ (å…¨ {len(match_ids)} è©¦åˆ)\\n\")\n",
        "\n",
        "best_overall_f1 = 0 # æœ€é«˜ã®F1ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²ç”¨\n",
        "\n",
        "for test_match in match_ids:\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\" Round: Match {test_match} ã‚’ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # 2. ãƒ‡ãƒ¼ã‚¿ã®åˆ‡ã‚Šåˆ†ã‘ (ä¿®æ­£ã—ãŸ match_id ã‚’ä½¿ç”¨)\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match]\n",
        "\n",
        "    # 3. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
        "    cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "\n",
        "    cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "    cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # 4. ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–\n",
        "    cv_model = PIGNNClassifier(hidden_channels=32).to(device)\n",
        "    cv_optimizer = torch.optim.Adam(cv_model.parameters(), lr=LR)\n",
        "\n",
        "    # 5. å­¦ç¿’ãƒ«ãƒ¼ãƒ—\n",
        "    for epoch in range(1, EPOCHS_CV + 1):\n",
        "        loss, _ = train_pignn_epoch(cv_model, cv_train_loader, cv_optimizer, ALPHA_P, device)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"  Epoch {epoch:02d} | Loss: {loss:.4f}\")\n",
        "\n",
        "    # 6. è©•ä¾¡\n",
        "    cv_model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in cv_test_loader:\n",
        "            data = preprocess_batch(data, device)\n",
        "            out = cv_model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    #è‰¯ã‹ã£ãŸãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜\n",
        "    current_f1 = report['1']['f1-score']\n",
        "    if current_f1 > best_overall_f1:\n",
        "        best_overall_f1 = current_f1\n",
        "        # æœ€ã‚‚æ±åŒ–æ€§èƒ½ãŒé«˜ã„ï¼ˆæœªçŸ¥ã®è©¦åˆã«å¼·ã„ï¼‰é‡ã¿ã‚’ä¿å­˜\n",
        "        torch.save(cv_model.state_dict(), 'best_pignn_cv_model.pth')\n",
        "        print(f\"  New Best Model Saved (Match {test_match} Test F1: {current_f1:.4f})\")\n",
        "\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    cv_final_reports.append({\n",
        "        'match': test_match,\n",
        "        'recall': report['1']['recall'],\n",
        "        'f1': report['1']['f1-score'],\n",
        "        'support': report['1']['support']\n",
        "    })\n",
        "    print(f\" >> Result: Recall(Success)={report['1']['recall']:.4f}, F1={report['1']['f1-score']:.4f}\")\n",
        "\n",
        "# æœ€çµ‚é›†è¨ˆ (çœç•¥ãªã—)\n",
        "print(f\"\\n\\n{'#'*50}\\n 7è©¦åˆã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ æœ€çµ‚å¹³å‡çµæœ\\n{'#'*50}\")\n",
        "avg_recall = np.mean([r['recall'] for r in cv_final_reports])\n",
        "avg_f1 = np.mean([r['f1'] for r in cv_final_reports])\n",
        "print(f\"\\n[OVERALL] Average Success Recall: {avg_recall:.4f}\")\n",
        "print(f\"[OVERALL] Average Success F1-score: {avg_f1:.4f}\")"
      ],
      "metadata": {
        "id": "XvcuJczK-BWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- å±¥æ­´ä¿å­˜ç”¨ã®ãƒªã‚¹ãƒˆ ---\n",
        "cv_histories = []\n",
        "\n",
        "for test_match in match_ids:\n",
        "    print(f\"\\nRound: Match {test_match}\")\n",
        "    round_history = {'train_loss': [], 'test_acc': []} # ã“ã®Roundå°‚ç”¨ã®å±¥æ­´\n",
        "\n",
        "    # ... (ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®åˆæœŸåŒ–ã¯å‰è¿°ã®é€šã‚Š) ...\n",
        "\n",
        "    for epoch in range(1, EPOCHS_CV + 1):\n",
        "        loss, _ = train_pignn_epoch(cv_model, cv_train_loader, cv_optimizer, ALPHA_P, device)\n",
        "        acc = test_pignn(cv_model, cv_test_loader, device)\n",
        "\n",
        "        # æ¯ã‚¨ãƒãƒƒã‚¯è¨˜éŒ²ã™ã‚‹\n",
        "        round_history['train_loss'].append(loss)\n",
        "        round_history['test_acc'].append(acc)\n",
        "\n",
        "    cv_histories.append(round_history)\n",
        "\n",
        "# ==========================================\n",
        "# ã‚°ãƒ©ãƒ•æç”»ï¼š7è©¦åˆåˆ†ã®è»Œè·¡ã‚’é‡ã­ã‚‹\n",
        "# ==========================================\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# 1. Train Loss ã®æ¨ç§»\n",
        "plt.subplot(1, 2, 1)\n",
        "for i, hist in enumerate(cv_histories):\n",
        "    plt.plot(hist['train_loss'], label=f'Match {match_ids[i]}', alpha=0.7)\n",
        "plt.title('Train Loss (7 Rounds)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Test Accuracy ã®æ¨ç§»\n",
        "plt.subplot(1, 2, 2)\n",
        "for i, hist in enumerate(cv_histories):\n",
        "    plt.plot(hist['test_acc'], label=f'Match {match_ids[i]}', alpha=0.7)\n",
        "plt.title('Test Accuracy (7 Rounds)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0.5, 1.0) # Accuracyã®ç¯„å›²ã‚’å›ºå®š\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0kCVXV6K9tLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# å±¥æ­´ã¨ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜ç”¨\n",
        "cv_histories = []\n",
        "cv_final_reports = []\n",
        "\n",
        "print(f\"PIGNN ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ (å…¨ {len(match_ids)} è©¦åˆ)\")\n",
        "\n",
        "for test_match in match_ids:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"  Round: Match {test_match} ã‚’ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨\")\n",
        "\n",
        "    # --- ãƒ‡ãƒ¼ã‚¿æº–å‚™ (å‰è¿°ã®é€šã‚Š) ---\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match]\n",
        "    cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "    cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "    cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # --- ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ– ---\n",
        "    cv_model = PIGNNClassifier(hidden_channels=32).to(device)\n",
        "    cv_optimizer = torch.optim.Adam(cv_model.parameters(), lr=LR)\n",
        "\n",
        "    round_history = {'train_loss': [], 'test_acc': []}\n",
        "\n",
        "    # --- å­¦ç¿’ãƒ«ãƒ¼ãƒ— ---\n",
        "    for epoch in range(1, EPOCHS_CV + 1):\n",
        "        loss, _ = train_pignn_epoch(cv_model, cv_train_loader, cv_optimizer, ALPHA_P, device)\n",
        "        acc = test_pignn(cv_model, cv_test_loader, device)\n",
        "\n",
        "        round_history['train_loss'].append(loss)\n",
        "        round_history['test_acc'].append(acc)\n",
        "\n",
        "    cv_histories.append(round_history)\n",
        "\n",
        "    # --- ã€ã“ã“ãŒé‡è¦ã€‘å„Roundçµ‚äº†å¾Œã«è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºã™ ---\n",
        "    cv_model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in cv_test_loader:\n",
        "            data = preprocess_batch(data, device)\n",
        "            out = cv_model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    # ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º\n",
        "    print(f\"\\n--- Match {test_match} æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ ---\")\n",
        "    rep = classification_report(y_true, y_pred, target_names=['Fail(0)', 'Success(1)'], zero_division=0)\n",
        "    print(rep)\n",
        "\n",
        "    # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜\n",
        "    cv_final_reports.append(classification_report(y_true, y_pred, output_dict=True, zero_division=0))\n",
        "\n",
        "# ==========================================\n",
        "# ã‚°ãƒ©ãƒ•æç”»ï¼ˆå­¦ç¿’æ›²ç·šï¼‰\n",
        "# ==========================================\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "for i, h in enumerate(cv_histories):\n",
        "    plt.plot(h['train_loss'], label=f'M{match_ids[i]}')\n",
        "plt.title('Train Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "for i, h in enumerate(cv_histories):\n",
        "    plt.plot(h['test_acc'], label=f'M{match_ids[i]}')\n",
        "plt.title('Test Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YFyM9YpTHYO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãƒ©ãƒ³ãƒ€ãƒ è©¦é¨“"
      ],
      "metadata": {
        "id": "oVB8AUc1Al4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ==========================================\n",
        "# ã‚·ãƒ£ãƒƒãƒ•ãƒ«è©¦é¨“ (Label Permutation Test) å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³\n",
        "# ==========================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 1. ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆï¼ˆå…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’å£Šã•ãªã„ãŸã‚ï¼‰\n",
        "shuffled_data_list = copy.deepcopy(all_data_list)\n",
        "\n",
        "# 2. å…¨ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«(y)ã ã‘ã‚’æŠ½å‡ºã—ã¦ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
        "all_labels = [int(d.y.item()) for d in shuffled_data_list]\n",
        "random.seed(42) # å†ç¾æ€§ã®ãŸã‚ã«ã‚·ãƒ¼ãƒ‰å›ºå®š\n",
        "random.shuffle(all_labels)\n",
        "\n",
        "# 3. ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ãŸãƒ©ãƒ™ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ã«æˆ»ã™ï¼ˆç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã®ç›¸é–¢ã‚’ç ´å£Šï¼‰\n",
        "for i, d in enumerate(shuffled_data_list):\n",
        "    shuffled_data_list[i].y = torch.tensor([all_labels[i]], dtype=torch.long)\n",
        "\n",
        "print(\"ãƒ©ãƒ™ãƒ«ã‚’å®Œå…¨ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¾ã—ãŸã€‚å˜˜ç™ºè¦‹å™¨ï¼ˆCVï¼‰ã‚’é–‹å§‹ã—ã¾ã™ã€‚\")\n",
        "print(\"â€»ã“ã‚Œã§ç²¾åº¦ãŒå‡ºãªã‘ã‚Œã°ã€ã‚ãªãŸã®å…ƒã®ãƒ¢ãƒ‡ãƒ«ã¯ã€é¸æ‰‹ã®å‹•ãã€ã‚’æ­£ã—ãå­¦ã‚“ã§ã„ãŸè¨¼æ‹ ã§ã™ã€‚\\n\")\n",
        "\n",
        "# è©¦é¨“ç”¨ãªã®ã§ã€æœ€åˆã®3è©¦åˆï¼ˆRoundï¼‰ã ã‘ã§ååˆ†åˆ¤å®šå¯èƒ½ã§ã™\n",
        "test_rounds = match_ids[:3]\n",
        "shuffled_cv_results = []\n",
        "\n",
        "for test_match in test_rounds:\n",
        "    print(f\"ğŸŒ€ Testing Shuffled Round: Match {test_match}\")\n",
        "\n",
        "    # ãƒ‡ãƒ¼ã‚¿ã®åˆ‡ã‚Šåˆ†ã‘\n",
        "    test_indices = [d for d in shuffled_data_list if int(d.match_id.item()) == test_match]\n",
        "    train_candidates = [d for d in shuffled_data_list if int(d.match_id.item()) != test_match]\n",
        "\n",
        "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
        "    cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "    cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "    cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # ãƒ¢ãƒ‡ãƒ«ãƒªã‚»ãƒƒãƒˆ\n",
        "    cv_model = PIGNNClassifier(hidden_channels=32).to(device)\n",
        "    cv_optimizer = torch.optim.Adam(cv_model.parameters(), lr=0.0005)\n",
        "\n",
        "    # å­¦ç¿’ (ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€æœ¬æ¥ã¯ä½•ã‚‚å­¦ã¹ãªã„ã¯ãš)\n",
        "    for epoch in range(1, 21): # è©¦é¨“ãªã®ã§20ã‚¨ãƒãƒƒã‚¯ã§ååˆ†\n",
        "        train_pignn_epoch(cv_model, cv_train_loader, cv_optimizer, 0.01, device)\n",
        "\n",
        "    # è©•ä¾¡\n",
        "    cv_model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in cv_test_loader:\n",
        "            data = preprocess_batch(data, device)\n",
        "            out = cv_model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    recall_1 = report['1']['recall']\n",
        "    shuffled_cv_results.append(recall_1)\n",
        "    print(f\" >> Shuffled Recall(Success): {recall_1:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\" ã‚·ãƒ£ãƒƒãƒ•ãƒ«è©¦é¨“ çµæœå ±å‘Š\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"å¹³å‡ã‚·ãƒ£ãƒƒãƒ•ãƒ«Recall: {np.mean(shuffled_cv_results):.4f}\")\n",
        "if np.mean(shuffled_cv_results) > 0.7:\n",
        "    print(\" è­¦å‘Š: ãƒ©ãƒ™ãƒ«ã‚’å£Šã—ã¦ã‚‚ç²¾åº¦ãŒå‡ºã¦ã„ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã®ãƒªãƒ¼ã‚¯(æ¼æ´©)ãŒå¼·ãç–‘ã‚ã‚Œã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\" åˆæ ¼: ãƒ©ãƒ™ãƒ«ã‚’å£Šã™ã¨ç²¾åº¦ãŒè½ã¡ã¾ã—ãŸã€‚å…ƒã®ãƒ¢ãƒ‡ãƒ«ã¯æ­£ã—ãç‰¹å¾´ã‚’å­¦ç¿’ã—ã¦ã„ã¾ã™ã€‚\")"
      ],
      "metadata": {
        "id": "MUMeH4obAjlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã¨ã‚Šã‚ãˆãšéå­¦ç¿’ã¯å¤§ä¸ˆå¤«èª¬ï¼Ÿã‚ã¨ã¯ã€åˆ¤å®šã—ãã„å€¤ã‚’èª¿æ•´ã™ã‚‹ï¼ˆä¸Šã’ã‚Œã°ï¼‰ã€ç©ºæŒ¯ã‚Šç‡ã¯æ¸›ã‚‹ã€‚"
      ],
      "metadata": {
        "id": "4mBjtCuLA3Jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä»¥ä¸ŠãŒPIGNNã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«"
      ],
      "metadata": {
        "id": "sVwMYowN6Qvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PFI"
      ],
      "metadata": {
        "id": "IzPgBIdiveAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 1. ä¿å­˜ã—ãŸæœ€æ–°ã® PIGNN ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "save_path = \"best_pignn_cv_model.pth\"\n",
        "\n",
        "# 2. ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ– (å­¦ç¿’æ™‚ã¨åŒã˜ PIGNNClassifier ã‚’ä½¿ã†)\n",
        "# â€» PIGNNClassifier ã®å®šç¾©ãŒåŒã˜ã‚»ãƒ«ã‹ä»¥å‰ã®ã‚»ãƒ«ã«ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™\n",
        "model = PIGNNClassifier(hidden_channels=32)\n",
        "\n",
        "# 3. é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if os.path.exists(save_path):\n",
        "    # weights_only=True ã¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¸Šæ¨å¥¨ã•ã‚Œã‚‹è¨­å®šã§ã™\n",
        "    checkpoint = torch.load(save_path, map_location=device, weights_only=True)\n",
        "    model.load_state_dict(checkpoint)\n",
        "\n",
        "    # 4. ãƒ‡ãƒã‚¤ã‚¹è»¢é€ã¨è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"PIGNNãƒ¢ãƒ‡ãƒ« ã®ãƒ­ãƒ¼ãƒ‰ã«æˆåŠŸã—ã¾ã—ãŸï¼ä¿å­˜ã—ãŸé‡ã¿ãŒé©ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(f\"ã‚¨ãƒ©ãƒ¼: {save_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä¿å­˜ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")"
      ],
      "metadata": {
        "id": "Qm0xYfDpuD6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# 1. è©•ä¾¡ã«ä½¿ã†è©¦åˆã‚’æ±ºã‚ã‚‹ (ä¾‹: Match 7)\n",
        "# å…¨ãƒ‡ãƒ¼ã‚¿ã§è¨ˆç®—ã—ãŸã„å ´åˆã¯ test_match_id = None ã«ã—ã¦ä¸‹ã‚’èª¿æ•´\n",
        "test_match_id = 7\n",
        "\n",
        "# 2. ãã®è©¦åˆã®ãƒ‡ãƒ¼ã‚¿ã ã‘ã‚’æŠ½å‡º\n",
        "final_test_set = [d for d in all_data_list if int(d.match_id.item()) == test_match_id]\n",
        "\n",
        "# 3. test_loader ã¨ã—ã¦å®šç¾© (NameError ã‚’è§£æ¶ˆ)\n",
        "test_loader = DataLoader(final_test_set, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Match {test_match_id} ã®ãƒ‡ãƒ¼ã‚¿ ({len(final_test_set)} frames) ã§ test_loader ã‚’ä½œæˆã—ã¾ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "IO5YxowQCk3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_feature_importance_pignn_v2(model, loader, device):\n",
        "    model.eval()\n",
        "    data_list = list(loader)\n",
        "\n",
        "    node_features = ['pos_x', 'pos_y', 'vel_x', 'vel_y', 'dist_goal', 'dist_ball', 'team_id']\n",
        "    importances = []\n",
        "\n",
        "    # 1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ã€Recallï¼ˆæˆåŠŸå†ç¾ç‡ï¼‰ã€ã‚’è¨ˆç®—\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in data_list:\n",
        "            batch_data = preprocess_batch(data.clone(), device)\n",
        "            out = model(batch_data)\n",
        "            y_true.extend(batch_data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    # æˆåŠŸ(1)ã«å¯¾ã™ã‚‹Recallã‚’è¨ˆç®—\n",
        "    from sklearn.metrics import recall_score\n",
        "    baseline_recall = recall_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
        "    print(f\"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ Recall (Success): {baseline_recall:.4f}\")\n",
        "\n",
        "    # 2. å„ç‰¹å¾´é‡ã‚’é †ç•ªã«ç ´å£Šã—ã¦ Recall ã®ä½ä¸‹ã‚’è¦‹ã‚‹\n",
        "    for i in range(len(node_features)):\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for data in data_list:\n",
        "                batch_data = preprocess_batch(data.clone(), device)\n",
        "                batch_data.x[:, i] = 0 # ç‰¹å¾´é‡ã‚’ç ´å£Š\n",
        "                out = model(batch_data)\n",
        "                preds.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "        drop_recall = recall_score(y_true, preds, pos_label=1, zero_division=0)\n",
        "        importance = baseline_recall - drop_recall\n",
        "        importances.append(max(0, importance))\n",
        "        print(f\"ç‰¹å¾´é‡ '{node_features[i]}' ç ´å£Š -> Recallä½ä¸‹: {importance:.4f} (æ®‹ã‚ŠRecall: {drop_recall:.4f})\")\n",
        "\n",
        "    return node_features, importances\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "feature_names, importance_values = calculate_feature_importance_pignn_v2(model, test_loader, device)"
      ],
      "metadata": {
        "id": "fSe8LVVaasiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "åº§æ¨™ã«ä¾å­˜ã—ã¦ã„ã‚‹ãŸã‚ã€åº§æ¨™ã‚’æŠœã„ã¦å­¦ç¿’ã‚’è©¦ã¿ã¦ã¿ã‚‹"
      ],
      "metadata": {
        "id": "uUsW8RsuFpOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#å†å®šç¾©\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GATv2Conv, global_mean_pool\n",
        "\n",
        "class PIGNNClassifier(nn.Module):\n",
        "    # in_channels ã‚’å¼•æ•°ã§å—ã‘å–ã‚Šã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä»¥å‰ã® 7 ã«è¨­å®š\n",
        "    def __init__(self, in_channels=7, hidden_channels=32):\n",
        "        super(PIGNNClassifier, self).__init__()\n",
        "        # ç‰©ç†ãƒã‚¤ã‚¢ã‚¹ã‚’è€ƒæ…®ã—ãŸGATå±¤ï¼ˆin_channels ã‚’å¤‰æ•°ã«ã™ã‚‹ï¼‰\n",
        "        self.conv1 = GATv2Conv(in_channels, hidden_channels, heads=4, concat=True)\n",
        "        self.conv2 = GATv2Conv(hidden_channels * 4, hidden_channels, heads=4, concat=False)\n",
        "\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        # ç¬¬1å±¤\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        # ç¬¬2å±¤\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ¼ãƒªãƒ³ã‚°ï¼ˆã‚°ãƒ©ãƒ•å…¨ä½“ã®ç‰¹å¾´é‡ã¸ï¼‰\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # å‡ºåŠ›å±¤ï¼ˆLogSoftmaxã‚’æƒ³å®šï¼‰\n",
        "        x = self.lin(x)\n",
        "        return torch.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "p-UMAY_UE60y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ï¼‘ï½ï¼–è©¦åˆã§å­¦ç¿’ã€ï¼—è©¦åˆã§ãƒ†ã‚¹ãƒˆ"
      ],
      "metadata": {
        "id": "nBsYnhOIFzI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# åº§æ¨™ã‚’é™¤å»ã—ãŸã€Œæˆ¦è¡“ãƒ»ç‰©ç†é‡ç‰¹åŒ–å‹ã€å­¦ç¿’\n",
        "# ==========================================\n",
        "\n",
        "# 1. ç‰©ç†é‡ã®ã¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŒ‡å®š\n",
        "# å…ƒã®7æ¬¡å…ƒ: 0:pos_x, 1:pos_y, 2:vel_x, 3:vel_y, 4:dist_goal, 5:dist_ball, 6:team_id\n",
        "# ç‰©ç†é‡ã®ã¿: 2, 3, 4, 5, 6 (è¨ˆ5æ¬¡å…ƒ)\n",
        "PHYSICAL_INDICES = [2, 3, 4, 5, 6]\n",
        "\n",
        "def preprocess_batch_no_pos(data, device):\n",
        "    \"\"\"åº§æ¨™ã‚’å‰Šé™¤ã—ã€ç‰©ç†é‡ã®ã¿ã‚’æ®‹ã™å‰å‡¦ç†\"\"\"\n",
        "    data = data.to(device)\n",
        "    # 2~6åˆ—ç›®ï¼ˆé€Ÿåº¦ã€è·é›¢ã€IDï¼‰ã ã‘ã‚’æŠ½å‡º\n",
        "    data.x = data.x[:, PHYSICAL_INDICES]\n",
        "    return data\n",
        "\n",
        "# 2. ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ– (in_channels=5 ã«å¤‰æ›´)\n",
        "# â€»ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã®å®šç¾©ãŒ in_channels ã‚’å¼•æ•°ã§å–ã‚Œã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™\n",
        "tactical_model = PIGNNClassifier(in_channels=5, hidden_channels=32).to(device)\n",
        "tactical_optimizer = torch.optim.Adam(tactical_model.parameters(), lr=0.0005)\n",
        "\n",
        "# 3. å­¦ç¿’ãƒ«ãƒ¼ãƒ— (CVã®1ãƒ©ã‚¦ãƒ³ãƒ‰åˆ†ã€ã¾ãŸã¯å…¨ãƒ‡ãƒ¼ã‚¿ã§å®Ÿæ–½)\n",
        "# ã“ã“ã§ã¯æ¯”è¼ƒã®ãŸã‚ã€å‰å›ã¨åŒã˜ Match 7 ã‚’ãƒ†ã‚¹ãƒˆã«ã™ã‚‹\n",
        "test_match_id = 7\n",
        "test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match_id]\n",
        "train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match_id]\n",
        "\n",
        "# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰\n",
        "cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"åº§æ¨™æŠœããƒ¢ãƒ‡ãƒ«å­¦ç¿’é–‹å§‹ (Input: 5 features)\")\n",
        "\n",
        "for epoch in range(1, 31):\n",
        "    # å­¦ç¿’ (å‰å‡¦ç†ã§åº§æ¨™ã‚’æŠœã)\n",
        "    tactical_model.train()\n",
        "    total_loss = 0\n",
        "    for data in cv_train_loader:\n",
        "        data = preprocess_batch_no_pos(data, device)\n",
        "        tactical_optimizer.zero_grad()\n",
        "        out = tactical_model(data)\n",
        "        loss = F.nll_loss(out, data.y)\n",
        "        loss.backward()\n",
        "        tactical_optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch:02d} | Loss: {total_loss/len(cv_train_loader):.4f}\")\n",
        "\n",
        "# 4. è©•ä¾¡\n",
        "tactical_model.eval()\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for data in cv_test_loader:\n",
        "        data = preprocess_batch_no_pos(data, device)\n",
        "        out = tactical_model(data)\n",
        "        y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "        y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "print(f\"\\n--- åº§æ¨™æŠœããƒ¢ãƒ‡ãƒ« (Match {test_match_id}) ãƒ¬ãƒãƒ¼ãƒˆ ---\")\n",
        "print(classification_report(y_true, y_pred, target_names=['Fail', 'Success'], zero_division=0))"
      ],
      "metadata": {
        "id": "VFZHcovMEnSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³"
      ],
      "metadata": {
        "id": "KBQKnnLpF4Yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "from sklearn.metrics import classification_report, recall_score\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# --- è¨­å®š ---\n",
        "PHYSICAL_INDICES = [2, 3, 4, 5, 6] # vel_x, vel_y, dist_goal, dist_ball, team_id\n",
        "cv_results = []\n",
        "best_recall = 0\n",
        "best_model_path = \"best_tactical_model_no_pos.pth\"\n",
        "\n",
        "print(f\"åº§æ¨™æŠœããƒ»5æ¬¡å…ƒå…¥åŠ›ãƒ¢ãƒ‡ãƒ«ï¼šå…¨ {len(match_ids)} è©¦åˆã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹\")\n",
        "\n",
        "for test_match in match_ids:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\" ğŸŒ€ Round: Match {test_match} ã‚’ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨\")\n",
        "\n",
        "    # 1. ãƒ‡ãƒ¼ã‚¿ã®åˆ‡ã‚Šåˆ†ã‘\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match]\n",
        "\n",
        "    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
        "    cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "\n",
        "    cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "    cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # 2. ãƒ¢ãƒ‡ãƒ«ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®åˆæœŸåŒ– (in_channels=5)\n",
        "    # â€»ã‚¯ãƒ©ã‚¹å®šç¾©ãŒ in_channels ã‚’å—ã‘å–ã‚Œã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹ã“ã¨ãŒå‰æ\n",
        "    cv_model = PIGNNClassifier(in_channels=5, hidden_channels=32).to(device)\n",
        "    cv_optimizer = torch.optim.Adam(cv_model.parameters(), lr=0.0005)\n",
        "\n",
        "    # 3. å­¦ç¿’ãƒ«ãƒ¼ãƒ— (30 Epoch)\n",
        "    # --- è¿½åŠ ï¼šç‰©ç†æå¤±ã‚’è¨˜éŒ²ã™ã‚‹ãƒªã‚¹ãƒˆ ---\n",
        "    for epoch in range(1, 31):\n",
        "        cv_model.train()\n",
        "        epoch_phys_loss = 0\n",
        "        for data in cv_train_loader:\n",
        "            data = data.to(device)\n",
        "            # åº§æ¨™æŠœãå‰å‡¦ç†\n",
        "            data.x = data.x[:, PHYSICAL_INDICES]\n",
        "\n",
        "            cv_optimizer.zero_grad()\n",
        "            out = cv_model(data)\n",
        "            loss = F.nll_loss(out, data.y)\n",
        "            loss.backward()\n",
        "            cv_optimizer.step()\n",
        "\n",
        "\n",
        "    # 4. è©•ä¾¡\n",
        "    cv_model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in cv_test_loader:\n",
        "            data = data.to(device)\n",
        "            data.x = data.x[:, PHYSICAL_INDICES] # åº§æ¨™æŠœã\n",
        "            out = cv_model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    # ãƒ¬ãƒãƒ¼ãƒˆã¨Recallã®ç®—å‡º\n",
        "    rep = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    rec_1 = rep['1']['recall']\n",
        "    cv_results.append(rec_1)\n",
        "\n",
        "    print(f\"--- Match {test_match} çµæœ ---\")\n",
        "    print(f\"Recall(Success): {rec_1:.4f} | Accuracy: {rep['accuracy']:.4f}\")\n",
        "\n",
        "    # æœ€ã‚‚RecallãŒé«˜ã‹ã£ãŸãƒ¢ãƒ‡ãƒ«ï¼ˆæˆ¦è¡“çœ¼ãŒé‹­ã„ãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’ä¿å­˜\n",
        "    if rec_1 > best_recall:\n",
        "        best_recall = rec_1\n",
        "        torch.save(cv_model.state_dict(), best_model_path)\n",
        "        print(f\" â­ Best Model Updated! (Recall: {best_recall:.4f})\")\n",
        "\n",
        "# 5. å…¨ä½“çµæœã®é›†è¨ˆ\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\" ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ æœ€çµ‚å ±å‘Š\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"å…¨è©¦åˆã®Recall(Success): {cv_results}\")\n",
        "print(f\"å¹³å‡Recall(Success): {np.mean(cv_results):.4f}\")\n",
        "print(f\"æœ€å¼·ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {best_model_path}\")"
      ],
      "metadata": {
        "id": "ljjgIEPGF5Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã®å¯è¦–åŒ–"
      ],
      "metadata": {
        "id": "nNhUKPcP6W4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã®å¯è¦–åŒ–\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def visualize_pignn_final_v3(model, loader, sample_idx=0):\n",
        "    model.eval()\n",
        "    data = next(iter(loader)).to(device)\n",
        "\n",
        "    # æ¨è«–\n",
        "    input_data = data.clone()\n",
        "    input_data = preprocess_batch(input_data, device)\n",
        "    with torch.no_grad():\n",
        "        out = model(input_data)\n",
        "        probs = torch.exp(out)\n",
        "        preds = out.argmax(dim=1)\n",
        "\n",
        "    mask = (data.batch == sample_idx)\n",
        "    pos = data.pos[mask].cpu().numpy()\n",
        "    vel = data.vel[mask].cpu().numpy()\n",
        "\n",
        "    # ãƒãƒ¼ãƒ‰æ•°ã‚’ç¢ºèªï¼ˆ36å€‹æƒ³å®šï¼‰\n",
        "    num_nodes = pos.shape[0]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    ax.set_facecolor('#f0f0f0')\n",
        "\n",
        "    # ãƒ”ãƒƒãƒæç”»\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=False, color='black', lw=2))\n",
        "    ax.plot([0, 0], [-34, 34], color='black', alpha=0.3)\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        # --- åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã‚’ã€Œã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ã«å¤‰æ›´ ---\n",
        "        if i == num_nodes - 1: # æœ€å¾Œã®ãƒãƒ¼ãƒ‰ãŒãƒœãƒ¼ãƒ«\n",
        "            color, marker, size, z = 'gold', '*', 450, 15\n",
        "        elif i < 11: # æœ€åˆã®11äººãŒå‘³æ–¹ï¼ˆä»®å®šï¼‰\n",
        "            color, marker, size, z = 'blue', 'o', 180, 10\n",
        "        elif i < 22: # æ¬¡ã®11äººãŒæ•µï¼ˆä»®å®šï¼‰\n",
        "            color, marker, size, z = 'red', 'o', 180, 10\n",
        "        else: # ãã‚Œä»¥å¤–ã®ãƒ€ãƒŸãƒ¼ç­‰\n",
        "            color, marker, size, z = 'gray', 'o', 100, 5\n",
        "\n",
        "        # æç”»\n",
        "        ax.scatter(pos[i, 0], pos[i, 1], c=color, marker=marker, s=size, edgecolors='black', zorder=z)\n",
        "        # ãƒ™ã‚¯ãƒˆãƒ«\n",
        "        ax.arrow(pos[i, 0], pos[i, 1], vel[i, 0]*1.5, vel[i, 1]*1.5,\n",
        "                 head_width=1.0, head_length=1.2, fc=color, ec=color, alpha=0.4, zorder=z-1)\n",
        "\n",
        "    plt.title(f\"PIGNN Fixed Visualization\\nPred: {preds[sample_idx]} | Prob: {probs[sample_idx, 1]:.2%}\")\n",
        "    plt.show()\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "visualize_pignn_final_v3(model, test_loader, sample_idx=0)"
      ],
      "metadata": {
        "id": "t1Pw-F11bPc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import torch\n",
        "\n",
        "# 1. é–¢æ•°ã®å®šç¾©ï¼ˆå¼•æ•°ã« device ã‚’æ˜ç¤ºçš„ã«è¿½åŠ ï¼‰\n",
        "def visualize_pignn_v14_fixed(model, loader, sample_idx=0, device='cuda'):\n",
        "    model.eval()\n",
        "\n",
        "    # DataLoaderã‹ã‚‰1ãƒãƒƒãƒå–å¾—\n",
        "    batch = next(iter(loader)).to(device)\n",
        "\n",
        "    # æ¨è«–\n",
        "    with torch.no_grad():\n",
        "        out = model(batch)\n",
        "        # Softmaxã‚’é©ç”¨ã—ã¦ç¢ºç‡ã«ã™ã‚‹\n",
        "        probs = torch.softmax(out, dim=1)\n",
        "        preds = out.argmax(dim=1)\n",
        "\n",
        "    # æŒ‡å®šã—ãŸã‚µãƒ³ãƒ—ãƒ«(sample_idx)ã‚’æŠ½å‡º\n",
        "    mask = (batch.batch == sample_idx)\n",
        "    node_features = batch.x[mask].cpu().numpy()\n",
        "\n",
        "    # --- åº§æ¨™ã®å¾©å…ƒ (-1~1 -> 105x68m) ---\n",
        "    pos_x = node_features[:, 0] * 52.5\n",
        "    pos_y = node_features[:, 1] * 34\n",
        "    # é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ« (3, 4åˆ—ç›®)\n",
        "    vel_x = node_features[:, 2] * 5\n",
        "    vel_y = node_features[:, 3] * 5\n",
        "    # ãƒãƒ¼ãƒ å±æ€§ (7åˆ—ç›®)\n",
        "    team_val = node_features[:, 6]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    ax.set_facecolor('#2e7d32') # èŠç”Ÿã®è‰²\n",
        "\n",
        "    # ãƒ”ãƒƒãƒæç”»\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=True, color='#2e7d32', zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=False, color='white', lw=2, zorder=2))\n",
        "    ax.plot([0, 0], [-34, 34], color='white', alpha=0.5, zorder=2) # ã‚»ãƒ³ã‚¿ãƒ¼ãƒ©ã‚¤ãƒ³\n",
        "\n",
        "    for i in range(len(pos_x)):\n",
        "        if team_val[i] == 2.0: # ãƒœãƒ¼ãƒ«\n",
        "            color, marker, size, z = 'gold', '*', 400, 10\n",
        "        elif team_val[i] == 0.0: # æ”»æ’ƒå´(Home)\n",
        "            color, marker, size, z = 'blue', 'o', 150, 5\n",
        "        else: # å®ˆå‚™å´(Away)\n",
        "            color, marker, size, z = 'red', 'o', 150, 5\n",
        "\n",
        "        ax.scatter(pos_x[i], pos_y[i], c=color, marker=marker, s=size, edgecolors='white', zorder=z)\n",
        "        # é€Ÿåº¦ã®çŸ¢å°\n",
        "        ax.arrow(pos_x[i], pos_y[i], vel_x[i], vel_y[i],\n",
        "                 head_width=1.0, head_length=1.2, fc='yellow', ec='yellow', alpha=0.6, zorder=z+1)\n",
        "\n",
        "    actual_label = batch.y[sample_idx].item()\n",
        "    plt.title(f\"v14 Fixed Visualization\\nActual: {actual_label} | Pred: {preds[sample_idx]} | Prob(Success): {probs[sample_idx, 1]:.2%}\")\n",
        "    plt.xlim(-60, 60)\n",
        "    plt.ylim(-40, 40)\n",
        "    plt.show()\n",
        "\n",
        "# 2. å®Ÿè¡Œï¼ˆdeviceå¤‰æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ãªã„å ´åˆã«å‚™ãˆã¦ç›´æ¥ 'cuda' ãªã©ã‚’æŒ‡å®šã™ã‚‹ã‹ã€å®šç¾©ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰\n",
        "# ã‚‚ã—ã‚¨ãƒ©ãƒ¼ãŒå‡ºã‚‹å ´åˆã¯ device='cpu' ã«å¤‰ãˆã¦ã¿ã¦ãã ã•ã„\n",
        "visualize_pignn_v14_fixed(model, test_loader, sample_idx=0, device='cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "pUjHpCI0zZ-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_attention_v14(model, loader, sample_idx=0, device='cuda'):\n",
        "    model.eval()\n",
        "    batch = next(iter(loader)).to(device)\n",
        "\n",
        "    # 1. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã‚’å–å¾—ã—ãªãŒã‚‰æ¨è«–\n",
        "    # â€»ãƒ¢ãƒ‡ãƒ«å´ã« return_attention=True ã®ã‚ˆã†ãªæ©Ÿèƒ½ãŒã‚ã‚‹å‰æã§ã™\n",
        "    with torch.no_grad():\n",
        "        # ãƒ¢ãƒ‡ãƒ«ã®ä»•æ§˜ã«åˆã‚ã›ã¦èª¿æ•´ãŒå¿…è¦ã§ã™ãŒã€ä¸€èˆ¬çš„ã«ã¯ã“ã†å–å¾—ã—ã¾ã™\n",
        "        out, (edge_index, att_weights) = model.forward_with_attention(batch)\n",
        "        probs = torch.softmax(out, dim=1)\n",
        "\n",
        "    # ç‰¹å®šã®ã‚µãƒ³ãƒ—ãƒ«ã®ãƒã‚¹ã‚¯\n",
        "    mask = (batch.batch == sample_idx)\n",
        "    node_features = batch.x[mask].cpu().numpy()\n",
        "\n",
        "    # åº§æ¨™å¾©å…ƒ\n",
        "    pos_x = node_features[:, 0] * 52.5\n",
        "    pos_y = node_features[:, 1] * 34\n",
        "    team_val = node_features[:, 6]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    ax.set_facecolor('#1a472a') # æ¿ƒã„ç·‘\n",
        "\n",
        "    # 2. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆã‚¨ãƒƒã‚¸ï¼‰ã‚’æç”»\n",
        "    # sample_idxã«è©²å½“ã™ã‚‹ã‚¨ãƒƒã‚¸ã ã‘ã‚’ãƒ«ãƒ¼ãƒ—\n",
        "    # att_weights ãŒé«˜ã„ã‚‚ã®ã»ã©å¤ªãã€æ˜ã‚‹ã„è‰²ã§æã\n",
        "    for i in range(edge_index.shape[1]):\n",
        "        src, dst = edge_index[0, i], edge_index[1, i]\n",
        "        # ã“ã®ã‚µãƒ³ãƒ—ãƒ«å†…ã®ãƒãƒ¼ãƒ‰ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰\n",
        "        if src < len(pos_x) and dst < len(pos_x):\n",
        "            weight = att_weights[i].item()\n",
        "            if weight > 0.1: # é‡è¦ãªç¹‹ãŒã‚Šã ã‘æç”»\n",
        "                ax.plot([pos_x[src], pos_x[dst]], [pos_y[src], pos_y[dst]],\n",
        "                        color='white', alpha=weight, lw=weight*5, zorder=3)\n",
        "\n",
        "    # 3. é¸æ‰‹ã‚’æç”»ï¼ˆå…ˆã»ã©ã¨åŒã˜ï¼‰\n",
        "    for i in range(len(pos_x)):\n",
        "        color = 'gold' if team_val[i] == 2.0 else ('blue' if team_val[i] == 0.0 else 'red')\n",
        "        ax.scatter(pos_x[i], pos_y[i], c=color, s=150, edgecolors='white', zorder=5)\n",
        "\n",
        "    plt.title(\"PIGNN Attention Visualization\\n(Lines show where AI is looking)\")\n",
        "    plt.show()\n",
        "visualize_attention_v14(model, test_loader, sample_idx=0, device='cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "PdAd_kA30WVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_geometric.utils import softmax\n",
        "\n",
        "class PIGNNLayer(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, tau=1.5):\n",
        "        super(PIGNNLayer, self).__init__(aggr='add')\n",
        "        self.tau = tau\n",
        "        self.lin = nn.Linear(in_channels, out_channels)\n",
        "        self.att = nn.Parameter(torch.Tensor(1, out_channels * 2))\n",
        "        nn.init.xavier_uniform_(self.att)\n",
        "\n",
        "    def forward(self, x, edge_index, pos, vel, return_attention=False):\n",
        "        h = self.lin(x)\n",
        "        # ä¼æ’­ã€‚ç‰©ç†ãƒã‚¤ã‚¢ã‚¹ã‚’å«ã‚ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¨ˆç®—ã‚’è¡Œã†\n",
        "        out, alpha = self.propagate(edge_index, x=h, pos=pos, vel=vel)\n",
        "        if return_attention:\n",
        "            return out, (edge_index, alpha)\n",
        "        return out\n",
        "\n",
        "    def message(self, x_i, x_j, pos_i, pos_j, vel_i, vel_j, edge_index_i):\n",
        "        # æœªæ¥ä½ç½®ã®äºˆæ¸¬ï¼ˆã‚ãªãŸã®ç‰©ç†è¨­è¨ˆï¼‰\n",
        "        pos_i_pred = pos_i + vel_i * self.tau\n",
        "        pos_j_pred = pos_j + vel_j * self.tau\n",
        "        dist_future = torch.norm(pos_i_pred - pos_j_pred, dim=-1, keepdim=True)\n",
        "        physics_bias = torch.exp(-dist_future / 5.0)\n",
        "\n",
        "        # å‹•çš„ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®è¨ˆç®—\n",
        "        alpha = torch.cat([x_i, x_j], dim=-1)\n",
        "        alpha = (alpha * self.att).sum(dim=-1, keepdim=True)\n",
        "        alpha = F.leaky_relu(alpha) + physics_bias  # ç‰©ç†ãƒã‚¤ã‚¢ã‚¹ã®çµ±åˆ\n",
        "        alpha = softmax(alpha, edge_index_i)\n",
        "\n",
        "        return alpha * x_j, alpha\n",
        "\n",
        "    def aggregate(self, inputs, index, ptr=None, dim_size=None):\n",
        "        # 0ç•ªç›®ãŒãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã€1ç•ªç›®ãŒã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°\n",
        "        out = super().aggregate(inputs[0], index, ptr, dim_size)\n",
        "        return out, inputs[1]\n",
        "\n",
        "class PIGNNClassifier(nn.Module):\n",
        "    def __init__(self, in_channels=5, hidden_channels=64):\n",
        "        super(PIGNNClassifier, self).__init__()\n",
        "        self.conv1 = PIGNNLayer(in_channels, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data, return_attention=False):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        if return_attention:\n",
        "            x, att_weights = self.conv1(x, edge_index, pos, vel, return_attention=True)\n",
        "        else:\n",
        "            x = self.conv1(x, edge_index, pos, vel)\n",
        "\n",
        "        x = F.elu(x)\n",
        "        x = self.conv2(x, edge_index, pos, vel)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        out = F.log_softmax(self.lin(x), dim=1)\n",
        "\n",
        "        if return_attention:\n",
        "            return out, att_weights\n",
        "        return out"
      ],
      "metadata": {
        "id": "bIW1N_gJKSBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- è¨­å®šã¨ãƒ­ã‚°ç”¨å¤‰æ•°ã®åˆæœŸåŒ– ---\n",
        "PHYSICAL_INDICES = [2, 3, 4, 5, 6]\n",
        "alpha_p = 0.01  # ç‰©ç†ãƒšãƒŠãƒ«ãƒ†ã‚£ã®å¼·ã•\n",
        "cv_results = []\n",
        "history = {'physics_loss': []} # NameError å¯¾ç­–\n",
        "best_recall = 0\n",
        "best_model_path = \"best_tactical_model_no_pos.pth\"\n",
        "\n",
        "for test_match in match_ids:\n",
        "    print(f\"\\nğŸŒ€ Round: Match {test_match} ã‚’æ¤œè¨¼ç”¨ã«ä½¿ç”¨\")\n",
        "\n",
        "    # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match]\n",
        "    cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "    # --- ã“ã“ã‚’è¿½åŠ ï¼šæ··åœ¨ã‚’é˜²ããŸã‚ã«å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ—¦CPUã¸æˆ»ã™ ---\n",
        "    cv_train_set = [d.to('cpu') for d in cv_train_set]\n",
        "    test_indices = [d.to('cpu') for d in test_indices]\n",
        "    cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "    cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ– (in_channels=5)\n",
        "    cv_model = PIGNNClassifier(in_channels=5, hidden_channels=64).to(device)\n",
        "    cv_optimizer = torch.optim.Adam(cv_model.parameters(), lr=0.0005)\n",
        "\n",
        "    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—\n",
        "    for epoch in range(1, 31):\n",
        "        cv_model.train()\n",
        "        epoch_phys = 0\n",
        "        for data in cv_train_loader:\n",
        "            data = data.to(device)\n",
        "            current_vel = data.vel.clone()\n",
        "            data.x = data.x[:, PHYSICAL_INDICES] # åº§æ¨™æŠœã\n",
        "\n",
        "            cv_optimizer.zero_grad()\n",
        "            out = cv_model(data)\n",
        "\n",
        "            # åˆ†é¡æå¤±ã¨ç‰©ç†æå¤±ã®è¨ˆç®—\n",
        "            c_loss, p_loss = pignn_loss_function(out, data.y.view(-1), current_vel, alpha_p=alpha_p)\n",
        "            loss = c_loss + alpha_p * p_loss # ç‰©ç†ã‚’è€ƒæ…®ã—ãŸå‹•çš„å­¦ç¿’\n",
        "\n",
        "            loss.backward()\n",
        "            cv_optimizer.step()\n",
        "            epoch_phys += p_loss.item()\n",
        "\n",
        "        history['physics_loss'].append(epoch_phys / len(cv_train_loader))\n",
        "\n",
        "    # è©•ä¾¡\n",
        "    cv_model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in cv_test_loader:\n",
        "            data = data.to(device)\n",
        "            data.x = data.x[:, PHYSICAL_INDICES]\n",
        "            out = cv_model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    rec_1 = recall_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
        "    cv_results.append(rec_1)\n",
        "    print(f\"Match {test_match} - Recall: {rec_1:.4f}\")\n",
        "\n",
        "    if rec_1 > best_recall:\n",
        "        best_recall = rec_1\n",
        "        torch.save(cv_model.state_dict(), best_model_path)\n",
        "\n",
        "print(f\"\\nå¹³å‡Recall: {np.mean(cv_results):.4f}\")"
      ],
      "metadata": {
        "id": "tRS9SYeeKWkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_physics_loss_history(history):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # ç‰©ç†æå¤±ã®ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "    plt.plot(history['physics_loss'], label='Physics Penalty (Kinematic Constraint)', color='teal', linewidth=2)\n",
        "\n",
        "    # åˆ¤å®šåŸºæº–ã®ãƒ©ã‚¤ãƒ³ï¼ˆ25ä»¥ä¸‹ãªã‚‰å„ªç§€ï¼‰\n",
        "    plt.axhline(y=25, color='red', linestyle='--', alpha=0.5, label='Target Threshold')\n",
        "\n",
        "    plt.title(\"Transition of Physics Loss during Training\", fontsize=14)\n",
        "    plt.xlabel(\"Epochs\", fontsize=12)\n",
        "    plt.ylabel(\"Loss Value (Penalty)\", fontsize=12)\n",
        "    plt.grid(True, which='both', linestyle='--', alpha=0.5)\n",
        "    plt.legend()\n",
        "\n",
        "    # æ•°å€¤çš„ãªè§£é‡ˆã‚’ãƒ†ã‚­ã‚¹ãƒˆã§è¿½åŠ \n",
        "    final_phys = history['physics_loss'][-1]\n",
        "    plt.annotate(f'Final: {final_phys:.2f}',\n",
        "                 xy=(len(history['physics_loss'])-1, final_phys),\n",
        "                 xytext=(len(history['physics_loss'])-5, final_phys+5),\n",
        "                 arrowprops=dict(facecolor='black', shrink=0.05))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "plot_physics_loss_history(history)"
      ],
      "metadata": {
        "id": "XmvAqBwwLUu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pignn_loss_function(out, target_y, vel, alpha_p=0.01):\n",
        "    classification_loss = F.nll_loss(out, target_y)\n",
        "\n",
        "    # é€Ÿåº¦ã®å¤§ãã•ã‚’è¨ˆç®—\n",
        "    v_magnitude = torch.norm(vel, dim=-1)\n",
        "\n",
        "    # ã€ãƒ‡ãƒãƒƒã‚°ã€‘ã‚‚ã—ã“ã‚ŒãŒå¸¸ã«å°ã•ã„ãªã‚‰ã€ãƒšãƒŠãƒ«ãƒ†ã‚£ã¯ç™ºç”Ÿã—ãªã„\n",
        "    print(f\"Max velocity in batch: {v_magnitude.max().item()}\")\n",
        "\n",
        "    # 12m/sã‚’è¶…ãˆãŸåˆ†ã ã‘ãƒšãƒŠãƒ«ãƒ†ã‚£\n",
        "    l_motion = torch.mean(F.relu(v_magnitude - 0.11)**2)\n",
        "\n",
        "    return classification_loss, l_motion"
      ],
      "metadata": {
        "id": "2iRqLm7eL1VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. è¨­å®š ---\n",
        "alpha_p = 0.1  # ãƒšãƒŠãƒ«ãƒ†ã‚£ã®é‡ã¿ã‚’10å€ã«å¼·åŒ–ï¼ˆ0.01 -> 0.1ï¼‰\n",
        "history = {'physics_loss': []}\n",
        "PHYSICAL_INDICES = [2, 3, 4, 5, 6] # vel_x, vel_y, dist_goal, dist_ball, team_id\n",
        "\n",
        "# --- 2. ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ ---\n",
        "for test_match in match_ids:\n",
        "    print(f\"\\nğŸŒ€ Round: Match {test_match} ã‚’æ¤œè¨¼ç”¨ã«ä½¿ç”¨\")\n",
        "\n",
        "    # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match]\n",
        "    cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "\n",
        "    # ãƒ‡ãƒã‚¤ã‚¹æ··åœ¨ã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ã«CPUã¸ä¸€æ—¦æˆ»ã™\n",
        "    cv_train_set = [d.to('cpu') for d in cv_train_set]\n",
        "    cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "\n",
        "    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–\n",
        "    cv_model = PIGNNClassifier(in_channels=5, hidden_channels=64).to(device)\n",
        "    cv_optimizer = torch.optim.Adam(cv_model.parameters(), lr=0.0005)\n",
        "\n",
        "    # --- 3. å­¦ç¿’ãƒ«ãƒ¼ãƒ— ---\n",
        "    for epoch in range(1, 31):\n",
        "        cv_model.train()\n",
        "        epoch_phys_sum = 0\n",
        "\n",
        "        for data in cv_train_loader:\n",
        "            data = data.to(device)\n",
        "            current_vel = data.vel.clone() # ç‰©ç†æå¤±è¨ˆç®—ç”¨ã®ç”Ÿé€Ÿåº¦\n",
        "\n",
        "            # å…¥åŠ›ã‹ã‚‰åº§æ¨™ã‚’æŠœã\n",
        "            data.x = data.x[:, PHYSICAL_INDICES]\n",
        "\n",
        "            cv_optimizer.zero_grad()\n",
        "            out = cv_model(data)\n",
        "\n",
        "            # ã€ã“ã“ãŒå¿ƒè‡“éƒ¨ã€‘æç¤ºã•ã‚ŒãŸæå¤±é–¢æ•°ã§2ã¤ã®ãƒ­ã‚¹ã‚’è¨ˆç®—\n",
        "            c_loss, p_loss = pignn_loss_function(out, data.y.view(-1), current_vel, alpha_p=alpha_p)\n",
        "\n",
        "            # åˆç®—ã—ã¦ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆã“ã‚Œã§ç‰©ç†ã‚’å­¦ç¿’ã™ã‚‹ï¼‰\n",
        "            loss = c_loss + alpha_p * p_loss\n",
        "\n",
        "            loss.backward()\n",
        "            cv_optimizer.step()\n",
        "\n",
        "            epoch_phys_sum += p_loss.item()\n",
        "\n",
        "        # å¹³å‡ç‰©ç†æå¤±ã‚’è¨˜éŒ²\n",
        "        history['physics_loss'].append(epoch_phys_sum / len(cv_train_loader))\n",
        "\n",
        "print(\"\\nå­¦ç¿’å®Œäº†ã€‚ç‰©ç†æå¤±ã®å¯è¦–åŒ–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")"
      ],
      "metadata": {
        "id": "Y3OEadHbMvrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_physics_loss_history(history):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    # ç‰©ç†æå¤±ã®ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "    plt.plot(history['physics_loss'], label='Physics Penalty', color='teal', linewidth=2)\n",
        "\n",
        "    # --- ä¿®æ­£ï¼šã—ãã„å€¤ã‚’ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆ0.1å‰å¾Œï¼‰ã«åˆã‚ã›ã‚‹ ---\n",
        "    # 25ã ã¨é«˜ã™ãã¦ã‚°ãƒ©ãƒ•ãŒæ½°ã‚Œã‚‹ã®ã§ã€0.01ãªã©ã«è¨­å®š\n",
        "    plt.axhline(y=0.01, color='red', linestyle='--', alpha=0.5, label='Target Threshold')\n",
        "\n",
        "    plt.title(\"Transition of Physics Loss during Training\", fontsize=14)\n",
        "    plt.xlabel(\"Epochs\", fontsize=12)\n",
        "    plt.ylabel(\"Loss Value (Penalty)\", fontsize=12)\n",
        "    plt.grid(True, which='both', linestyle='--', alpha=0.5)\n",
        "    plt.legend()\n",
        "\n",
        "    final_phys = history['physics_loss'][-1]\n",
        "    # xytext ã®ãƒ—ãƒ©ã‚¹ã™ã‚‹å€¤ã‚‚å°ã•ãèª¿æ•´ï¼ˆ+5ã ã¨ç”»é¢å¤–ã«é£›ã¶ãŸã‚ï¼‰\n",
        "    plt.annotate(f'Final: {final_phys:.4f}',\n",
        "                 xy=(len(history['physics_loss'])-1, final_phys),\n",
        "                 xytext=(len(history['physics_loss'])-5, final_phys + 0.005),\n",
        "                 arrowprops=dict(facecolor='black', shrink=0.05))\n",
        "\n",
        "    plt.show()\n",
        "plot_physics_loss_history(history)"
      ],
      "metadata": {
        "id": "nYABYViJPkBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_best_attention(model_path, data_list, match_id, device):\n",
        "    # 1. æŒ‡å®šã—ãŸMatchã®æˆåŠŸã‚·ãƒ¼ãƒ³ã‚’1ã¤ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—\n",
        "    sample_data = next(d for d in data_list if int(d.match_id.item()) == match_id and d.y.item() == 1)\n",
        "\n",
        "    # 2. ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨æ¨è«–\n",
        "    model = PIGNNClassifier(in_channels=5, hidden_channels=64).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    data = sample_data.clone().to(device)\n",
        "    # ã€é‡è¦ã€‘æç”»ç”¨ã«å…ƒã®åº§æ¨™ã‚’ä¿æŒã—ã€ãƒ”ãƒƒãƒã‚µã‚¤ã‚º(105x68)ã«å¾©å…ƒ\n",
        "    raw_pos = data.x[:, :2].cpu().numpy()\n",
        "    raw_pos[:, 0] *= 105\n",
        "    raw_pos[:, 1] *= 68\n",
        "\n",
        "    # ç‰¹å¾´é‡ã‹ã‚‰åº§æ¨™ã‚’æŠœãï¼ˆå­¦ç¿’æ™‚ã¨åŒã˜è¨­å®šï¼‰\n",
        "    data.x = data.x[:, [2, 3, 4, 5, 6]]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out, (edge_index, alpha) = model(data, return_attention=True)\n",
        "\n",
        "    # 3. æç”»\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    # ãƒ”ãƒƒãƒå¤–æ ã®æç”»\n",
        "    plt.plot([0, 105, 105, 0, 0], [0, 0, 68, 68, 0], color=\"black\", lw=2)\n",
        "    plt.axvline(52.5, color=\"black\", linestyle=\"--\", alpha=0.3)\n",
        "\n",
        "    edge_index = edge_index.cpu().numpy()\n",
        "    alpha = alpha.cpu().numpy().flatten()\n",
        "    max_a = alpha.max()\n",
        "\n",
        "    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆã‚¨ãƒƒã‚¸ï¼‰ã®æç”»\n",
        "    for i in range(len(alpha)):\n",
        "        # ä¸Šä½ã®ã‚¨ãƒƒã‚¸ã ã‘æç”»ã—ã¦è¦‹ã‚„ã™ãã™ã‚‹\n",
        "        if alpha[i] > max_a * 0.2:\n",
        "            src, dst = edge_index[0, i], edge_index[1, i]\n",
        "            plt.plot([raw_pos[src, 0], raw_pos[dst, 0]],\n",
        "                     [raw_pos[src, 1], raw_pos[dst, 1]],\n",
        "                     color='orange', alpha=alpha[i]/max_a,\n",
        "                     linewidth=alpha[i]/max_a * 5, zorder=1)\n",
        "\n",
        "    # é¸æ‰‹ã®ä½ç½®ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "    teams = sample_data.x[:, 6].cpu().numpy()\n",
        "    for i in range(len(raw_pos)):\n",
        "        # ãƒãƒ¼ãƒ ID 0:æ”»æ’ƒ(é’), 1:å®ˆå‚™(èµ¤), 2:ãƒœãƒ¼ãƒ«(é»„)\n",
        "        color = 'blue' if teams[i] == 0 else 'red' if teams[i] == 1 else 'yellow'\n",
        "        size = 300 if teams[i] == 2 else 200\n",
        "        plt.scatter(raw_pos[i, 0], raw_pos[i, 1], c=color, s=size, edgecolors='black', zorder=2)\n",
        "        plt.text(raw_pos[i, 0]+1, raw_pos[i, 1]+1, f\"P{i}\", fontsize=9)\n",
        "\n",
        "    prob = torch.exp(out)[0, 1].item()\n",
        "    plt.title(f\"Match {match_id} Attention Map (Success Prob: {prob:.2%})\", fontsize=14)\n",
        "    plt.xlabel(\"Pitch Length (m)\")\n",
        "    plt.ylabel(\"Pitch Width (m)\")\n",
        "    plt.show()\n",
        "\n",
        "# å®Ÿè¡Œï¼ˆMatch 6 ã®æœ€å¼·ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œè¨¼ï¼‰\n",
        "plot_best_attention(\"best_tactical_model_no_pos.pth\", all_data_list, 6, device)"
      ],
      "metadata": {
        "id": "eTtv8TrQQZVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def plot_refined_attention(model, sample_data, device, threshold_ratio=0.6):\n",
        "    model.eval()\n",
        "    data = sample_data.clone().to(device)\n",
        "\n",
        "    # ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ç”¨ã«åº§æ¨™ä»¥å¤–ã®ç‰¹å¾´é‡ã‚’å–å¾—\n",
        "    # [2: vel_x, 3: vel_y, 4: dist_goal, 5: dist_ball, 6: team_id]\n",
        "    input_data = data.clone()\n",
        "    input_data.x = data.x[:, [2, 3, 4, 5, 6]]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out, (edge_index, alpha) = model(input_data, return_attention=True)\n",
        "\n",
        "    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®æœ€å¤§å€¤ã‚’ã“ã“ã§å®šç¾©ï¼ˆNameErrorå›é¿ï¼‰\n",
        "    alpha_np = alpha.cpu().numpy().flatten()\n",
        "    max_a = alpha_np.max()\n",
        "    edge_index_np = edge_index.cpu().numpy()\n",
        "\n",
        "    # åº§æ¨™ã®å–å¾—ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆãƒ”ãƒƒãƒã‚µã‚¤ã‚ºã«åˆã‚ã›ã‚‹ï¼‰\n",
        "    # å…ƒã®åº§æ¨™ data.x[:, :2] ãŒ -0.5~0.5 ãªã©ã®ç¯„å›²ãªã‚‰è£œæ­£ãŒå¿…è¦\n",
        "    raw_pos = data.x[:, :2].cpu().numpy()\n",
        "\n",
        "    # ã‚‚ã—é¸æ‰‹ãŒæ å¤–ãªã‚‰ã€åº§æ¨™ã‚’ã‚·ãƒ•ãƒˆã•ã›ã‚‹ï¼ˆä¾‹ï¼š+52.5, +34ï¼‰\n",
        "    # ã‚ãªãŸã®ãƒ‡ãƒ¼ã‚¿ã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„\n",
        "    pos_x = raw_pos[:, 0] * 105 + 52.5\n",
        "    pos_y = raw_pos[:, 1] * 68 + 34\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # ãƒ”ãƒƒãƒå¤–æ ï¼ˆ0~105, 0~68ï¼‰\n",
        "    plt.plot([0, 105, 105, 0, 0], [0, 0, 68, 68, 0], color=\"black\", lw=2)\n",
        "    plt.axvline(52.5, color=\"black\", linestyle=\"--\", alpha=0.3)\n",
        "\n",
        "    # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®æç”»ï¼ˆã—ãã„å€¤ã§çµã‚Šè¾¼ã¿ï¼‰\n",
        "    threshold = max_a * threshold_ratio\n",
        "    for i in range(len(alpha_np)):\n",
        "        if alpha_np[i] > threshold:\n",
        "            src, dst = edge_index_np[0, i], edge_index_np[1, i]\n",
        "            plt.plot([pos_x[src], pos_x[dst]],\n",
        "                     [pos_y[src], pos_y[dst]],\n",
        "                     color='orange',\n",
        "                     alpha=alpha_np[i]/max_a,\n",
        "                     linewidth=(alpha_np[i]/max_a) * 6, # å¤ªã•ã‚’å¼·èª¿\n",
        "                     zorder=1)\n",
        "\n",
        "    # é¸æ‰‹ã®ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "    teams = data.x[:, 6].cpu().numpy()\n",
        "    for i in range(len(pos_x)):\n",
        "        color = 'blue' if teams[i] == 0 else 'red' if teams[i] == 1 else 'yellow'\n",
        "        plt.scatter(pos_x[i], pos_y[i], c=color, s=250, edgecolors='black', zorder=2)\n",
        "        plt.text(pos_x[i]+1, pos_y[i]+1, f\"P{i}\", fontsize=10, fontweight='bold')\n",
        "\n",
        "    prob = torch.exp(out)[0, 1].item()\n",
        "    plt.title(f\"Refined Attention Map (Success Prob: {prob:.2%})\", fontsize=15)\n",
        "    plt.xlim(-5, 110)\n",
        "    plt.ylim(-5, 73)\n",
        "    plt.show()\n",
        "\n",
        "# å®Ÿè¡Œï¼ˆã—ãã„å€¤ã‚’0.6ã«ã—ã¦é‡è¦ãªç·šã ã‘ã«çµã‚‹ï¼‰\n",
        "plot_refined_attention(cv_model, sample_data, device, threshold_ratio=0.6)"
      ],
      "metadata": {
        "id": "OGhoA1-JQ5__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def plot_tactical_attention_fixed_v3(model, sample_data, device, threshold_ratio=0.5):\n",
        "    \"\"\"\n",
        "    ã‚»ãƒ³ã‚¿ãƒ¼ãƒ©ã‚¤ãƒ³0ã€æ­£è¦åŒ–çŠ¶æ…‹(-0.5~0.5)ã®ãƒ‡ãƒ¼ã‚¿ã‚’\n",
        "    105m x 68m ã®ãƒ”ãƒƒãƒã‚¹ã‚±ãƒ¼ãƒ«ã«å¾©å…ƒã—ã¦æç”»ã™ã‚‹æ±ºå®šç‰ˆ\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    data = sample_data.clone().to(device)\n",
        "\n",
        "    # 1. å…¥åŠ›ç‰¹å¾´é‡ã®æº–å‚™ (å­¦ç¿’æ™‚ã¨åŒã˜ [vel_x, vel_y, dist_goal, dist_ball, team_id])\n",
        "    input_data = data.clone()\n",
        "    input_data.x = data.x[:, [2, 3, 4, 5, 6]]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³é‡ã¿ã‚’å–å¾—\n",
        "        out, (edge_index, alpha) = model(input_data, return_attention=True)\n",
        "\n",
        "    # Numpyã¸å¤‰æ›\n",
        "    alpha_np = alpha.cpu().numpy().flatten()\n",
        "    edge_index_np = edge_index.cpu().numpy()\n",
        "    max_a = alpha_np.max()\n",
        "\n",
        "    # --- 2. ã€åº§æ¨™å¾©å…ƒã®æ ¸å¿ƒã€‘ ---\n",
        "    # raw_pos ã¯ã‚»ãƒ³ã‚¿ãƒ¼ãƒ©ã‚¤ãƒ³0åŸºæº–ã®æ­£è¦åŒ–å€¤ (-0.5 ~ 0.5)\n",
        "    raw_pos = data.x[:, :2].cpu().numpy()\n",
        "\n",
        "    # æ¯”ç‡ã‚’ãƒ¡ãƒ¼ãƒˆãƒ«ã«ç›´ã—(105, 68å€)ã€ãƒ”ãƒƒãƒå·¦ä¸‹(0,0)ãŒåŸç‚¹ã«ãªã‚‹ã‚ˆã†ã‚·ãƒ•ãƒˆ(+52.5, +34)\n",
        "    pos_x = (raw_pos[:, 0] * 105) + 52.5\n",
        "    pos_y = (raw_pos[:, 1] * 68) + 34\n",
        "\n",
        "    # --- 3. æç”»é–‹å§‹ ---\n",
        "    plt.figure(figsize=(14, 10))\n",
        "\n",
        "    # ã‚µãƒƒã‚«ãƒ¼ãƒ”ãƒƒãƒã®å¤–æ ã¨ã‚»ãƒ³ã‚¿ãƒ¼ãƒ©ã‚¤ãƒ³\n",
        "    plt.plot([0, 105, 105, 0, 0], [0, 0, 68, 68, 0], color=\"black\", lw=2)\n",
        "    plt.axvline(52.5, color=\"black\", linestyle=\"--\", alpha=0.3)\n",
        "\n",
        "    # (A) é¸æ‰‹åŒå£«ã®é€£æºï¼ˆæ°´è‰²ï¼‰\n",
        "    # ãƒœãƒ¼ãƒ«(P22)ãŒé–¢ã‚ã‚‰ãªã„ã‚¨ãƒƒã‚¸ã‚’æŠ½å‡º\n",
        "    mask_players = (edge_index_np[0] != 22) & (edge_index_np[1] != 22)\n",
        "    alpha_players = alpha_np[mask_players]\n",
        "    edges_players = edge_index_np[:, mask_players]\n",
        "\n",
        "    if len(alpha_players) > 0:\n",
        "        max_p = alpha_players.max()\n",
        "        # é¸æ‰‹é–“ã®ç›¸å¯¾è©•ä¾¡ã§ä¸Šä½30%(0.7)ã«çµã‚‹\n",
        "        for i in range(len(alpha_players)):\n",
        "            if alpha_players[i] > max_p * 0.7:\n",
        "                src, dst = edges_players[0, i], edges_players[1, i]\n",
        "                plt.plot([pos_x[src], pos_x[dst]], [pos_y[src], pos_y[dst]],\n",
        "                         color='cyan', alpha=0.3, linewidth=1.5, zorder=1)\n",
        "\n",
        "    # (B) ãƒœãƒ¼ãƒ«ï¼ˆP22ï¼‰ã¨ã®é–¢ä¿‚ï¼ˆã‚ªãƒ¬ãƒ³ã‚¸ï¼‰\n",
        "    mask_ball = (edge_index_np[0] == 22) | (edge_index_np[1] == 22)\n",
        "    alpha_ball = alpha_np[mask_ball]\n",
        "    edges_ball = edge_index_np[:, mask_ball]\n",
        "\n",
        "    for i in range(len(alpha_ball)):\n",
        "        if alpha_ball[i] > max_a * threshold_ratio:\n",
        "            src, dst = edges_ball[0, i], edges_ball[1, i]\n",
        "            plt.plot([pos_x[src], pos_x[dst]], [pos_y[src], pos_y[dst]],\n",
        "                     color='orange', alpha=alpha_ball[i]/max_a,\n",
        "                     linewidth=(alpha_ball[i]/max_a) * 5, zorder=2)\n",
        "\n",
        "    # (C) é¸æ‰‹ã®ãƒ—ãƒ­ãƒƒãƒˆ\n",
        "    teams = data.x[:, 6].cpu().numpy()\n",
        "    for i in range(len(pos_x)):\n",
        "        # ãƒãƒ¼ãƒ ID 0:é’(æ”»æ’ƒ), 1:èµ¤(å®ˆå‚™), 2:é»„(ãƒœãƒ¼ãƒ«)\n",
        "        color = 'blue' if teams[i] == 0 else 'red' if teams[i] == 1 else 'yellow'\n",
        "        size = 400 if i == 22 else 250\n",
        "\n",
        "        plt.scatter(pos_x[i], pos_y[i], c=color, s=size, edgecolors='black', zorder=3)\n",
        "        plt.text(pos_x[i]+1.5, pos_y[i]+1.5, f\"P{i}\", fontsize=11, fontweight='bold')\n",
        "\n",
        "    # ã‚¿ã‚¤ãƒˆãƒ«ã¨è¡¨ç¤ºè¨­å®š\n",
        "    prob = torch.exp(out)[0, 1].item()\n",
        "    plt.title(f\"Final Tactical Map (Success Prob: {prob:.2%})\", fontsize=16)\n",
        "    plt.xlim(-5, 110)\n",
        "    plt.ylim(-5, 73)\n",
        "    plt.gca().set_aspect('equal', adjustable='box')\n",
        "    plt.grid(True, linestyle=':', alpha=0.4)\n",
        "    plt.show()\n",
        "\n",
        "# --- å®Ÿè¡Œ ---\n",
        "# threshold_ratioã‚’ä¸‹ã’ã‚‹ã¨ã‚ˆã‚Šå¤šãã®ã‚ªãƒ¬ãƒ³ã‚¸ã®ç·šãŒè¦‹ãˆã¾ã™\n",
        "plot_tactical_attention_fixed_v3(cv_model, sample_data, device, threshold_ratio=0.5)"
      ],
      "metadata": {
        "id": "uyo81_imReu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ¤œè¨¼"
      ],
      "metadata": {
        "id": "Fq9_erp86ynd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãƒ©ãƒ™ãƒ«ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ãŸæ™‚ã®ç²¾åº¦"
      ],
      "metadata": {
        "id": "SshWo4xj62Oi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç‰©ç†æå¤±ï¼ˆæå¤±é–¢æ•°ã«ä»˜ã‘åŠ ãˆãŸç‰©ç†é …ï¼‰"
      ],
      "metadata": {
        "id": "Hnh1Z9O767qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ç‰©ç†æå¤±ã®å¹³å‡å€¤ã‚’ç®—å‡º\n",
        "avg_phys = sum(history['physics_loss']) / len(history['physics_loss'])\n",
        "print(f\"å¹³å‡ç‰©ç†æå¤±: {avg_phys:.4f}\")\n",
        "\n",
        "if avg_phys < 25: # 18å‰å¾Œãªã‚‰éå¸¸ã«å„ªç§€\n",
        "    print(\">> ç‰©ç†çš„æ•´åˆæ€§ã¯ä¿ãŸã‚Œã¦ã„ã¾ã™ã€‚AIã¯ç¾å®Ÿçš„ãªå‹•ãã®ç¯„å›²å†…ã§äºˆæ¸¬ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\">> ç‰©ç†æå¤±ãŒé«˜ã„ã§ã™ã€‚AIãŒç•°å¸¸ãªé€Ÿåº¦ã‚’æƒ³å®šã—ã¦äºˆæ¸¬ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")"
      ],
      "metadata": {
        "id": "PP6lAzBVD6GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "éå­¦ç¿’ã‚’é˜²ããŸã‚ã«ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤ã‚’è¿½åŠ ã—ãŸä¿®æ­£ç‰ˆPIGNNãƒ¢ãƒ‡ãƒ«"
      ],
      "metadata": {
        "id": "8CW6TCnB7Bff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "class PIGNNClassifier_drop(nn.Module):\n",
        "    def __init__(self, hidden_channels=64, dropout_rate=0.3):\n",
        "        super(PIGNNClassifier_drop, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # ä¿®æ­£: 7æ¬¡å…ƒå…¥åŠ› [x, y, vx, vy, d_goal, d_ball, team_id]\n",
        "        self.conv1 = PIGNNLayer(7, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        # 1å±¤ç›® + Dropout\n",
        "        x = self.conv1(x, edge_index, pos, vel)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        # 2å±¤ç›® + Dropout\n",
        "        x = self.conv2(x, edge_index, pos, vel)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # æœ€çµ‚å‡ºåŠ›\n",
        "        return F.log_softmax(self.lin(x), dim=1)\n",
        "\n",
        "class PIGNNClassifier_v7_Final(nn.Module):\n",
        "    def __init__(self, hidden_channels=64, dropout_rate=0.3):\n",
        "        super(PIGNNClassifier_v7_Final, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # ä¿®æ­£: 7æ¬¡å…ƒ [x, y, vx, vy, d_goal, d_ball, team_id]\n",
        "        self.conv1 = PIGNNLayer(7, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        # 1å±¤ç›® + Dropout\n",
        "        x = F.elu(self.conv1(x, edge_index, pos, vel))\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        # 2å±¤ç›® + Dropout\n",
        "        x = F.elu(self.conv2(x, edge_index, pos, vel))\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return F.log_softmax(self.lin(x), dim=1)"
      ],
      "metadata": {
        "id": "BCUmvO4FYues"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–\n",
        "model_dropout = PIGNNClassifier_drop(hidden_channels=64, dropout_rate=0.3).to(device)\n",
        "optimizer_dropout = torch.optim.Adam(model_dropout.parameters(), lr=LR)\n",
        "\n",
        "# 2. å±¥æ­´ä¿å­˜ç”¨ï¼ˆåå‰ã‚’åˆ†ã‘ã‚‹ï¼šhistory_dropoutï¼‰\n",
        "history_dropout = {\n",
        "    'total_loss': [],\n",
        "    'physics_loss': [],\n",
        "    'test_acc': []\n",
        "}\n",
        "\n",
        "print(f\"PIGNNå­¦ç¿’é–‹å§‹ï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‰ˆï¼‰\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # model_dropout ã‚’ä½¿ã†\n",
        "    avg_loss, avg_phys = train_pignn_epoch(model_dropout, train_loader, optimizer_dropout, ALPHA_P, device)\n",
        "    acc = test_pignn(model_dropout, test_loader, device)\n",
        "\n",
        "    history_dropout['total_loss'].append(avg_loss)\n",
        "    history_dropout['physics_loss'].append(avg_phys)\n",
        "    history_dropout['test_acc'].append(acc)\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | Loss: {avg_loss:.4f} | Phys_L: {avg_phys:.4f} | Acc: {acc:.4f}\")\n",
        "\n",
        "    # ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å¤‰ãˆã‚‹ï¼ˆé‡è¦ï¼ï¼‰\n",
        "    if epoch == 1 or acc > max(history_dropout['test_acc'][:-1]):\n",
        "        torch.save(model_dropout.state_dict(), 'best_pignn_model_v6_dropout.pth')\n",
        "        print(f\" >> Model Saved (Best Dropout Acc: {acc:.4f})\")"
      ],
      "metadata": {
        "id": "cjks8bFpY2Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç²¾åº¦æ›²ç·š"
      ],
      "metadata": {
        "id": "2Xe4VMee7JJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤ãªã—ã¨ã‚ã‚Šã‚’é‡ã­ã¦æç”»\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history['test_acc'], label='Base Model (No Dropout)', alpha=0.6)\n",
        "plt.plot(history_dropout['test_acc'], label='Improved Model (With Dropout)', linewidth=2)\n",
        "plt.title('Effect of Dropout on Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MQvfkBq5ZbCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šä¹±é«˜ä¸‹ã®å¹…ãŒå°ã•ããªã£ã¦ã„ã‚‹â†’æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ"
      ],
      "metadata": {
        "id": "OT_gMd-h7QKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# ==========================================\n",
        "# 5. æœ€çµ‚è©•ä¾¡ã¨ãƒ¬ãƒãƒ¼ãƒˆ (Dropoutç‰ˆãƒ»å°‚ç”¨)\n",
        "# ==========================================\n",
        "\n",
        "# 1. ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "# å­¦ç¿’æ™‚ã«ä¿å­˜ã—ãŸã€Œdropoutã€ã¨ã„ã†åå‰ã®æ–¹ã‚’èª­ã¿è¾¼ã¿ã¾ã™\n",
        "model_drop_eval = PIGNNClassifier_drop(hidden_channels=64, dropout_rate=0.3).to(device)\n",
        "model_drop_eval.load_state_dict(torch.load('best_pignn_model_v6_dropout.pth'))\n",
        "model_drop_eval.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# 2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚äºˆæ¸¬\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        data = preprocess_batch(data, device)\n",
        "        out = model_drop_eval(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# 3. ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"       PIGNN æœ€çµ‚è©•ä¾¡çµæœ (Dropoutæ”¹è‰¯ç‰ˆ)\")\n",
        "print(\"=\"*50)\n",
        "print(classification_report(all_labels, all_preds, target_names=['Failure (0)', 'Success (1)']))\n",
        "\n",
        "# 4. æ··åŒè¡Œåˆ—ã®æç”»\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', # åŒºåˆ¥ã™ã‚‹ãŸã‚ã«è‰²ã‚’ã‚ªãƒ¬ãƒ³ã‚¸ç³»ã«\n",
        "            xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title('Confusion Matrix (PIGNN with Dropout)')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EvaPTIG8b-m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ¤œè¨¼"
      ],
      "metadata": {
        "id": "7biH1yrO7eMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_label_test_dropout_ver(model_to_test, loader, device):\n",
        "    model_to_test.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # åˆ¤å®šç”¨ã®é–¾å€¤ï¼ˆä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€å¤šæ•°æ´¾ã®å‰²åˆã«å¼•ã£å¼µã‚‰ã‚Œã‚‹ãŸã‚ï¼‰\n",
        "    # ä»Šå›ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ Failure:239, Success:90 ãªã®ã§ã€ãƒ©ãƒ³ãƒ€ãƒ ãªã‚‰\n",
        "    # (239/329)^2 + (90/329)^2 â‰’ 0.6 ãã‚‰ã„ã«ãªã‚‹ã®ãŒçµ±è¨ˆå­¦çš„ãªã€Œå‹˜ã€ã®é™ç•Œã§ã™ã€‚\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = preprocess_batch(data, device)\n",
        "\n",
        "            # ãƒ©ãƒ™ãƒ«ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
        "            random_y = data.y[torch.randperm(data.y.size(0))]\n",
        "\n",
        "            out = model_to_test(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            correct += (pred == random_y.view(-1)).sum().item()\n",
        "            total += data.num_graphs\n",
        "\n",
        "    shuffle_acc = correct / total\n",
        "    print(f\"ãƒ©ãƒ™ãƒ«ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ™‚ã®ç²¾åº¦: {shuffle_acc:.4f}\")\n",
        "\n",
        "    if shuffle_acc < 0.58: # 0.62ã‹ã‚‰ä¸‹ãŒã£ã¦ã„ã‚Œã°æ”¹å–„\n",
        "        print(\">> åˆæ ¼ï¼šæš—è¨˜ï¼ˆéå­¦ç¿’ï¼‰ãŒæŠ‘åˆ¶ã•ã‚Œã€ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã®çœŸã®ç›¸é–¢ã‚’å­¦ã‚“ã§ã„ã¾ã™ã€‚\")\n",
        "    else:\n",
        "        print(\">> è­¦å‘Šï¼šä¾ç„¶ã¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã®åã‚Šï¼ˆåˆæœŸé…ç½®ãªã©ï¼‰ã‚’å¼·ãè¦šãˆã™ãã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "\n",
        "# å®Ÿè¡Œï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šï¼‰\n",
        "shuffle_label_test_dropout_ver(model_drop_eval, test_loader, device)"
      ],
      "metadata": {
        "id": "hm1YcFwvc21q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç‰©ç†æå¤±"
      ],
      "metadata": {
        "id": "DW3LormL7h9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ç‰©ç†çš„æ•´åˆæ€§ã®æœ€çµ‚ç¢ºèª (Dropoutç‰ˆ)\n",
        "# ==========================================\n",
        "\n",
        "# history_dropout ã®ä¸­èº«ã‚’ä½¿ã£ã¦è¨ˆç®—ã—ã¾ã™\n",
        "if 'physics_loss' in history_dropout and len(history_dropout['physics_loss']) > 0:\n",
        "    avg_phys = sum(history_dropout['physics_loss']) / len(history_dropout['physics_loss'])\n",
        "    print(f\"ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‰ˆ å¹³å‡ç‰©ç†æå¤±: {avg_phys:.4f}\")\n",
        "\n",
        "    # åˆ¤å®š\n",
        "    if avg_phys < 25:\n",
        "        print(\">> åˆæ ¼ï¼šç‰©ç†çš„æ•´åˆæ€§ã¯ä¿ãŸã‚Œã¦ã„ã¾ã™ã€‚\")\n",
        "        print(\">> ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’å°å…¥ã—ã¦ã‚‚ã€AIã¯ç¾å®Ÿçš„ãªç‰©ç†æ³•å‰‡ï¼ˆé€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã‚’ç„¡è¦–ã—ã¦ã„ã¾ã›ã‚“ã€‚\")\n",
        "    else:\n",
        "        print(\">> è­¦å‘Šï¼šç‰©ç†æå¤±ãŒå¢—å¤§ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "        print(\">> æ±åŒ–æ€§èƒ½ã‚’å„ªå…ˆã™ã‚‹ã‚ã¾ã‚Šã€ç‰©ç†ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®åˆ¶ç´„ãŒå¼±ã¾ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\">> ã‚¨ãƒ©ãƒ¼ï¼šhistory_dropout ã« physics_loss ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\")"
      ],
      "metadata": {
        "id": "LGzGGSqSdA8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.explain import Explainer, GNNExplainer\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# --- ä¿®æ­£ã®æ ¸å¿ƒï¼šé‡è¦åº¦ã‚’æŠ½å‡ºã™ã‚‹1è¡Œ ---\n",
        "# node_mask ã¯ (ãƒãƒ¼ãƒ‰æ•°, ç‰¹å¾´é‡æ•°) ã®å½¢çŠ¶ãªã®ã§ã€å…¨ãƒãƒ¼ãƒ‰ã§å¹³å‡ã—ã¦ç‰¹å¾´é‡ã”ã¨ã®é‡è¦åº¦ã«ã™ã‚‹\n",
        "importances = explanation.node_mask.mean(dim=0)\n",
        "\n",
        "# --- ä»¥é™ã€ã‚ãªãŸã®å¯è¦–åŒ–ã‚³ãƒ¼ãƒ‰ã¸ç¶šã ---\n",
        "if torch.is_tensor(importances):\n",
        "    importances = importances.cpu().numpy()\n",
        "\n",
        "# 1. ãƒãƒ©ãƒãƒ©ã®å¼•æ•°ã‚’ã€Œdataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€ã«æ¢±åŒ…ã—ã¦ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™ãƒ©ãƒƒãƒ‘ãƒ¼\n",
        "class ExplainerWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None, **kwargs):\n",
        "        # GNNExplainerã‹ã‚‰å±Šãå„ãƒ†ãƒ³ã‚½ãƒ«ã‚’ã€ãƒ¢ãƒ‡ãƒ«ãŒæœŸå¾…ã™ã‚‹ Data ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«æ“¬ä¼¼å†ç¾\n",
        "        # pos ã¨ vel ã¯ x ã®ä¸­ã«å…¥ã£ã¦ã„ã‚‹ã€ã‚ã‚‹ã„ã¯åˆ¥é€”æ¸¡ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®š\n",
        "        # ã‚ãªãŸã®ãƒ¢ãƒ‡ãƒ«å®šç¾©ã«åˆã‚ã›ã¦ x ã‹ã‚‰ pos, vel ã‚’åˆ‡ã‚Šå‡ºã™ã€\n",
        "        # ã¾ãŸã¯ data.pos, data.vel ã¨ã—ã¦ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚\n",
        "\n",
        "        # ä»®ã« x ã® 0-1åˆ—ç›®ãŒ pos, 2-3åˆ—ç›®ãŒ vel ã ã¨æƒ³å®šã•ã‚Œã‚‹å ´åˆï¼š\n",
        "        # (ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„ã€‚xè‡ªä½“ã«å…¨ã¦å«ã¾ã‚Œã¦ã„ã‚‹ãªã‚‰ãã®ã¾ã¾ã§ã‚‚å¯)\n",
        "        tmp_data = Data(x=x, edge_index=edge_index, batch=batch)\n",
        "\n",
        "        # ã‚‚ã—ãƒ¢ãƒ‡ãƒ«ãŒ data.pos ã‚„ data.vel ã‚’ç›´æ¥å‚ç…§ã—ã¦ã„ã‚‹ãªã‚‰ã€ã“ã“ã§ä»£å…¥\n",
        "        # x ã®æ§‹æˆãŒ [ç‰¹å¾´é‡...] ã§ã€pos/velãŒåˆ¥ç®¡ç†ãªã‚‰ä»¥ä¸‹ã®ã‚ˆã†ã«å¾©å…ƒ\n",
        "        tmp_data.pos = x[:, :2]  # ä¾‹: æœ€åˆã®2åˆ—ãŒåº§æ¨™\n",
        "        tmp_data.vel = x[:, 2:4] # ä¾‹: æ¬¡ã®2åˆ—ãŒé€Ÿåº¦\n",
        "\n",
        "        return self.model(tmp_data)\n",
        "\n",
        "# ãƒ©ãƒƒãƒ—ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ\n",
        "wrapped_model = ExplainerWrapper(model_drop_eval)\n",
        "\n",
        "# 2. Explainerã®è¨­å®š\n",
        "model_config = {\n",
        "    'mode': 'multiclass_classification',\n",
        "    'task_level': 'graph',\n",
        "    'return_type': 'log_probs',\n",
        "}\n",
        "\n",
        "explainer = Explainer(\n",
        "    model=wrapped_model,\n",
        "    algorithm=GNNExplainer(epochs=200),\n",
        "    explanation_type='model',\n",
        "    node_mask_type='attributes',\n",
        "    edge_mask_type='object',\n",
        "    model_config=model_config,\n",
        ")\n",
        "\n",
        "# --- ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ ---\n",
        "test_batch = next(iter(test_loader))\n",
        "test_batch = preprocess_batch(test_batch, device)\n",
        "data_list = test_batch.to_data_list()\n",
        "data_single = data_list[0]\n",
        "\n",
        "# 3. é‡è¦åº¦ã®ç®—å‡º\n",
        "explanation = explainer(\n",
        "    x=data_single.x,\n",
        "    edge_index=data_single.edge_index,\n",
        "    batch=torch.zeros(data_single.x.size(0), dtype=torch.long).to(device)\n",
        ")\n",
        "\n",
        "print(\"é‡è¦åº¦ã®ç®—å‡ºã«æˆåŠŸã—ã¾ã—ãŸï¼\")\n",
        "\n",
        "# --- ä¿®æ­£ç‰ˆï¼šé‡è¦åº¦ã®å¯è¦–åŒ–ã‚³ãƒ¼ãƒ‰ ---\n",
        "\n",
        "# ãƒ©ãƒ™ãƒ«ã‚’7æ¬¡å…ƒç”¨ã«æ›´æ–°\n",
        "labels = ['x', 'y', 'vx', 'vy', 'dist_goal', 'dist_ball', 'team_id']\n",
        "\n",
        "# importances ãŒ numpy å½¢å¼ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\n",
        "if torch.is_tensor(importances):\n",
        "    importances = importances.cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "# ã“ã“ã§æ¬¡å…ƒæ•°ã‚’è‡ªå‹•ã§åˆã‚ã›ã¾ã™\n",
        "plt.bar(labels, importances, color='teal')\n",
        "\n",
        "plt.title('GNNExplainer: Feature Importance (PIGNN v7)')\n",
        "plt.ylabel('Importance Score')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# å€¤ã‚’æ£’ã®ä¸Šã«è¡¨ç¤º\n",
        "for i, v in enumerate(importances):\n",
        "    plt.text(i, v + 0.01, f\"{v:.3f}\", ha='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9i9AUGXx97sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. è§£æã®è¨­å®š\n",
        "num_samples = 100\n",
        "all_node_importances = []\n",
        "\n",
        "print(f\"{num_samples}ã‚·ãƒ¼ãƒ³ã®è§£æã‚’é–‹å§‹ã—ã¾ã™ã€‚å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹ã«ã—ã¦æœ€é©åŒ–ã‚’è¡Œã†ãŸã‚ã€å°‘ã—æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™...\")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«ã—ã¤ã¤ã€GNNExplainerå†…éƒ¨ã®å­¦ç¿’ã¯è¨±å¯ã™ã‚‹\n",
        "model_drop_eval.eval()\n",
        "\n",
        "# é€²æ—ç®¡ç†ç”¨ã®ã‚«ã‚¦ãƒ³ã‚¿\n",
        "count = 0\n",
        "\n",
        "for data in tqdm(test_loader):\n",
        "    if count >= num_samples:\n",
        "        break\n",
        "\n",
        "    data = preprocess_batch(data, device)\n",
        "    data_list = data.to_data_list()\n",
        "\n",
        "    for data_single in data_list:\n",
        "        if count >= num_samples:\n",
        "            break\n",
        "\n",
        "        # --- ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆï¼šwith torch.no_grad() ã‚’å‰Šé™¤ ---\n",
        "        # GNNExplainerã¯å†…éƒ¨ã§ãƒ­ã‚¹ã‚’è¨ˆç®—ã— .backward() ã‚’å‘¼ã¶ãŸã‚å‹¾é…ãŒå¿…è¦\n",
        "        explanation = explainer(\n",
        "            x=data_single.x,\n",
        "            edge_index=data_single.edge_index,\n",
        "            batch=torch.zeros(data_single.x.size(0), dtype=torch.long).to(device)\n",
        "        )\n",
        "\n",
        "        node_importance = explanation.node_mask.abs().mean(dim=0).cpu().numpy()\n",
        "        all_node_importances.append(node_importance)\n",
        "        count += 1\n",
        "\n",
        "# 3. å¹³å‡ã¨æ¨™æº–èª¤å·®ã®è¨ˆç®—\n",
        "avg_importance = np.mean(all_node_importances, axis=0)\n",
        "std_importance = np.std(all_node_importances, axis=0) / np.sqrt(len(all_node_importances))\n",
        "\n",
        "# --- ä¿®æ­£ç‰ˆï¼šãƒ©ãƒ™ãƒ«ã¨ã‚°ãƒ©ãƒ•æç”» ---\n",
        "\n",
        "# 1. ãƒ©ãƒ™ãƒ«ã‚’ç¾åœ¨ã®7æ¬¡å…ƒä»•æ§˜ã«å®Œå…¨ã«åˆã‚ã›ã‚‹\n",
        "# [x, y, vx, vy, dist_goal, dist_ball, team_id]\n",
        "labels = ['PosX', 'PosY', 'VelX', 'VelY', 'DistGoal', 'DistBall', 'TeamID']\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# 2. ãƒ‡ãƒ¼ã‚¿ã®æ•°ã¨ãƒ©ãƒ™ãƒ«ã®æ•°ãŒä¸€è‡´ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦æç”»\n",
        "# avg_importance ã¨ std_importance ãŒ 7è¦ç´ ã§ã‚ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¾ã™\n",
        "bars = plt.bar(labels, avg_importance, yerr=std_importance,\n",
        "               color='teal', edgecolor='navy', capsize=5, alpha=0.8)\n",
        "\n",
        "plt.title('GNNExplainer: Mean Feature Importance over 100 Scenes (v7)', fontsize=14)\n",
        "plt.ylabel('Importance Score', fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# æ£’ã®ä¸Šã«æ•°å€¤ã‚’è¡¨ç¤º\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fztqYk2oBvLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# æˆåŠŸã¨å¤±æ•—ã®æ¯”è¼ƒè§£æ (PIGNN v7å¯¾å¿œç‰ˆ)\n",
        "# ==========================================\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. ãƒ‡ãƒ¼ã‚¿ã®ä»•åˆ†ã‘ç”¨ãƒªã‚¹ãƒˆ\n",
        "success_importances = []\n",
        "failure_importances = []\n",
        "\n",
        "num_samples = 150  # çµ±è¨ˆçš„å®‰å®šæ€§ã®ãŸã‚ã«150ã‚·ãƒ¼ãƒ³ã‚’æ¨å¥¨\n",
        "count = 0\n",
        "\n",
        "print(f\"{num_samples}ã‚·ãƒ¼ãƒ³ã‚’æˆåŠŸ/å¤±æ•—åˆ¥ã«è§£æã—ã¾ã™...\")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«\n",
        "model_drop_eval.eval()\n",
        "\n",
        "# tqdmã§é€²æ—ã‚’è¡¨ç¤ºã—ãªãŒã‚‰ãƒ«ãƒ¼ãƒ—\n",
        "for data in tqdm(test_loader):\n",
        "    if count >= num_samples:\n",
        "        break\n",
        "\n",
        "    # ãƒãƒƒãƒã‚’ãƒ‡ãƒã‚¤ã‚¹ã«é€ã‚Šã€å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã«åˆ†è§£\n",
        "    data = preprocess_batch(data, device)\n",
        "    data_list = data.to_data_list()\n",
        "\n",
        "    for data_single in data_list:\n",
        "        if count >= num_samples:\n",
        "            break\n",
        "\n",
        "        # GNNExplainerã§é‡è¦åº¦ç®—å‡º\n",
        "        explanation = explainer(\n",
        "            x=data_single.x,\n",
        "            edge_index=data_single.edge_index,\n",
        "            # batchãƒ†ãƒ³ã‚½ãƒ«ã‚‚ãƒ‡ãƒã‚¤ã‚¹ã«åˆã‚ã›ã‚‹\n",
        "            batch=torch.zeros(data_single.x.size(0), dtype=torch.long).to(device)\n",
        "        )\n",
        "\n",
        "        # ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆçµ¶å¯¾å€¤ã®å¹³å‡ï¼‰ã‚’å–å¾—\n",
        "        node_importance = explanation.node_mask.abs().mean(dim=0).cpu().numpy()\n",
        "\n",
        "        # æ­£è§£ãƒ©ãƒ™ãƒ«(y)ã«åŸºã¥ã„ã¦ä»•åˆ†ã‘\n",
        "        if data_single.y.item() == 1:\n",
        "            success_importances.append(node_importance)\n",
        "        else:\n",
        "            failure_importances.append(node_importance)\n",
        "\n",
        "        count += 1\n",
        "\n",
        "# 2. ã€é‡è¦ã€‘ãƒ©ãƒ™ãƒ«ã‚’7æ¬¡å…ƒï¼ˆv7ä»•æ§˜ï¼‰ã«æ›´æ–°\n",
        "labels = ['PosX', 'PosY', 'VelX', 'VelY', 'DistGoal', 'DistBall', 'TeamID']\n",
        "\n",
        "# å„ã‚°ãƒ«ãƒ¼ãƒ—ã®å¹³å‡ã‚’ç®—å‡º\n",
        "# ã“ã“ã§ avg_success ã® shape ã¯ (7,) ã«ãªã‚Šã¾ã™\n",
        "avg_success = np.mean(success_importances, axis=0)\n",
        "avg_failure = np.mean(failure_importances, axis=0)\n",
        "\n",
        "# 3. æ¯”è¼ƒã‚°ãƒ©ãƒ•ã®æç”»\n",
        "x = np.arange(len(labels)) # 0ã‹ã‚‰6ã¾ã§ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# æ£’ã‚°ãƒ©ãƒ•ã®æç”»\n",
        "rects1 = ax.bar(x - width/2, avg_success, width, label='Success (1)', color='forestgreen', alpha=0.8)\n",
        "rects2 = ax.bar(x + width/2, avg_failure, width, label='Failure (0)', color='crimson', alpha=0.8)\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ã®è£…é£¾\n",
        "ax.set_ylabel('Mean Importance Score', fontsize=12)\n",
        "ax.set_title('Feature Importance Comparison: Success vs Failure (PIGNN v7)', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels, fontsize=11)\n",
        "ax.legend(fontsize=12)\n",
        "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
        "\n",
        "# æ•°å€¤ãƒ©ãƒ™ãƒ«ã‚’è¡¨ç¤ºã™ã‚‹è£œåŠ©é–¢æ•°\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.3f}',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3), # 3ptä¸Šã«è¡¨ç¤º\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. æ•°å€¤ã®è¦ç´„è¡¨ç¤º\n",
        "print(\"\\n--- è§£æçµæœã®è¦ç´„ ---\")\n",
        "for i, label in enumerate(labels):\n",
        "    diff = avg_success[i] - avg_failure[i]\n",
        "    trend = \"â†‘ Successã§é‡è¦–\" if diff > 0 else \"â†“ Failureã§é‡è¦–\"\n",
        "    print(f\"[{label}] Success: {avg_success[i]:.4f} | Failure: {avg_failure[i]:.4f} | {trend}\")"
      ],
      "metadata": {
        "id": "Jqps80gtE7Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãƒã‚¤ã‚¢ã‚¹ã®å¯è¦–åŒ–"
      ],
      "metadata": {
        "id": "sYGevTJQeSGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# æç”»ã®å‰ã«ã“ã‚Œã‚’å…¥ã‚Œã¦ãã ã•ã„\n",
        "sample = next(iter(test_loader))\n",
        "# æœ€åˆã®1ãƒãƒƒãƒåˆ†ã®ãƒãƒ¼ãƒ IDã®ä¸­èº«ã‚’ã™ã¹ã¦è¡¨ç¤º\n",
        "print(\"--- TeamID Raw Data Check ---\")\n",
        "print(sample.x[:, 6])\n",
        "print(\"-----------------------------\")\n",
        "\n",
        "# ã‚‚ã—ã“ã“ã§ 0.0 ã—ã‹å‡ºã¦ã“ãªã„ãªã‚‰ã€ãƒ‡ãƒ¼ã‚¿ã®ä½œã‚Šç›´ã—ãŒå¿…è¦ã§ã™ã€‚\n",
        "# ã‚‚ã— 1.0 ã‚„ 2.0 ãŒæ··ã–ã£ã¦ã„ã‚‹ãªã‚‰ã€ä»¥ä¸‹ã®ã€Œçµ¶å¯¾è‰²åˆ†ã‘ã‚³ãƒ¼ãƒ‰ã€ã§ç›´ã‚Šã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "acgKVh70hQLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_physics_bias(model, data, tau=1.5):\n",
        "    \"\"\"\n",
        "    ãƒ¢ãƒ‡ãƒ«å†…ã®ç‰©ç†ãƒã‚¤ã‚¢ã‚¹ã‚’è§£æã™ã‚‹é–¢æ•°ï¼ˆåå‰ã‚’ä¿®æ­£ï¼‰\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pos = data.pos\n",
        "        vel = data.vel\n",
        "        edge_index = data.edge_index\n",
        "\n",
        "        # æœªæ¥ä½ç½®ã®äºˆæ¸¬\n",
        "        pos_pred = pos + vel * tau\n",
        "\n",
        "        # ã‚¨ãƒƒã‚¸ã”ã¨ã®æœªæ¥è·é›¢ã¨ç‰©ç†ãƒã‚¤ã‚¢ã‚¹\n",
        "        row, col = edge_index\n",
        "        dist_future = torch.norm(pos_pred[row] - pos_pred[col], dim=-1)\n",
        "        physics_bias = torch.exp(-dist_future / 5.0)\n",
        "\n",
        "        # æœ€å¤§ãƒã‚¤ã‚¢ã‚¹ã®å–å¾—\n",
        "        top_idx = torch.argmax(physics_bias)\n",
        "        top_pair = (edge_index[0, top_idx].item(), edge_index[1, top_idx].item())\n",
        "\n",
        "    # æˆ»ã‚Šå€¤ã®ã‚­ãƒ¼ã‚’ 'max_bias' ã«ä¿®æ­£ã—ã¦ã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆ\n",
        "    return {\n",
        "        \"top_pair\": top_pair,\n",
        "        \"max_bias\": physics_bias[top_idx].item(), # ã“ã“ã‚’ bias_value ã‹ã‚‰ max_bias ã¸ä¿®æ­£\n",
        "        \"pos_pred\": pos_pred\n",
        "    }"
      ],
      "metadata": {
        "id": "b6RnU4u7DYFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_pignn_v7_absolute_colors(model, loader, sample_idx=0, tau=1.5):\n",
        "    model.eval()\n",
        "    # ãƒ‡ãƒ¼ã‚¿ã‚’1ã¤å–å¾—\n",
        "    batch = next(iter(loader)).to(device)\n",
        "    input_data = preprocess_batch(batch.clone(), device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(input_data)\n",
        "        probs = torch.exp(out)\n",
        "        preds = out.argmax(dim=1)\n",
        "\n",
        "    # æç”»å¯¾è±¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŠ½å‡º\n",
        "    mask = (batch.batch == sample_idx)\n",
        "    pos = batch.pos[mask].cpu().numpy()\n",
        "    vel = batch.vel[mask].cpu().numpy()\n",
        "\n",
        "    # ã€æœ€é‡è¦ã€‘team_id (index 6) ã‚’ç›´æ¥å–å¾—ã—ã¦ä¸­èº«ã‚’ç¢ºèª\n",
        "    team_ids = input_data.x[mask, 6].cpu().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    # ãƒ”ãƒƒãƒèƒŒæ™¯\n",
        "    ax.set_facecolor('#f0f0f0')\n",
        "    ax.add_patch(plt.Rectangle((-52.5, -34), 105, 68, fill=False, color='black', lw=2))\n",
        "    ax.plot([0, 0], [-34, 34], color='black', alpha=0.3)\n",
        "\n",
        "    for i in range(len(pos)):\n",
        "        t_val = team_ids[i]\n",
        "\n",
        "        # --- åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã‚’ã€Œç¯„å›²ã€ã«ã—ã¦èª¤å·®ã‚’è¨±å®¹ ---\n",
        "        if t_val > 1.5:          # Ball (2.0)\n",
        "            color, marker, size, z = '#FFD700', '*', 500, 15 # Gold\n",
        "            lbl = \"Ball\"\n",
        "        elif t_val > 0.5:        # Defender (1.0)\n",
        "            color, marker, size, z = '#EE3333', 'o', 250, 10 # Red\n",
        "            lbl = \"Defender (Away)\"\n",
        "        else:                    # Attacker (0.0)\n",
        "            color, marker, size, z = '#3366FF', 'o', 250, 10 # Blue\n",
        "            lbl = \"Attacker (Home)\"\n",
        "\n",
        "        # æç”»\n",
        "        ax.scatter(pos[i, 0], pos[i, 1], c=color, marker=marker, s=size,\n",
        "                   edgecolors='black', linewidths=1.2, zorder=z, label=lbl)\n",
        "\n",
        "        # é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«\n",
        "        ax.arrow(pos[i, 0], pos[i, 1], vel[i, 0]*tau, vel[i, 1]*tau,\n",
        "                 head_width=0.8, head_length=1.0, fc=color, ec=color,\n",
        "                 alpha=0.3, zorder=z-1)\n",
        "\n",
        "    # ç‰©ç†ãƒã‚¤ã‚¢ã‚¹ã®æç”»ï¼ˆç·‘ã®Xå°ã¨ç‚¹ç·šï¼‰\n",
        "    res = analyze_physics_bias(model, batch.to_data_list()[sample_idx], tau=tau)\n",
        "    p1, p2 = res[\"top_pair\"] # ã‚¿ãƒ—ãƒ«ãªã®ã§ãã®ã¾ã¾å—ã‘å–ã‚‹ã ã‘ã§OK\n",
        "    p1, p2 = int(p1), int(p2) # å¿µã®ãŸã‚æ•´æ•°å‹ã«å¤‰æ›\n",
        "    ax.plot([pos[p1,0], pos[p2,0]], [pos[p1,1], pos[p2,1]], 'green', linestyle='--', lw=2, alpha=0.6)\n",
        "    ax.scatter(res[\"pos_pred\"][p1,0].cpu(), res[\"pos_pred\"][p1,1].cpu(),\n",
        "               color='green', marker='X', s=350, edgecolors='white', label='Conflict Point', zorder=20)\n",
        "\n",
        "    # ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±\n",
        "    res_str = \"SUCCESS\" if preds[sample_idx] == 1 else \"FAILURE\"\n",
        "    plt.title(f\"PIGNN v7 Tactical Analysis: {res_str}\\n\"\n",
        "              f\"AI Prediction Prob: {probs[sample_idx, 1]:.2%} | Max Physics Bias: {res['max_bias']:.3f}\",\n",
        "              fontsize=14, fontweight='bold')\n",
        "\n",
        "    # å‡¡ä¾‹ã®é‡è¤‡ã‚’å‰Šé™¤\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    by_label = dict(zip(labels, handles))\n",
        "    ax.legend(by_label.values(), by_label.keys(), loc='upper right', frameon=True, shadow=True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "visualize_pignn_v7_absolute_colors(model, test_loader, sample_idx=0)"
      ],
      "metadata": {
        "id": "qCRYWGTXgvJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# ==========================================\n",
        "# 1. ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆå…¨ä½“ã‹ã‚‰æœ€å°ãƒã‚¤ã‚¢ã‚¹ã‚’æ¢ç´¢\n",
        "# ==========================================\n",
        "model.eval()\n",
        "min_bias = float('inf')\n",
        "low_bias_data = None\n",
        "low_bias_idx = -1\n",
        "\n",
        "print(\"ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‹ã‚‰æœ€ã‚‚ç‰©ç†çš„ã«å®‰å®šã—ãŸã‚·ãƒ¼ãƒ³ã‚’æ¢ç´¢ä¸­...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    # test_set ã¯ Data ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™\n",
        "    for i, data in enumerate(test_set):\n",
        "        # ãƒ‡ãƒ¼ã‚¿ã®ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•\n",
        "        data_to_device = data.to(device)\n",
        "\n",
        "        # è§£æé–¢æ•°ã‚’å‘¼ã³å‡ºã—ï¼ˆtau=1.5ç§’å¾Œã®æœªæ¥ã‚’äºˆæ¸¬ï¼‰\n",
        "        res = analyze_physics_bias(model, data_to_device, tau=1.5)\n",
        "\n",
        "        current_max_bias = res['max_bias']\n",
        "\n",
        "        # æœ€å°å€¤ã‚’æ›´æ–°\n",
        "        if current_max_bias < min_bias:\n",
        "            min_bias = current_max_bias\n",
        "            low_bias_idx = i\n",
        "            low_bias_data = data_to_device\n",
        "\n",
        "print(f\"æ¢ç´¢å®Œäº†\")\n",
        "print(f\"ç™ºè¦‹ã•ã‚ŒãŸæœ€å°ãƒã‚¤ã‚¢ã‚¹: {min_bias:.4f}\")\n",
        "print(f\"è©²å½“ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {low_bias_idx}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. ä¿®æ­£ç‰ˆï¼šIndexErrorå›é¿ç”¨å¯è¦–åŒ–å‘¼ã³å‡ºã—\n",
        "# ==========================================\n",
        "\n",
        "def visualize_specific_scene(model, data_list, target_idx, tau=1.5):\n",
        "    \"\"\"\n",
        "    ç‰¹å®šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã ã‘ã‚’æŠ½å‡ºã—ã€\n",
        "    ãƒãƒƒãƒã¨ã—ã¦å¯è¦–åŒ–é–¢æ•°ã«æ¸¡ã™ã“ã¨ã§ IndexError ã‚’é˜²ã\n",
        "    \"\"\"\n",
        "    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒ‡ãƒ¼ã‚¿1æšã ã‘ã‚’å«ã‚€ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
        "    single_data_list = [data_list[target_idx]]\n",
        "\n",
        "    # ãƒãƒƒãƒã‚µã‚¤ã‚º1ã®å°‚ç”¨ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ\n",
        "    single_loader = DataLoader(single_data_list, batch_size=1)\n",
        "\n",
        "    # æ—¢å­˜ã®å¯è¦–åŒ–é–¢æ•°ã‚’å‘¼ã³å‡ºã—\n",
        "    # ãƒãƒƒãƒå†…ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯å¿…ãš 0 ã«ãªã‚‹\n",
        "    visualize_pignn_v7_absolute_colors(model, single_loader, sample_idx=0, tau=tau)\n",
        "\n",
        "# å®Ÿè¡Œï¼šç‰©ç†çš„ã«æœ€ã‚‚ã€Œç¶ºéº—ã€ãªã‚·ãƒ¼ãƒ³ã‚’æç”»\n",
        "visualize_specific_scene(model, test_set, low_bias_idx, tau=1.5)"
      ],
      "metadata": {
        "id": "Ur3rTejiX-3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_most_intense_duel(model, data_list, tau=1.5):\n",
        "    model.eval()\n",
        "    max_duel_bias = -1.0\n",
        "    best_idx = -1\n",
        "\n",
        "    print(\"æ”»å®ˆãŒæœ€ã‚‚æ¿€ã—ãã€ã¶ã¤ã‹ã‚‹ã€ã‚·ãƒ¼ãƒ³ã‚’æ¢ç´¢ä¸­...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(data_list):\n",
        "            data_to_device = data.to(device)\n",
        "            # å…¨ãƒšã‚¢ã®ãƒã‚¤ã‚¢ã‚¹è©³ç´°ã‚’å–å¾—\n",
        "            res = analyze_physics_bias(model, data_to_device, tau=tau)\n",
        "\n",
        "            p1, p2 = res[\"top_pair\"]\n",
        "            # ãƒãƒ¼ãƒ IDã‚’å–å¾— (index 6)\n",
        "            t1 = data.x[p1, 6].item()\n",
        "            t2 = data.x[p2, 6].item()\n",
        "\n",
        "            # ç•°ãªã‚‹ãƒãƒ¼ãƒ åŒå£«ï¼ˆ0.0:Home vs 1.0:Awayï¼‰ã®è¡çªã®ã¿ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã™ã‚‹\n",
        "            # 1.0 - 0.0 = 1.0 ã®çµ¶å¯¾å€¤ã§åˆ¤å®š\n",
        "            if abs(t1 - t2) == 1.0:\n",
        "                if res['max_bias'] > max_duel_bias:\n",
        "                    max_duel_bias = res['max_bias']\n",
        "                    best_idx = i\n",
        "\n",
        "    print(f\"ç™ºè¦‹ï¼ æœ€å¤§æ”»å®ˆè¡çªãƒã‚¤ã‚¢ã‚¹: {max_duel_bias:.4f} (Index: {best_idx})\")\n",
        "    return best_idx\n",
        "\n",
        "# 1. æ¿€ã—ã„ç«¶ã‚Šåˆã„ã‚·ãƒ¼ãƒ³ã‚’ç‰¹å®š\n",
        "duel_idx = find_most_intense_duel(model, test_set)\n",
        "\n",
        "# 2. å¯è¦–åŒ–\n",
        "if duel_idx != -1:\n",
        "    visualize_specific_scene(model, test_set, duel_idx, tau=1.5)\n",
        "else:\n",
        "    print(\"æ¡ä»¶ã«åˆã†ã‚·ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "zJvzmHVsY30G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# çµ±åˆå¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_v14_final.pt\", weights_only=False)\n",
        "train_set = checkpoint['train_data']\n",
        "\n",
        "# å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã®å¹³å‡é€Ÿåº¦(vx)ã‚’ãƒªã‚¹ãƒˆåŒ–\n",
        "all_v_means = [torch.abs(d.x[:, 2]).mean().item() for d in train_set]\n",
        "\n",
        "print(f\"å…¨{len(train_set)}ãƒ•ãƒ¬ãƒ¼ãƒ ä¸­ã€é€Ÿåº¦ãŒ0ã®ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {all_v_means.count(0.0)}\")\n",
        "print(f\"æœ€åˆã®5æšã®é€Ÿåº¦å¹³å‡: {all_v_means[:5]}\")"
      ],
      "metadata": {
        "id": "P5_9nS3GytTp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}