{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtN23vJHihKs5wez+gXSiK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryu622/gnn-counterattack-xai-v2/blob/feat%2Fnew-train/GAT_CounterAttack_Prediction_Train_Scientific8_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAT_CounterAttack_Prediction_Train_Scientific8_V2.ipynbã«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã®å¯è¦–åŒ–ã‚’è¿½åŠ "
      ],
      "metadata": {
        "id": "8WHsPBpFnp-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7ã§ã¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å‹â†’ç‰©ç†æå¤±ã‚’åŠ ãˆãŸï¼ˆLosså‹ï¼‰ã«ã€‚å°†æ¥ä½ç½®ã®äºˆæ¸¬ã«ã‚ˆã‚‹ç‰©ç†çš„ã«ã‚ã‚Šãˆãªã„å‹•ãã«å„ªå…ˆçš„ã«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’æ‰•ã†ã‚ˆã†ã«ä»•å‘ã‘ã‚‹å½¢ï¼ˆãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’ä¸ãˆã‚‹å½¢ã§ã¯ãªã„ï¼‰ã€‚æå¤±ã¯ãƒãƒ¼ãƒ ã®æ¨é€²åŠ›ã«ã‚ˆã‚‹æå¤±ã€‚\n",
        "ï¼ˆå°†æ¥ä½ç½®ã«åŸºã¥ãæå¤±ã¯ãªã—ã€‚ï¼‰"
      ],
      "metadata": {
        "id": "NW8a7J_dHgjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "å¥ªã£ãŸç¬é–“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã¿ã‹ã‚‰ï¼“ç§’å¾Œã®æˆåŠŸã‚’äºˆæ¸¬"
      ],
      "metadata": {
        "id": "mlkc9YLIOod5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç‰©ç†æƒ…å ±æä¾›å‹GNN (SoccerPIGNN) ã®å®Ÿè£…(è©¦ã—ï¼‰https://gemini.google.com/app/aa9bd03d4deab059?hl=ja\n",
        "zip_open_scientificdata6.ipynbã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã€‚åŸºæœ¬ç‰¹å¾´é‡ã®ã¿ã§ç‰©ç†çš„ã‚¢ãƒ¬ãƒ³ã‚¸ã‚’åŠ ãˆãŸãƒ¢ãƒ‡ãƒ«ã€‚\n",
        "åŸæ¡ˆï¼ˆåŠ é€Ÿåº¦ã®é …ã‚ã‚ŠãŒåŸæ¡ˆï¼‰ï¼šhttps://gemini.google.com/app/28c87c3fec42ca7e?hl=ja"
      ],
      "metadata": {
        "id": "l4YWVHcuezDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã‚¢ã‚¤ãƒ‡ã‚¢ã®ç†è«–ã®èª¬æ˜ï¼š1. ã‚°ãƒ©ãƒ•æ§‹é€ ã«ã‚ˆã‚‹ç©ºé–“è¡¨ç¾ãƒ”ãƒƒãƒä¸Šã®11äººï¼ˆã¾ãŸã¯ãã‚Œä»¥ä¸Šï¼‰ã®é¸æ‰‹ã‚’ã‚°ãƒ©ãƒ• $G = (V, E)$ ã¨ã—ã¦å®šç¾©ã—ã¾ã™ã€‚ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ $\\mathbf{x}_i$: é¸æ‰‹ã®åº§æ¨™ $p = (x, y)$ ã¨é€Ÿåº¦ $v = (v_x, v_y)$ ã‚’å«ã¿ã¾ã™ã€‚ã‚¨ãƒƒã‚¸ $e_{ij}$: é¸æ‰‹ $i$ ã¨ $j$ ã®é–“ã®ç›¸äº’ä½œç”¨ï¼ˆè·é›¢ã‚„ãƒ‘ã‚¹ã‚³ãƒ¼ã‚¹ã®å¯èƒ½æ€§ï¼‰ã‚’è¡¨ã—ã¾ã™ã€‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ï¼ˆç©ºé–“çš„ãªç‰¹å¾´æŠ½å‡ºï¼‰å„é¸æ‰‹ãŒå‘¨å›²ã®çŠ¶æ³ã‚’èª­ã¿å–ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€ä»¥ä¸‹ã®æ•°å¼ã§è¡¨ã•ã‚Œã¾ã™ã€‚$$\\mathbf{h}_i^{(l+1)} = \\sigma \\left( \\sum_{j \\in \\mathcal{N}(i)} \\text{MLP} \\left( \\mathbf{h}_i^{(l)}, \\mathbf{h}_j^{(l)}, \\mathbf{pos}_i - \\mathbf{pos}_j, \\mathbf{vel}_i - \\mathbf{vel}_j \\right) \\right)$$$\\mathbf{h}_i^{(l)}$: ãƒ¬ã‚¤ãƒ¤ãƒ¼ $l$ ã«ãŠã‘ã‚‹é¸æ‰‹ $i$ ã®æ½œåœ¨ç‰¹å¾´ï¼ˆã€Œã“ã®é¸æ‰‹ã¯ãƒãƒ£ãƒ³ã‚¹ã«çµ¡ã¿ãã†ã‹ã€ç­‰ã®æƒ…å ±ã®å¡Šï¼‰ã€‚$\\mathbf{pos}_i - \\mathbf{pos}_j$: é¸æ‰‹é–“ã®ç›¸å¯¾ä½ç½®ï¼ˆç‰©ç†çš„è·é›¢æ„Ÿï¼‰ã€‚$\\mathbf{vel}_i - \\mathbf{vel}_j$: é¸æ‰‹é–“ã®ç›¸å¯¾é€Ÿåº¦ï¼ˆè¿½ã„è¶Šãã†ã¨ã—ã¦ã„ã‚‹ã‹ã€é›¢ã‚Œã¦ã„ã‚‹ã‹ï¼‰ã€‚$\\sigma$: éç·šå½¢æ´»æ€§åŒ–é–¢æ•°ï¼ˆELUãªã©ï¼‰ã€‚2. ç‰©ç†æƒ…å ±ã«åŸºã¥ã„ãŸæå¤±é–¢æ•°ï¼ˆPIGNNã®æ ¸å¿ƒï¼‰é€šå¸¸ã®AIã®å­¦ç¿’ã§ã¯ã€äºˆæ¸¬ã¨æ­£è§£ï¼ˆSuccess/Failï¼‰ã®ã‚ºãƒ¬ã‚’è¦‹ã‚‹ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤± ($L_{task}$) ã ã‘ã‚’ä½¿ã„ã¾ã™ã€‚$$L_{task} = - \\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)]$$ã—ã‹ã—ã€PIGNNã§ã¯ã“ã“ã«ç‰©ç†çš„åˆ¶ç´„ ($L_{phys}$) ã‚’åŠ ç®—ã—ãŸç·æå¤± $L_{total}$ ã§å­¦ç¿’ã—ã¾ã™ã€‚$$L_{total} = L_{task} + \\alpha L_{phys}$$ã“ã“ã§ã€$\\alpha$ ã¯ç‰©ç†æ³•å‰‡ã‚’ã©ã‚Œãã‚‰ã„é‡è¦–ã™ã‚‹ã‹ã‚’æ±ºã‚ã‚‹é‡ã¿ä¿‚æ•°ï¼ˆä»Šå›ã®è¨­å®šã§ã¯ 0.01ï¼‰ã§ã™ã€‚ç‰©ç†æå¤±ã®å…·ä½“çš„å®šç¾©ï¼ˆé‹å‹•å­¦çš„åˆ¶ç´„ï¼‰é¸æ‰‹ãŒç¾åœ¨ã®ä½ç½® $\\mathbf{p}_t$ ã¨é€Ÿåº¦ $\\mathbf{v}_t$ ã‚’æŒã£ã¦ã„ã‚‹ã¨ãã€å¾®å°æ™‚é–“ $\\Delta t$ å¾Œã®äºˆæ¸¬ä½ç½® $\\hat{\\mathbf{p}}_{t+\\Delta t}$ ãŒç‰©ç†çš„ã«å¦¥å½“ã§ã‚ã‚‹ã‹ã‚’ä»¥ä¸‹ã®æ•°å¼ã§è©•ä¾¡ã—ã¾ã™ã€‚$$L_{phys} = \\frac{1}{N} \\sum_{i=1}^{N} \\| \\hat{\\mathbf{p}}_{i, t+\\Delta t} - (\\mathbf{p}_{i, t} + \\mathbf{v}_{i, t} \\Delta t) \\|^2$$æ„å‘³: ã€ŒAIãŒäºˆæ¸¬ã—ãŸæœªæ¥ã®ä½ç½®ã€ã¨ã€Œä»Šã®é€Ÿåº¦ã‹ã‚‰è¨ˆç®—ã—ãŸæ…£æ€§ç§»å‹•å¾Œã®ä½ç½®ã€ã®ã‚ºãƒ¬ã‚’è¨ˆç®—ã—ã¦ã„ã¾ã™ã€‚åŠ¹æœ: ã“ã®å€¤ãŒå¤§ãããªã‚‹ã¨ã€Œç‰©ç†çš„ã«ã‚ã‚Šå¾—ãªã„å‹•ãï¼ˆæ€¥ãªæ–¹å‘è»¢æ›ã‚„ç•°å¸¸ãªåŠ é€Ÿï¼‰ã€ã¨åˆ¤æ–­ã•ã‚Œã€AIã«ä¿®æ­£ï¼ˆå­¦ç¿’ï¼‰ãŒã‹ã‹ã‚Šã¾ã™ã€‚3. ã“ã®æ•°å¼ãŒå’è«–ã§ã©ã†å½¹ç«‹ã¤ã‹ï¼Ÿã“ã®æ•°å¼ã‚’è¼‰ã›ã‚‹ã“ã¨ã§ã€ã‚ãªãŸã®ç ”ç©¶ã®ç‹¬è‡ªæ€§ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«ä¸»å¼µã§ãã¾ã™ã€‚ã€Œå˜ãªã‚‹AIã§ã¯ãªã„ã€: ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¦šãˆã‚‹ã ã‘ã§ãªãã€**ã€Œé‹å‹•æ–¹ç¨‹å¼ã¨ã„ã†ç‰©ç†çš„ãªåˆ¶ç´„ã€**ã‚’ãƒ¢ãƒ‡ãƒ«ã«ç›´æ¥çµ„ã¿è¾¼ã‚“ã  (Physics-Informed) ç‚¹ã€‚ã€Œã‚µãƒƒã‚«ãƒ¼ã®æ–‡è„ˆã‚’ç†è§£ã—ã¦ã„ã‚‹ã€: GNNã®ç›¸å¯¾ä½ç½®ãƒ»é€Ÿåº¦ï¼ˆ$\\mathbf{pos}_i - \\mathbf{pos}_j, \\mathbf{vel}_i - \\mathbf{vel}_j$ï¼‰ã‚’ä½¿ã†ã“ã¨ã§ã€**ã€Œã‚¹ãƒšãƒ¼ã‚¹ã®å‰µå‡ºã€ã‚„ã€Œå®ˆå‚™ã®èƒŒå¾Œã¸ã®é£›ã³å‡ºã—ã€**ã¨ã„ã£ãŸå‹•çš„ãªé–¢ä¿‚ã‚’æ•°å¼ãƒ¬ãƒ™ãƒ«ã§æ‰ãˆã¦ã„ã‚‹ç‚¹ã€‚"
      ],
      "metadata": {
        "id": "tVnFjmyCfIx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "å…·ä½“çš„ã«ã€ã‚ãªãŸã®ã‚³ãƒ¼ãƒ‰ã®ä¸­ã§ä½•ãŒèµ·ãã¦ã„ã‚‹ã®ã‹ã€3ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã§å™›ã¿ç •ãã¾ã™ã€‚1. ã€ŒãŸã ã®ç‚¹ã€ã‹ã‚‰ã€Œå‹•ãè»Œè·¡ã€ã¸å¾“æ¥ã®GNNï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä»¥å‰ã®ä¸€èˆ¬çš„ãªãƒ¢ãƒ‡ãƒ«ï¼‰ã¯ã€é¸æ‰‹ã®åº§æ¨™ $(x, y)$ ã ã‘ã‚’è¦‹ã¦ã„ã¾ã—ãŸã€‚ã“ã‚Œã ã¨ã€ã€Œæ­¢ã¾ã£ã¦ã„ã‚‹é¸æ‰‹ã€ã¨ã€ŒçŒ›ãƒ€ãƒƒã‚·ãƒ¥ã—ã¦ã„ã‚‹é¸æ‰‹ã€ã®åŒºåˆ¥ãŒã¤ãã¾ã›ã‚“ã€‚ã‚ãªãŸã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ï¼ˆé¸æ‰‹é–“ã®æƒ…å ±äº¤æ›ï¼‰ã®éš›ã«ã€ä»¥ä¸‹ã®è¨ˆç®—ã‚’ã“ã£ãã‚Šè¡Œã£ã¦ã„ã¾ã™ã€‚ä»Šã®ä½ç½®: $p_t$ä»Šã®é€Ÿåº¦: $v_t$æœªæ¥ã®ä»®æƒ³ä½ç½®: $p_{future} = p_t + v_t \\times \\Delta t$ã“ã® $p_{future}$ ã‚’ä½¿ã£ã¦ã€é¸æ‰‹åŒå£«ã®è·é›¢ã‚„é–¢ä¿‚æ€§ã‚’è¨ˆç®—ã—ç›´ã—ã¦ã„ã¾ã™ã€‚ã“ã‚ŒãŒã€ŒåŸ‹ã‚è¾¼ã¿ã«å°†æ¥ä½ç½®ã‚’å…¥ã‚Œã‚‹ã€ã¨ã„ã†æ“ä½œã®å®Ÿä½“ã§ã™ã€‚2. ã€Œç‰©ç†çš„ãªæ„å‘³ã€ã§ã®åŸ‹ã‚è¾¼ã¿ã€ŒåŸ‹ã‚è¾¼ã¿ã€ã¨ã¯ã€AIãŒæ‰±ã„ã‚„ã™ã„æ•°å€¤ã®å¡Šï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã«å¤‰æ›ã™ã‚‹ã“ã¨ã§ã™ã€‚æ™®é€šã®AI: ã€Œé¸æ‰‹Aã¯ $(10, 20)$ ã«ã„ã‚‹ã€ã¨ã„ã†æƒ…å ±ã ã‘ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã«ã™ã‚‹ã€‚ã‚ãªãŸã®AI: ã€Œé¸æ‰‹Aã¯ $(10, 20)$ ã«ã„ã‚‹ãŒã€æ¬¡ã®ç¬é–“ã«ã¯ $(12, 21)$ ã«åˆ°é”ã™ã‚‹å‹¢ã„ã‚’æŒã£ã¦ã„ã‚‹ã€ã¨ã„ã†æƒ…å ±ã‚’ã²ã¨ã¾ã¨ã‚ã«ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆåŸ‹ã‚è¾¼ã¿ï¼‰ã«ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€GNNã®å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆå±¤ï¼‰ã‚’ãƒ‡ãƒ¼ã‚¿ãŒé€šéã™ã‚‹ãŸã³ã«ã€AIã¯**ã€Œã“ã„ã¤ã¯ã“ã®ã‚¹ãƒšãƒ¼ã‚¹ã«é£›ã³è¾¼ã‚‚ã†ã¨ã—ã¦ã„ã‚‹ãªã€**ã¨ã„ã†æœªæ¥ã®æ„å›³ã‚’æ±²ã¿å–ã£ãŸçŠ¶æ…‹ã§è¨ˆç®—ã‚’é€²ã‚ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚3. ãªãœã€Œç­‰é€Ÿã€ã§ã‚‚ã€ŒåŠ é€Ÿåº¦ã€ã®æº–å‚™ã«ãªã‚‹ã®ã‹ï¼Ÿç¾åœ¨ã¯ç°¡å˜ã®ãŸã‚ã« $\\Delta t$ ï¼ˆå¾®å°æ™‚é–“ï¼‰ã‚’ã‹ã‘ãŸç­‰é€Ÿãƒ¢ãƒ‡ãƒ«ã§ã™ãŒã€ã“ã‚Œã¯**ã€Œæœªæ¥ã¸ã®çª“ã€**ã‚’é–‹ã‘ãŸã“ã¨ã¨åŒã˜ã§ã™ã€‚ä»Šã®ã‚³ãƒ¼ãƒ‰: $p + v\\Delta t$ ï¼ˆä»Šã®å‹¢ã„ã®ã¾ã¾é€²ã‚€ï¼‰ä»Šå¾Œã®æ”¹è‰¯: $p + v\\Delta t + \\frac{1}{2}a\\Delta t^2$ ï¼ˆåŠ é€Ÿã—ã¦ã‚°ãƒ³ã¨ä¼¸ã³ã‚‹ï¼‰ç†è«–å¼ã‚’å…ˆã«æ›¸ã„ã¦ãŠãã“ã¨ã§ã€ã€Œä»Šã¯åŠ é€Ÿåº¦ $a=0$ ã¨ä»®å®šã—ãŸç‰¹æ®Šãªã‚±ãƒ¼ã‚¹ã‚’è§£ã„ã¦ã„ã‚‹ãŒã€ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹é€ è‡ªä½“ã¯æœªæ¥ã®ç©ºé–“å æœ‰ã‚’è€ƒæ…®ã§ãã‚‹è¨­è¨ˆã«ãªã£ã¦ã„ã‚‹ã€ã¨ä¸»å¼µã§ãã‚‹ã‚ã‘ã§ã™"
      ],
      "metadata": {
        "id": "d7WIqeGSgj1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã“ã® combined_feature ã‚’å—ã‘å–ã£ãŸMLPã¯ã€å†…éƒ¨ã®é‡ã¿ï¼ˆã‚¦ã‚§ã‚¤ãƒˆï¼‰ã«ã‚ˆã£ã¦ã€å®Ÿè³ªçš„ã«ä»¥ä¸‹ã®è¨ˆç®—ã‚’ã—ã¦ã„ã‚‹ã“ã¨ã¨åŒç­‰ã«ãªã‚Šã¾ã™ã€‚$$\\text{Future\\_Interaction} \\approx W_1(pos_j - pos_i) + W_2(vel_j - vel_i) \\times \\Delta t$$ã¤ã¾ã‚Šã€**ã€Œä»Šã®è·é›¢ã€ã«ã€Œæœªæ¥ã®æ¥è¿‘åº¦åˆã„ã€ã‚’åŠ å‘³ã—ãŸç‰¹åˆ¥ãªç‰¹å¾´é‡ï¼ˆåŸ‹ã‚è¾¼ã¿ï¼‰**ã‚’ä½œã‚Šå‡ºã—ã¦ã„ã‚‹ã®ã§ã™ã€‚1. ãªãœã€Œ$\\Delta t$ ã‚’æ›ã‘ã¦ã„ã‚‹ã€ã¨è¨€ãˆã‚‹ã®ã‹ï¼ŸMLPï¼ˆå¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ï¼‰ã¯ã€å…¥åŠ›ã•ã‚ŒãŸå€¤ã«ã€Œé‡ã¿ï¼ˆ$W$ï¼‰ã€ã‚’æ›ã‘ã¦è¶³ã—åˆã‚ã›ã‚‹è¨ˆç®—æ©Ÿã§ã™ã€‚ã‚ãªãŸãŒæ›¸ã„ãŸã‚³ãƒ¼ãƒ‰ï¼šcombined_feature = torch.cat([rel_pos, rel_vel], dim=-1)ã“ã‚Œã‚’MLPãŒå‡¦ç†ã™ã‚‹æ™‚ã€å†…éƒ¨ã§ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªè¨ˆç®—ï¼ˆç·šå½¢çµåˆï¼‰ãŒè¡Œã‚ã‚Œã¾ã™ã€‚$$\\text{Output} = W_{pos} \\cdot (\\text{rel\\_pos}) + W_{vel} \\cdot (\\text{rel\\_vel}) + \\text{bias}$$ã“ã“ã§ã€$W_{vel}$ ã«æ³¨ç›®ã—ã¦ãã ã•ã„ã€‚ã‚‚ã—ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ãŒã€Œ0.5ç§’å¾Œã®çŠ¶æ³ãŒé‡è¦ã ã€ã¨å­¦ç¿’ã—ãŸå ´åˆã€$W_{vel}$ ã®å€¤ã¯è‡ªç„¶ã¨ã€Œ0.5ã€ã«è¿‘ã„å€¤ã¸ã¨æ›´æ–°ã•ã‚Œã¦ã„ãã¾ã™ã€‚ ã¤ã¾ã‚Šã€é‡ã¿ $W_{vel}$ ãã®ã‚‚ã®ãŒã€ç‰©ç†å¼ã«ãŠã‘ã‚‹ $\\Delta t$ ã®å½¹å‰²ã‚’è‚©ä»£ã‚ã‚Šã—ã¦ã„ã‚‹ã®ã§ã™ã€‚2. ã€Œç‰©ç†çš„ãªæ„å‘³ã€ã®è‡ªå‹•æŠ½å‡ºAIã¯ã€Œé€Ÿåº¦ãŒã“ã‚Œãã‚‰ã„ã§ã€ä½ç½®ãŒã“ã‚Œãã‚‰ã„ãªã‚‰ã€æ¬¡ã¯ã“ã†ãªã‚‹ã¯ãšã ã€ã¨ã„ã†æ­£è§£ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆSuccess/Failï¼‰ã‚’ä½•ç™¾å›ã‚‚è¦‹ã›ã‚‰ã‚Œã¾ã™ã€‚ãã®éç¨‹ã§ï¼šã€Œä½ç½®ã®å·®ï¼ˆrel_posï¼‰ã ã‘è¦‹ã‚‹ã‚ˆã‚Šã€é€Ÿåº¦ã®å·®ï¼ˆrel_velï¼‰ã‚’è¶³ã—ã¦è€ƒãˆãŸã»ã†ãŒã€æœªæ¥ã®è¡çªã‚„ã‚¹ãƒšãƒ¼ã‚¹ã®ç™ºç”Ÿã‚’äºˆæ¸¬ã—ã‚„ã™ã„ã€ã¨ã„ã†ã“ã¨ã«MLPãŒæ°—ã¥ãã¾ã™ã€‚çµæœã¨ã—ã¦ã€é‡ã¿ $W$ ã‚’é€šã˜ã¦ã€Œä½ç½® + (é€Ÿåº¦ $\\times$ æ™‚é–“)ã€ã«ç›¸å½“ã™ã‚‹ç‰¹å¾´é‡ã‚’å†…éƒ¨ã§ä½œã‚Šå‡ºã™ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚ã“ã‚ŒãŒã€Œå°†æ¥ä½ç½®ã®äºˆæ¸¬ã‚’åŸ‹ã‚è¾¼ã¿ã«å«ã‚“ã§ã„ã‚‹ã€ã¨ã„ã†è¨€è‘‰ã®çœŸæ„ã§ã™ã€‚\n",
        "æ•°å¼ã§ã¯ $\\Delta t$ ã‚’å›ºå®šã—ã¾ã™ãŒã€MLPã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€Œã©ã®ç¨‹åº¦ã®æ™‚é–“å¹…ï¼ˆ$\\Delta t$ï¼‰ã§äºˆæ¸¬ã™ã‚‹ã®ãŒæœ€ã‚‚æ­£è§£ç‡ãŒé«˜ã„ã‹ã€ã‚’é‡ã¿ $W$ ã‚’é€šã˜ã¦å‹•çš„ã«æ±ºå®šã—ã¾ã™ã€‚ã“ã‚Œã¯**ã€Œæœ€é©åŒ–ã•ã‚ŒãŸæ™‚é–“ç©åˆ†ã€**ã¨å‘¼ã°ã‚Œã€ç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ç½®ãæ›ãˆã‚‹éš›ã®æ¨™æº–çš„ãªæ‰‹æ³•ã§ã™\n",
        "è¦ã™ã‚‹ã«æ™‚é–“å¹…Î”tã‚’AIãŒï¼ˆå­¦ç¿’ã™ã‚‹é‡ã¿ã«ã‚ˆã£ã¦ï¼‰ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹çš„ã«æ±ºã‚ã¦ã„ã‚‹â†’ç‰©ç†æå¤±ã«ã‚ˆã£ã¦ç¾å®Ÿã®ãƒ‡ãƒ¼ã‚¿ã§æ®´ã£ã¦ç‰©ç†ç¾è±¡ã¨ä¹–é›¢ã—ãªã„ã‚ˆã†ã«ã—ã¦ã„ã‚‹ã€‚"
      ],
      "metadata": {
        "id": "k-x8lqA8hTU3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç†è«–ã®ç« ã®è‰æ¡ˆï¼š\n",
        "ç¬¬3ç«  ç†è«–çš„èƒŒæ™¯ã¨ææ¡ˆæ‰‹æ³•3.1 ç‰©ç†æƒ…å ±å‹ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆPIGNNï¼‰ã®è¨­è¨ˆæ€æƒ³æœ¬ç ”ç©¶ã§ææ¡ˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã®å­¦ç¿’èƒ½åŠ›ã‚’æŒã¤ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆGNNï¼‰ã«ã€é‹å‹•å­¦çš„ãªåˆ¶ç´„ã‚’çµ±åˆã—ãŸç‰©ç†æƒ…å ±å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆPhysics-Informed Neural Networks; PINNsï¼‰ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚ã“ã‚Œã¯ã€é™ã‚‰ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆç´„800ä»¶ï¼‰ã«ãŠã„ã¦ç”Ÿã˜ã‚„ã™ã„ã€ç‰©ç†çš„ã«ä¸åˆç†ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã¸ã®éå­¦ç¿’ã‚’æŠ‘åˆ¶ã—ã€æ±åŒ–æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã‚‹ã€‚3.2 é‹å‹•å­¦çš„å°†æ¥ä½ç½®äºˆæ¸¬ã®æ½œåœ¨çš„åŸ‹ã‚è¾¼ã¿æœ¬ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°éç¨‹ã§ã¯ã€ç¾åœ¨ã®ãƒãƒ¼ãƒ‰ï¼ˆé¸æ‰‹ï¼‰ã®ä½ç½® $\\mathbf{p}_i(t)$ ã«åŠ ãˆã€é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ« $\\mathbf{v}_i(t)$ ã‚’å…¥åŠ›ç‰¹å¾´é‡ã¨ã—ã¦ç”¨ã„ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã¯æ½œåœ¨ç©ºé–“ï¼ˆLatent Spaceï¼‰ã«ãŠã„ã¦ã€ä¸€æ¬¡ã®ãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ã«åŸºã¥ãå°†æ¥ä½ç½®ã®è¿‘ä¼¼ã‚’å†…éƒ¨çš„ã«æ§‹æˆã™ã‚‹ã€‚é¸æ‰‹ $i$ ã¨ $j$ ã®ç›¸äº’ä½œç”¨ï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰ $m_{ij}$ ã¯ä»¥ä¸‹ã®é–¢æ•°ã§å®šç¾©ã•ã‚Œã‚‹ï¼š$$m_{ij} = \\text{MLP} \\left( \\mathbf{h}_i, \\mathbf{h}_j, (\\mathbf{p}_j - \\mathbf{p}_i), (\\mathbf{v}_j - \\mathbf{v}_i) \\right)$$ã“ã“ã§ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆMLPï¼‰å†…ã®ç·šå½¢çµåˆã«ãŠã„ã¦ã€ç›¸å¯¾é€Ÿåº¦ $(\\mathbf{v}_j - \\mathbf{v}_i)$ ã«å¯¾ã™ã‚‹é‡ã¿è¡Œåˆ— $W_v$ ã¯ã€ç‰©ç†å­¦çš„ãªæ™‚é–“å¹… $\\Delta t$ ã®å½¹å‰²ã‚’å†…åŒ…ã™ã‚‹ã€‚ã™ãªã‚ã¡ã€ç‰¹å¾´æŠ½å‡ºãƒ—ãƒ­ã‚»ã‚¹ã«ãŠã„ã¦ä»¥ä¸‹ã®é‹å‹•å­¦çš„è¿‘ä¼¼ãŒæˆç«‹ã—ã¦ã„ã‚‹ï¼š$$\\text{Future\\_Context} \\approx W_p(\\mathbf{p}_j - \\mathbf{p}_i) + W_v(\\mathbf{v}_j - \\mathbf{v}_i) \\cdot \\Delta t$$ã“ã‚Œã«ã‚ˆã‚Šã€GNNã¯å˜ãªã‚‹é™çš„ãªé…ç½®ã§ã¯ãªãã€å°†æ¥çš„ãªç©ºé–“å æœ‰ã®å¯èƒ½æ€§ã‚’è€ƒæ…®ã—ãŸå‹•çš„ãªéš£æ¥æ€§ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã¨ãªã‚‹ã€‚3.3 ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æå¤±é–¢æ•°ã«ã‚ˆã‚‹ç‰©ç†çš„åˆ¶ç´„ã®å°å…¥ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¯ã€ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®æˆå¦ã‚’åˆ¤å®šã™ã‚‹åˆ†é¡æå¤± $L_{task}$ ã¨ã€äºˆæ¸¬ã®ç‰©ç†çš„å¦¥å½“æ€§ã‚’æ‹…ä¿ã™ã‚‹ç‰©ç†æå¤± $L_{phys}$ ã®åŠ é‡å’Œã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã§è¡Œã‚ã‚Œã‚‹ã€‚$$L_{total} = L_{task} + \\alpha L_{phys}$$3.3.1 é‹å‹•å­¦çš„æ•´åˆæ€§ï¼ˆç‰©ç†æå¤±ï¼‰ã®å®šç¾©ç‰©ç†æå¤± $L_{phys}$ ã¯ã€äºˆæ¸¬ã•ã‚ŒãŸæ¬¡çŠ¶æ…‹ã®åº§æ¨™ $\\hat{\\mathbf{p}}_{t+\\Delta t}$ ãŒã€ç¾åœ¨ã®é‹å‹•çŠ¶æ…‹ã‹ã‚‰è¨ˆç®—ã•ã‚Œã‚‹ç†è«–ä¸Šã®æ…£æ€§ç§»å‹•è·é›¢ã‹ã‚‰é€¸è„±ã—ã¦ã„ãªã„ã‹ã‚’è©•ä¾¡ã™ã‚‹ã€‚$$L_{phys} = \\frac{1}{N} \\sum_{i=1}^{N} \\| \\hat{\\mathbf{p}}_{i, t+\\Delta t} - (\\mathbf{p}_{i, t} + \\mathbf{v}_{i, t} \\Delta t) \\|^2$$ã“ã“ã§ã€$\\Delta t$ ã¯ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”ã«åŸºã¥ãå®šæ•°ï¼ˆå®Ÿæ™‚é–“ï¼‰ã§ã‚ã‚‹ã€‚ã“ã®æå¤±é–¢æ•°ã®å°å…¥ã«ã‚ˆã‚Šã€GNNå†…éƒ¨ã®é‡ã¿ $W$ ãŒã€Œæ­£è§£ãƒ©ãƒ™ãƒ«ã¨ã®é©åˆã€ã®ã¿ãªã‚‰ãšã€Œç¾å®Ÿã®ç‰©ç†æ³•å‰‡ï¼ˆ$\\Delta t$ ãƒ•ãƒ¬ãƒ¼ãƒ å†…ã®é€£ç¶šæ€§ï¼‰ã€ã‚’éµå®ˆã™ã‚‹ã‚ˆã†ã«æœ€é©åŒ–ã•ã‚Œã‚‹ã€‚ã“ã‚Œã¯ã€æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹çš„ãªå°†æ¥äºˆæ¸¬ã«å¯¾ã—ã€ç‰©ç†çš„è§£é‡ˆå¯èƒ½æ€§ã¨ã„ã†åˆ¶ç´„ã‚’ä»˜ä¸ã—ã€éå­¦ç¿’ã‚’æ•°å­¦çš„ã«æŠ‘åˆ¶ã™ã‚‹åŠ¹æœã‚’æŒã¤ã€‚å®¢è¦³çš„ãªãƒã‚¤ãƒ³ãƒˆãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ã¨ã„ã†è¨€è‘‰ã‚’å‡ºã™ã“ã¨ã§ã€ãªãœé€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã‚’å…¥åŠ›ã™ã‚‹ã®ã‹ã«æ•°å­¦çš„æ ¹æ‹ ã‚’ä¸ãˆã¦ã„ã¾ã™ã€‚**ã€Œ$\\Delta t$ ã‚’é‡ã¿ãŒå†…åŒ…ã™ã‚‹ã€**ã¨æ›¸ãã“ã¨ã§ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã«æ˜ç¤ºçš„ã«æ›¸ã„ã¦ã„ãªã„éƒ¨åˆ†ã‚’ç†è«–çš„ã«è£œå®Œã—ã¦ã„ã¾ã™ã€‚**PINNsï¼ˆç‰©ç†æƒ…å ±å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰**ã¨ã„ã†ã€ç¾åœ¨AIç•Œéšˆã§éå¸¸ã«è©•ä¾¡ã®é«˜ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ã„ã€æ‰‹æ³•ã®æ­£å½“æ€§ã‚’é«˜ã‚ã¦ã„ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "PBwRt6fqjNgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç¬¬3ç«  ææ¡ˆæ‰‹æ³•ï¼šç‰©ç†æƒ…å ±å‹ã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (PIGNN)3.1 ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æœ¬ç ”ç©¶ã§ã¯ã€ã‚µãƒƒã‚«ãƒ¼ã®ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼æ”»æ’ƒã«ãŠã‘ã‚‹æˆå¦åˆ¤å®šã®ãŸã‚ã€é¸æ‰‹ã®ç©ºé–“çš„é–¢ä¿‚æ€§ã¨é‹å‹•å­¦çš„ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’åŒæ™‚ã«å‡¦ç†å¯èƒ½ãªPhysics-Informed Graph Neural Network (PIGNN) ã‚’ææ¡ˆã™ã‚‹ã€‚æœ¬ãƒ¢ãƒ‡ãƒ«ã¯ã€å…¥åŠ›å±¤ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°å±¤ï¼ˆPIGNN Layerï¼‰ã€ãŠã‚ˆã³åˆ¤å®šå±¤ï¼ˆMLPï¼‰ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ã€‚3.2 ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã«ãŠã‘ã‚‹å°†æ¥ä½ç½®ã®åŸ‹ã‚è¾¼ã¿ææ¡ˆãƒ¢ãƒ‡ãƒ«ã®æ ¸å¿ƒã¯ã€GNNã®ç‰¹å¾´æŠ½å‡ºãƒ—ãƒ­ã‚»ã‚¹ã«ãŠã„ã¦ã€å˜ãªã‚‹åº§æ¨™ $p$ ã§ã¯ãªãã€é€Ÿåº¦ $v$ ã‚’å«ã‚ãŸç›¸å¯¾çš„ãªé‹å‹•çŠ¶æ…‹ã‚’ç‰¹å¾´é‡ã¨ã—ã¦åŸ‹ã‚è¾¼ã‚€ï¼ˆEmbeddingï¼‰ç‚¹ã«ã‚ã‚‹ã€‚å„ãƒãƒ¼ãƒ‰ï¼ˆé¸æ‰‹ï¼‰é–“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆé–¢æ•°ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã‚‹ï¼š$$m_{ij} = \\text{MLP} \\left( \\text{concat}(\\mathbf{p}_j - \\mathbf{p}_i, \\mathbf{v}_j - \\mathbf{v}_i) \\right)$$ã“ã“ã§ã€ç›¸å¯¾é€Ÿåº¦ $(\\mathbf{v}_j - \\mathbf{v}_i)$ ãŒå…¥åŠ›ã•ã‚Œã‚‹ã“ã¨ã§ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å†…éƒ¨ã®å­¦ç¿’ã•ã‚ŒãŸé‡ã¿ $W_v$ ã‚’ä»‹ã—ã€å®Ÿè³ªçš„ã«ä¸€æ¬¡ã®ãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ã«åŸºã¥ã„ãŸå°†æ¥äºˆæ¸¬ãŒæ½œåœ¨ç©ºé–“ï¼ˆLatent Spaceï¼‰ä¸Šã§å®Ÿè¡Œã•ã‚Œã‚‹ã€‚$$\\text{Embedding}_{ij} \\approx W_p(\\Delta \\mathbf{p}) + W_v(\\Delta \\mathbf{v})$$ã“ã® $W_v$ ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€é©åŒ–ã•ã‚ŒãŸã€Œäºˆè¦‹æ™‚é–“ï¼ˆEffective Time Horizonï¼‰ã€ã¨ã—ã¦ã®å½¹å‰²ã‚’æœãŸã—ã€ãƒ¢ãƒ‡ãƒ«ã¯ã€Œæ•°ãƒ•ãƒ¬ãƒ¼ãƒ å¾Œã«é¸æ‰‹ãŒåˆ°é”ã™ã‚‹ã§ã‚ã‚ã†ã‚¹ãƒšãƒ¼ã‚¹ã€ã‚’è€ƒæ…®ã—ãŸå‹•çš„ãªéš£æ¥æ€§ã‚’æŠ½å‡ºã™ã‚‹ã€‚3.3 ç‰©ç†æå¤±é–¢æ•°ï¼ˆPhysics Lossï¼‰ã«ã‚ˆã‚‹æ‹˜æŸæ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãŒç‰©ç†çš„ã«ä¸å¯èƒ½ãªç§»å‹•çµŒè·¯ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã‚’é˜²ããŸã‚ã€æœ¬ç ”ç©¶ã§ã¯æå¤±é–¢æ•°ã«ç‰©ç†çš„æ•´åˆæ€§åˆ¶ç´„ã‚’å°å…¥ã™ã‚‹ã€‚å…¨æå¤±é–¢æ•° $L_{total}$ ã¯ã€åˆ†é¡ç²¾åº¦ã‚’å¸ã‚‹ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤± $L_{task}$ ã¨ã€é‹å‹•å­¦çš„æ•´åˆæ€§ã‚’å¸ã‚‹ç‰©ç†æå¤± $L_{phys}$ ã®åŠ é‡å’Œã§ã‚ã‚‹ï¼š$$L_{total} = L_{task} + \\lambda L_{phys} \\quad (\\lambda = 0.01)$$ã“ã“ã§ã€ç‰©ç†æå¤± $L_{phys}$ ã¯ä»¥ä¸‹ã®é‹å‹•æ–¹ç¨‹å¼ã®æ®‹å·®ã¨ã—ã¦å®šç¾©ã•ã‚Œã‚‹ï¼š$$L_{phys} = \\frac{1}{N} \\sum_{i=1}^{N} \\| \\hat{\\mathbf{p}}_{i, t+\\Delta t} - (\\mathbf{p}_{i, t} + \\mathbf{v}_{i, t} \\Delta t) \\|^2$$æœ¬ç ”ç©¶ã®å®Ÿè£…ã«ãŠã„ã¦ã¯ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã«åŸºã¥ã $\\Delta t = 0.1$ [s] ã¨è¨­å®šã—ã¦ã„ã‚‹ã€‚ã“ã® $L_{phys}$ ãŒæœ€å°åŒ–ã•ã‚Œã‚‹ã“ã¨ã§ã€AIãŒå†…éƒ¨ã§æƒ³å®šã™ã‚‹ã€Œäºˆè¦‹æ™‚é–“ã€ã®ã‚¹ã‚±ãƒ¼ãƒ«ãŒç¾å®Ÿã® 0.1ç§’å˜ä½ã®ç‰©ç†ç¾è±¡ã¸ã¨å¼·åˆ¶çš„ã«å¼•ãæˆ»ã•ã‚Œã€ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹åŒ–ã•ã‚ŒãŒã¡ãªç‰¹å¾´ç©ºé–“ã«ç‰©ç†çš„è§£é‡ˆå¯èƒ½æ€§ãŒä»˜ä¸ã•ã‚Œã‚‹ã€‚3.4 ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã«ã‚ˆã‚‹æ±åŒ–æ€§èƒ½ã®ç¢ºä¿å°è¦æ¨¡ãªã‚µãƒƒã‚«ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ$n \\approx 800$ï¼‰ã«ãŠã‘ã‚‹éå­¦ç¿’ã‚’æŠ‘åˆ¶ã™ã‚‹ãŸã‚ã€åˆ¤å®šå±¤ã®å‰ã«ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆï¼ˆ$p = 0.3$ï¼‰ã‚’å°å…¥ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç‰¹å®šã®é¸æ‰‹é…ç½®ã‚„ãƒãƒ¼ãƒ‰ã®çµ„ã¿åˆã‚ã›ã¸ã®ä¾å­˜ã‚’æ’ã—ã€ç‰©ç†çš„ãªæ–‡è„ˆï¼ˆã‚¹ãƒšãƒ¼ã‚¹ã¨é€Ÿåº¦ã®é–¢ä¿‚æ€§ï¼‰ã«åŸºã¥ã„ãŸé ‘å¥ãªåˆ¤æ–­åŸºæº–ã‚’ç²å¾—ã•ã›ãŸ"
      ],
      "metadata": {
        "id": "Q9nY9jdLjWAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ã¨ç‰©ç†æå¤±ã®å¦¥å½“æ€§ï¼š\n",
        "ææ¡ˆæ‰‹æ³•ã§è¨€åŠã•ã‚Œã¦ã„ã‚‹ã€Œä¸€æ¬¡ã®ãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ã«ã‚ˆã‚‹å°†æ¥ä½ç½®è¿‘ä¼¼ã€ãŠã‚ˆã³ã€Œç‰©ç†æå¤±ã‚’é€šã˜ãŸ $\\Delta t$ ã®é‡ã¿å­¦ç¿’ã€ã¯ã€ä»–åˆ†é‡ï¼ˆæµ·äº‹ãƒ»ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ï¼‰ã®æœ€å…ˆç«¯ç ”ç©¶ã§ãã®æœ‰åŠ¹æ€§ãŒè¨¼æ˜ã•ã‚Œã¦ã„ã‚‹ã€‚æµ·äº‹ãƒˆãƒ©jectoryäºˆæ¸¬ (2025): èˆ¹èˆ¶ã®è»Œé“äºˆæ¸¬ã«ãŠã„ã¦ã€ãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ã«åŸºã¥ãä¸€ãƒ»äºŒæ¬¡ã®æœ‰é™å·®åˆ†ç‰©ç†æå¤±ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€ãƒã‚¤ã‚ºã®å¤šã„ãƒ‡ãƒ¼ã‚¿ä¸‹ã§ã‚‚äºˆæ¸¬èª¤å·®ã‚’32%å‰Šæ¸›ã—ãŸäº‹ä¾‹ãŒã‚ã‚‹ 4ã€‚SoccerPIGNNãŒã“ã‚Œã‚’ã‚µãƒƒã‚«ãƒ¼ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆWyscoutï¼‰ã«é©ç”¨ã™ã‚‹ã“ã¨ã¯ã€**ã€Œç–ãªãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é¸æ‰‹ã®æ„å›³ã‚’ç‰©ç†çš„ã«å¤–æŒ¿ã™ã‚‹ã€**ã¨ã„ã†èª²é¡Œã«å¯¾ã™ã‚‹æ¥µã‚ã¦åˆç†çš„ãªè§£æ±ºç­–ã§ã‚ã‚‹ã€‚Equi-Euler GraphNet: ãƒãƒ¼ãƒ‰æ›´æ–°ã«ã‚ªã‚¤ãƒ©ãƒ¼ç©åˆ†ï¼ˆ$p + v\\Delta t$ï¼‰ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€å¤šä½“ç³»ã®åŠ›å­¦ã‚’å­¦ç¿’ã™ã‚‹GNNãŒå­˜åœ¨ã™ã‚‹ã€‚SoccerPIGNNã®ã€ŒMLPã®é‡ã¿ãŒ $\\Delta t$ ã‚’è‚©ä»£ã‚ã‚Šã™ã‚‹ã€ã¨ã„ã†è§£é‡ˆã¯ã€ã“ã®å­¦è¡“çš„æ–‡è„ˆï¼ˆæœ€é©åŒ–ã•ã‚ŒãŸæ™‚é–“ç©åˆ†ï¼‰ã¨åˆè‡´ã—ã¦ãŠã‚Šã€ç†è«–çš„ãªèª¬å¾—åŠ›ãŒéå¸¸ã«å¼·ã„ã€‚"
      ],
      "metadata": {
        "id": "Spj1pZBCriBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "â†‘ä¸Šã®è¨˜è¿°ã¯å³å¯†ã«ã¯é–“é•ã„ã€‚ä»Šå›ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ç‰©ç†æå¤±ã¯çµ„ã¿è¾¼ã‚“ã§ã„ãªã„ã€‚ãªãœãªã‚‰ã€ãƒ¢ãƒ‡ãƒ«ã¯ã‚¿ã‚¹ã‚¯ã¨ã—ã¦äºˆæ¸¬ç¢ºç‡ã‚’å‡ºåŠ›ã™ã‚‹ã®ã§ä¸Šå¼ã®ç‰©ç†æå¤±ï¼ˆåº§æ¨™ã‚’å‡ºåŠ›ã—ã¦ã„ãªã„ã¨è¨ˆç®—ã§ããªã„ï¼‰ã¯å®Ÿè£…ã—ã¦ã„ãªã„ã€‚ä»Šå›ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã„ã‚ã°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å‹ã§ã‚ã‚Šã€æ§‹é€ ã¨ã—ã¦ç­‰é€Ÿç›´ç·šé‹å‹•ã‚’å¼·åˆ¶ã™ã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹ã€‚ï¼ˆç‰©ç†æå¤±ã¯Losså‹ã¨åä»˜ã‘ã‚‰ã‚Œã‚‹ã€‚ï¼‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å‹ã®æ–¹ãŒãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå°ã•ã„å ´åˆã«æœ‰ç”¨ã‚‰ã—ã„ã€‚\n",
        "ä»Šå›ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å‹ã®åŸºæœ¬ç™ºæƒ³ã¯ã€ã¾ãšè‡ªåˆ†ãŒæ±ºã‚ãŸäºˆè¦‹æ™‚é–“Ï„ï¼1.5sã‚’ä½¿ç”¨ã—ã¦å°†æ¥äºˆæ¸¬ä½ç½®ã‚’è¨ˆç®—ã™ã‚‹â†’ãã®å¾Œãã®äºˆæ¸¬ä½ç½®ãŒéš£æ¥é¸æ‰‹ã¨ç­‰ã—ããªã‚‹ï¼ã¶ã¤ã‹ã‚‹å ´åˆãƒã‚¤ã‚¢ã‚¹ãŒï¼‘ã«è¿‘ã¥ãã‚ˆã†ã«ã™ã‚‹â†’ã“ã®ãƒã‚¤ã‚¢ã‚¹ã§ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è£œæ­£ã™ã‚‹ã€‚ã¾ãŸã€æå¤±é–¢æ•°ã«é€Ÿåº¦ã®é‹å‹•å­¦çš„åˆ¶ç´„ã‚’èª²ã™ã“ã¨ã§é‹å‹•å­¦çš„ã«ã‚ã‚Šãˆãªã„é€Ÿåº¦ã§ã®ç§»å‹•ã«ã¯ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’ä¸ãˆã¦ã„ã‚‹ã¨ã„ã†ã‚‚ã®ã€‚"
      ],
      "metadata": {
        "id": "HFCw5WyY9wHS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä»¥ä¸‹ã€å’è«–ã®ç†è«–ãƒ‘ãƒ¼ãƒˆæ¡ˆï¼šææ¡ˆæ‰‹æ³•ã®ç†è«–çš„å®šå¼åŒ–1. ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã¨æ½œåœ¨ç©ºé–“ã¸ã®å†™åƒæœ¬ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã¯ã€ãƒ”ãƒƒãƒä¸Šã® $N$ å€‹ã®ãƒãƒ¼ãƒ‰ï¼ˆé¸æ‰‹ï¼‰ã§ã‚ã‚Šã€å„ãƒãƒ¼ãƒ‰ $i$ ã¯æ™‚åˆ» $t$ ã«ãŠã‘ã‚‹ç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ« $\\vec{x}_i = [p_i, v_i, d_i]^T$ ã‚’æŒã¤ã€‚ã“ã“ã§ $p_i \\in \\mathbb{R}^2$ ã¯åº§æ¨™ã€$v_i \\in \\mathbb{R}^2$ ã¯é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã€$d_i$ ã¯ã‚´ãƒ¼ãƒ«ã‚„ãƒœãƒ¼ãƒ«ã¸ã®ç›¸å¯¾è·é›¢ã§ã‚ã‚‹ã€‚å„ãƒãƒ¼ãƒ‰ã®ç‰¹å¾´é‡ã¯ã€ç·šå½¢å¤‰æ›è¡Œåˆ— $W \\in \\mathbb{R}^{d' \\times d}$ ã‚’ç”¨ã„ã¦æ½œåœ¨ç©ºé–“ä¸Šã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ« $\\vec{h}_i$ ã¸å†™åƒã•ã‚Œã‚‹ã€‚$$\\vec{h}_i = W \\vec{x}_i = W_p p_i + W_v v_i + W_d d_i \\quad \\dots (1)$$ã“ã“ã§ã€$W_p, W_v$ ã¯ãã‚Œãã‚Œä½ç½®ãŠã‚ˆã³é€Ÿåº¦æˆåˆ†ã«å¯¾ã™ã‚‹å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚‹ã€‚2. æ½œåœ¨ç©ºé–“ã«ãŠã‘ã‚‹é‹å‹•å­¦çš„äºˆè¦‹ã®ç²å¾—å¼(1)ã«ãŠã„ã¦ã€$W_p$ ã‚’å…±é€šå› å­ã¨ã—ã¦æŠ½å‡ºã™ã‚‹ã¨ä»¥ä¸‹ã®è¿‘ä¼¼å¼ãŒå¾—ã‚‰ã‚Œã‚‹ã€‚$$\\vec{h}_i \\approx W_p \\left( p_i + \\frac{W_v}{W_p} v_i \\right) \\quad \\dots (2)$$ç‰©ç†å­¦ã«ãŠã‘ã‚‹ç­‰é€Ÿç›´ç·šé‹å‹•ã®äºˆæ¸¬å¼ $p(t+\\Delta t) = p(t) + v(t)\\Delta t$ ã¨æ¯”è¼ƒã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«å†…éƒ¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¯” $\\frac{W_v}{W_p}$ ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€é©åŒ–ã•ã‚Œã‚‹ã€Œæœ‰åŠ¹äºˆè¦‹æ™‚é–“ï¼ˆEffective Time Horizonï¼‰$\\Delta t_{eff}$ã€ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ½œåœ¨ç©ºé–“ã«ãŠã„ã¦å°†æ¥ã®ä½ç½®å¤‰åŒ–ã‚’è€ƒæ…®ã—ãŸç‰¹å¾´æŠ½å‡ºãŒè¡Œã‚ã‚Œã‚‹ã€‚3. ç‰©ç†æƒ…å ±ã‚’å†…åŒ…ã—ãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ï¼ˆPIGNNLayerï¼‰ãƒãƒ¼ãƒ‰ $i$ ã¨ãã®éš£æ¥ãƒãƒ¼ãƒ‰ $j \\in \\mathcal{N}_i$ ã®ç›¸äº’ä½œç”¨ã‚’è¨ˆç®—ã™ã‚‹éš›ã€æœ¬æ‰‹æ³•ã§ã¯ä»¥ä¸‹ã®ç‰©ç†ãƒã‚¤ã‚¢ã‚¹ $b_{ij}$ ã‚’å°å…¥ã™ã‚‹ã€‚ã¾ãšã€è¨­è¨ˆè€…ãŒå®šç¾©ã—ãŸæ˜ç¤ºçš„ãªäºˆè¦‹æ™‚é–“ $\\tau$ ã‚’ç”¨ã„ã€å°†æ¥äºˆæ¸¬ä½ç½® $p_{i, pred}, p_{j, pred}$ ã‚’ç®—å‡ºã™ã‚‹ã€‚$$p_{i, pred} = p_i + v_i \\cdot \\tau, \\quad p_{j, pred} = p_j + v_j \\cdot \\tau \\quad \\dots (3)$$ã“ã®äºˆæ¸¬ä½ç½®ã«åŸºã¥ãã€å°†æ¥çš„ãªç©ºé–“çš„è¿‘æ¥æ€§ã‚’ã‚¬ã‚¦ã‚¹ã‚«ãƒ¼ãƒãƒ«ã«æº–ã˜ãŸæŒ‡æ•°é–¢æ•°ã«ã‚ˆã£ã¦ãƒã‚¤ã‚¢ã‚¹å€¤ã¨ã—ã¦å®šå¼åŒ–ã™ã‚‹ã€‚$$b_{ij} = \\exp\\left( - \\frac{\\| p_{i, pred} - p_{j, pred} \\|^2}{\\sigma^2} \\right) \\quad \\dots (4)$$ã“ã“ã§ $\\sigma$ ã¯ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚‹ã€‚æœ€çµ‚çš„ãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ $\\alpha_{ij}$ ã¯ã€ãƒ‡ãƒ¼ã‚¿é§†å‹•å‹ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•° $e_{ij}$ ã«ã“ã®ç‰©ç†ãƒã‚¤ã‚¢ã‚¹ã‚’åŠ ç®—ã—ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã§æ­£è¦åŒ–ã™ã‚‹ã“ã¨ã§å¾—ã‚‰ã‚Œã‚‹ã€‚$$e_{ij} = \\text{LeakyReLU}\\left( \\vec{a}^T [ \\vec{h}_i \\Vert \\vec{h}_j ] \\right) + b_{ij} \\quad \\dots (5)$$$$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})} \\quad \\dots (6)$$4. æ§‹é€ çš„åˆ¶ç´„ã«ã‚ˆã‚‹ç‰©ç†çš„æ•´åˆæ€§ã®æ‹…ä¿æœ¬æ‰‹æ³•ã¯ã€æå¤±é–¢æ•°ã«é …ã‚’è¿½åŠ ã™ã‚‹ã€ŒLosså‹ã€ã¨ã¯ç•°ãªã‚Šã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã®è¨ˆç®—ãƒ—ãƒ­ã‚»ã‚¹è‡ªä½“ã«ç­‰é€Ÿç›´ç·šé‹å‹•ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿è¾¼ã‚“ã§ã„ã‚‹ã€‚å¼(5)ã«ãŠã„ã¦ã€$\\alpha_{ij}$ ã¯ã€Œãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’ã•ã‚ŒãŸç‰¹å¾´ï¼ˆ$\\vec{h}$ï¼‰ã€ã¨ã€Œç‰©ç†çš„ã«äºˆæ¸¬ã•ã‚Œã‚‹è¿‘æ¥æ€§ï¼ˆ$b_{ij}$ï¼‰ã€ã®å’Œã¨ã—ã¦æ±ºå®šã•ã‚Œã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã¯**ã€Œå°†æ¥çš„ã«è¿‘æ¥ã—ã€ç›¸äº’ä½œç”¨ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒé«˜ã„é¸æ‰‹ãƒšã‚¢ã€ã«å¯¾ã—ã¦ã€æ§‹é€ çš„ã«é«˜ã„æ³¨æ„ï¼ˆAttentionï¼‰ã‚’æ‰•ã†ã‚ˆã†æ‹˜æŸã•ã‚Œã‚‹ã€‚** ã“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚‹åˆ¶ç´„ã¯ã€å°è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã„ã¦çµ±è¨ˆçš„ãªãƒã‚¤ã‚ºã¸ã®éå­¦ç¿’ã‚’æŠ‘åˆ¶ã—ã€é‹å‹•å­¦çš„ãªå¿…ç„¶æ€§ã«åŸºã¥ã„ãŸè§£é‡ˆæ€§ã®é«˜ã„ç‰¹å¾´æŠ½å‡ºã‚’å¯èƒ½ã«ã™ã‚‹ã€‚"
      ],
      "metadata": {
        "id": "cMYo9cDf_VWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "â†‘ã¤ã¾ã‚Šã€é¸æ‰‹åŒå£«ã®è¡çªãŒèµ·ãã‚‹å ´åˆã«ãƒ¢ãƒ‡ãƒ«ã«æ³¨ç›®ã•ã›ã‚‹ã“ã¨ã§äºˆæ¸¬ç²¾åº¦ã®å‘ä¸Šã‚’ç‹™ã£ã¦ã„ã‚‹ã¨ã„ã†ç™ºæƒ³ã€‚\n",
        "ã“ã‚Œã¯ã€ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®å¤±æ•—ã‚’è€ƒãˆã‚‹ã¨ã‚ã‹ã‚Šã‚„ã™ã„ãŒã€ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®æˆå¦ã¯é¸æ‰‹é–“ã®å¯†é›†åº¦åˆã„ã«å·¦å³ã•ã‚Œã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã‹ã‚‰ã§ã‚ã‚‹ã€‚\n",
        "æœ¬ã‚¿ã‚¹ã‚¯ã¯ã€ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒœãƒ¼ãƒ«å¥ªå–æ™‚ã‚’ç‰¹å®šã—ã€ãã“ã‹ã‚‰ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å„ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ï¼“ç§’å¾Œã«ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãŒæˆåŠŸã™ã‚‹ã‹ã‚’äºˆæ¸¬ã™ã‚‹â†ã§ã¯é¸æ‰‹åŒå£«ã®è¡çªï¼ãƒœãƒ¼ãƒ«å¥ªå–ã®ç™ºç”Ÿã«æ³¨ç›®ã™ã‚‹å¿…è¦ãªã„ã®ã§ã¯ï¼Ÿã®æ„è¦‹ã«å¯¾ã—ã¦ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«åè«–ã§ãã‚‹\n",
        "ã€Œå¥ªå–ã®ç¬é–“ã«æ³¨ç›®ã—ã¦ã‚‚é–¢ä¿‚ãªã„ã®ã§ã¯ï¼Ÿã€ã¨ã„ã†å•ã„ã«å¯¾ã—ã€ç†è«–ã®ç« ã§ã“ã†å…ˆå›ã‚Šã—ã¦æ›¸ãã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "ã€Œæœ¬ã‚¿ã‚¹ã‚¯ã¯ãƒœãƒ¼ãƒ«å¥ªå–å¾Œã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰å°†æ¥ã®æˆå¦ã‚’äºˆæ¸¬ã™ã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®æˆå¦ã¯ã€å¥ªå–ç›´å¾Œã®é¸æ‰‹é…ç½®ãã®ã‚‚ã®ã‚ˆã‚Šã‚‚ã€ãã®å¾Œã®æ•°ç§’é–“ã«ãŠã‘ã‚‹**ã€ç©ºé–“çš„ãªç«¶åˆã®è§£æ¶ˆã¾ãŸã¯ç™ºç”Ÿã€**ã«ä¾å­˜ã™ã‚‹ã€‚æœ¬ãƒ¢ãƒ‡ãƒ«ã®ç‰©ç†ãƒã‚¤ã‚¢ã‚¹ã¯ã€ç¾æ™‚ç‚¹ã®é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã«åŸºã¥ãã€æ•°ç§’å¾Œã«ç™ºç”Ÿã—å¾—ã‚‹é¸æ‰‹é–“ã®æ¥è¿‘ï¼ˆãƒ‡ãƒ¥ã‚¨ãƒ«ã‚„ã‚¤ãƒ³ã‚¿ãƒ¼ã‚»ãƒ—ãƒˆã®å¯èƒ½æ€§ï¼‰ã‚’ç‰¹å®šã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ”»æ’ƒå´ãŒå®ˆå‚™å´ã®æ¥è¿‘ã‚’å›é¿ã—ã¦ã‚¹ãƒšãƒ¼ã‚¹ã‚’å æœ‰ã§ãã‚‹ã‹ã¨ã„ã†ã€ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®æœ¬è³ªçš„ãªãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã¨ãªã£ã¦ã„ã‚‹ã€‚ã€"
      ],
      "metadata": {
        "id": "hpokaKi6AXcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ã‚·ãƒ¼ãƒ‰å€¤\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    # Pythonè‡ªä½“ã®ä¹±æ•°å›ºå®š\n",
        "    random.seed(seed)\n",
        "    # OSç’°å¢ƒã®ä¹±æ•°å›ºå®š\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    # Numpyã®ä¹±æ•°å›ºå®š\n",
        "    np.random.seed(seed)\n",
        "    # PyTorchã®ä¹±æ•°å›ºå®š\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # ãƒãƒ«ãƒGPUã®å ´åˆ\n",
        "    # è¨ˆç®—ã®æ±ºå®šè«–çš„æŒ™å‹•ã‚’å¼·åˆ¶ï¼ˆã“ã‚Œã‚’å…¥ã‚Œã‚‹ã¨å°‘ã—é…ããªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ãŒã€å†ç¾æ€§ã¯å®Œç’§ã«ãªã‚Šã¾ã™ï¼‰\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# å¥½ããªæ•°å­—ï¼ˆ42ãŒä¸€èˆ¬çš„ï¼‰ã§å›ºå®š\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "QKyaHmiRGtjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcw4b0y1F0PK"
      },
      "outputs": [],
      "source": [
        "#GoogleDriveã‚’ãƒã‚¦ãƒ³ãƒˆ\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Driveã‚’ä»®æƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ãƒã‚¦ãƒ³ãƒˆ\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å¿…é ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "!pip install torch_geometric\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch import optim\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import re\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# è¡¨ç¤ºè¨­å®š\n",
        "np.set_printoptions(suppress=True, precision=3)\n",
        "pd.set_option('display.precision', 3)    # å°æ•°ç‚¹ä»¥ä¸‹ã®è¡¨ç¤ºæ¡\n",
        "pd.set_option('display.max_rows', 50)   # è¡¨ç¤ºã™ã‚‹è¡Œæ•°ã®ä¸Šé™\n",
        "pd.set_option('display.max_columns', 15)  # è¡¨ç¤ºã™ã‚‹åˆ—æ•°ã®ä¸Šé™\n",
        "%precision 3"
      ],
      "metadata": {
        "id": "K7DHFGzCGYlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿ã¯ä»¥å‰ã¾ã§ã¨åŒæ§˜"
      ],
      "metadata": {
        "id": "EpfKnRXp6DxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''import torch\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# ==========================================\n",
        "# ãƒ­ãƒ¼ãƒ‰ãƒ»ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ»æœ€çµ‚ç¢ºèª\n",
        "# ==========================================\n",
        "v7_load_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_v14_final.pt\"\n",
        "\n",
        "try:\n",
        "    print(f\"v7 æœ€çµ‚ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {v7_load_path}\")\n",
        "    # çµ±åˆæ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "    checkpoint = torch.load(v7_load_path, weights_only=False)\n",
        "\n",
        "    # v7 ã® Builder ã§ã™ã§ã« 7æ¬¡å…ƒç‰¹å¾´é‡ (x, y, vx, vy, dist_goal, dist_ball, team_id)\n",
        "    # ã‚’ä»˜ä¸ã—ã¦ã„ã‚‹ãŸã‚ã€åŸºæœ¬çš„ã«ã¯ãã®ã¾ã¾ DataLoader ã«æ¸¡ã›ã¾ã™ã€‚\n",
        "    train_set = checkpoint['train_data']\n",
        "    test_set = checkpoint['test_data']\n",
        "\n",
        "    # DataLoader ã‚’æ§‹ç¯‰ (ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯ãƒ¡ãƒ¢ãƒªã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„)\n",
        "    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
        "\n",
        "    print(f\"--- ãƒ­ãƒ¼ãƒ‰å®Œäº† ---\")\n",
        "    print(f\"è¨“ç·´ã‚»ãƒƒãƒˆ: {len(train_set)} æš\")\n",
        "    print(f\"ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆ: {len(test_set)} æš\")\n",
        "\n",
        "    # ã€æœ€é‡è¦ãƒã‚§ãƒƒã‚¯ã€‘ãƒãƒ¼ãƒ‰æ•°ã¨æ¬¡å…ƒæ•°ã®ç¢ºèª\n",
        "    sample_train = train_set[0]\n",
        "    sample_test = test_set[0]\n",
        "\n",
        "    # æœŸå¾…å€¤: [23, 7] (22äºº + ãƒœãƒ¼ãƒ«1ã¤ã€7ç¨®é¡ã®ç‰¹å¾´é‡)\n",
        "    print(f\"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶: {sample_train.x.shape}\")\n",
        "    print(f\"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶: {sample_test.x.shape}\")\n",
        "\n",
        "    # 1. æ¬¡å…ƒæ•°ãƒã‚§ãƒƒã‚¯\n",
        "    if sample_train.x.shape[1] == 7:\n",
        "        print(\"âœ… æ¬¡å…ƒæ•°: OK (7æ¬¡å…ƒ)\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ æ¬¡å…ƒæ•°è­¦å‘Š: {sample_train.x.shape[1]}æ¬¡å…ƒã«ãªã£ã¦ã„ã¾ã™ã€‚\")\n",
        "\n",
        "    # 2. ãƒãƒ¼ãƒ‰æ•°ãƒã‚§ãƒƒã‚¯\n",
        "    if sample_train.x.shape[0] == 23:\n",
        "        print(\"âœ… ãƒãƒ¼ãƒ‰æ•°: OK (23ãƒãƒ¼ãƒ‰å›ºå®š)\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ ãƒãƒ¼ãƒ‰æ•°è­¦å‘Š: {sample_train.x.shape[0]}ãƒãƒ¼ãƒ‰ã«ãªã£ã¦ã„ã¾ã™ã€‚\")\n",
        "\n",
        "    # 3. é€Ÿåº¦ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯\n",
        "    # 2åˆ—ç›®(vx)ã®çµ¶å¯¾å€¤å¹³å‡ãŒ0ã§ãªã‘ã‚Œã°ã€é€Ÿåº¦ãŒæ­£ã—ãå…¥ã£ã¦ã„ã¾ã™\n",
        "    v_mean = torch.abs(sample_train.x[:, 2]).mean().item()\n",
        "    if v_mean > 0.01:\n",
        "        print(f\"âœ… ç‰©ç†é‡ãƒã‚§ãƒƒã‚¯: OK (å¹³å‡é€Ÿåº¦å±æ€§ã‚’ç¢ºèª)\")\n",
        "    else:\n",
        "        print(\"âŒ ç‰©ç†é‡è­¦å‘Š: é€Ÿåº¦ãŒ0ã«å¼µã‚Šä»˜ã„ã¦ã„ã¾ã™ã€‚Builderã‚’å†ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "    if sample_train.x.shape[1] == 7 and v_mean > 0.01:\n",
        "        print(\"\\nğŸš€ ã™ã¹ã¦ã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚PIGNN å­¦ç¿’ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {v7_load_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}\")'''"
      ],
      "metadata": {
        "id": "XjQznJNPUTHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# ==========================================\n",
        "# 1. è£œåŠ©é–¢æ•°: è©¦åˆå˜ä½ã®ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
        "# ==========================================\n",
        "def balance_dataset_by_undersampling(data_list):\n",
        "    \"\"\"\n",
        "    ã‚·ãƒ¼ã‚¯ã‚¨ãƒ³ã‚¹å˜ä½ã§1:1ã«èª¿æ•´ã€‚è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ã®ã¿é©ç”¨ã€‚\n",
        "    \"\"\"\n",
        "    seq_groups = defaultdict(list)\n",
        "    for d in data_list:\n",
        "        sid = int(d.sequence_id.item()) if torch.is_tensor(d.sequence_id) else int(d.sequence_id)\n",
        "        seq_groups[sid].append(d)\n",
        "\n",
        "    l0_groups, l1_groups = [], []\n",
        "    for sid, frames in seq_groups.items():\n",
        "        label = int(frames[0].y.item())\n",
        "        if label == 0: l0_groups.append(frames)\n",
        "        else: l1_groups.append(frames)\n",
        "\n",
        "    # å°‘ãªã„æ–¹ï¼ˆæˆåŠŸï¼‰ã«åˆã‚ã›ã¦å¤±æ•—ã‚’å‰Šã‚‹\n",
        "    min_size = min(len(l0_groups), len(l1_groups))\n",
        "    sampled_l0 = random.sample(l0_groups, min_size)\n",
        "    sampled_l1 = l1_groups # æˆåŠŸã¯å…¨æ•°ä¿æŒ\n",
        "\n",
        "    balanced_list = [frame for group in (sampled_l0 + sampled_l1) for frame in group]\n",
        "    random.shuffle(balanced_list)\n",
        "\n",
        "    print(f\"    [Sampling] Success Seqs: {len(sampled_l1)} | Failure Seqs: {len(sampled_l0)} | Total Frames: {len(balanced_list)}\")\n",
        "    return balanced_list\n",
        "\n"
      ],
      "metadata": {
        "id": "Rc2Wl21OBF67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©ï¼ˆPIGNNã®ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚¯ãƒ©ã‚¹ï¼‰"
      ],
      "metadata": {
        "id": "ocDgeAXM6Ifi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_geometric.utils import softmax\n",
        "from torch_geometric.data import Data\n",
        "import os\n",
        "\n",
        "# ==========================================\n",
        "# 1. å‰å‡¦ç†é–¢æ•°ã®å®šç¾©\n",
        "# ==========================================\n",
        "def preprocess_batch(data, device):\n",
        "    # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¯Builderå´ã§è¡Œã‚ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã“ã“ã§ã¯å‹å¤‰æ›ã¨Deviceè»¢é€ã«é›†ä¸­\n",
        "    data.x = data.x.float()\n",
        "    data.pos = data.pos.float()\n",
        "    data.vel = data.vel.float()\n",
        "    return data.to(device)\n",
        "\n",
        "# ==========================================\n",
        "# 2. ãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆãƒãƒ¼ãƒ å±æ€§ã«ã‚ˆã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åˆ†å²ã®å®Ÿè£…ï¼‰\n",
        "# ==========================================\n",
        "class PIGNNLayer(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, tau=1.5):\n",
        "        super(PIGNNLayer, self).__init__(aggr='add')\n",
        "        self.tau = tau\n",
        "        self.lin = nn.Linear(in_channels, out_channels)\n",
        "        self.att = nn.Parameter(torch.Tensor(1, out_channels * 2))\n",
        "        nn.init.xavier_uniform_(self.att)\n",
        "\n",
        "    def forward(self, x, edge_index, pos, vel):\n",
        "        h = self.lin(x)\n",
        "        # ãƒãƒ¼ãƒ ID(index 6)ã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã«æ¸¡ã™\n",
        "        return self.propagate(edge_index, x=h, pos=pos, vel=vel, team=x[:, 6:7])\n",
        "\n",
        "    def message(self, x_i, x_j, pos_i, pos_j, vel_i, vel_j, edge_index_i, team_i, team_j):\n",
        "        # ç†è«–1: æœªæ¥ä½ç½®äºˆæ¸¬ãƒ™ãƒ¼ã‚¹ã®ãƒã‚¤ã‚¢ã‚¹\n",
        "        pos_i_pred = pos_i + vel_i * self.tau\n",
        "        pos_j_pred = pos_j + vel_j * self.tau\n",
        "        dist_future = torch.norm(pos_i_pred - pos_j_pred, dim=-1, keepdim=True)\n",
        "        physics_bias = torch.exp(-dist_future / 2.0)\n",
        "\n",
        "        # ã€è¿½åŠ ã€‘ãƒãƒ¼ãƒ é–¢ä¿‚åˆ†å²: å‘³æ–¹ãªã‚‰+0.5, æ•µãªã‚‰-0.5ã®æ³¨ç›®åº¦è£œæ­£\n",
        "        is_teammate = (team_i == team_j).float()\n",
        "        team_bias = torch.where(is_teammate > 0.5, 0.5, -0.5)\n",
        "\n",
        "        alpha = torch.cat([x_i, x_j], dim=-1)\n",
        "        alpha = (alpha * self.att).sum(dim=-1, keepdim=True)\n",
        "        # ç‰©ç†ãƒã‚¤ã‚¢ã‚¹ + ãƒãƒ¼ãƒ å±æ€§ãƒã‚¤ã‚¢ã‚¹ ã‚’çµ±åˆ\n",
        "        alpha = softmax(F.leaky_relu(alpha) + physics_bias + team_bias, edge_index_i)\n",
        "\n",
        "        return alpha * x_j\n",
        "\n",
        "class PIGNNClassifier(nn.Module):\n",
        "    def __init__(self, hidden_channels=64):\n",
        "        super(PIGNNClassifier, self).__init__()\n",
        "        self.conv1 = PIGNNLayer(7, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        x = F.elu(self.conv1(x, edge_index, pos, vel))\n",
        "        x = self.conv2(x, edge_index, pos, vel)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return F.log_softmax(self.lin(x), dim=1)\n",
        "\n",
        "# ==========================================\n",
        "# 3. ç†è«–ä¿®æ­£ï¼šé›†å›£é‹å‹•å­¦çš„åˆ¶ç´„ï¼ˆL_physï¼‰\n",
        "# ==========================================\n",
        "def pignn_theoretical_loss(output, target, data, alpha=0.1):\n",
        "    # L_task: ã‚¯ãƒ©ã‚¹é‡ã¿ä»˜ãï¼ˆSuccess:3.3å€ï¼‰\n",
        "    weights = torch.tensor([1.0, 3.3], device=output.device)\n",
        "    loss_task = F.nll_loss(output, target, weight=weights)\n",
        "\n",
        "    # L_phys: é›†å›£æ¨é€²åŠ›åˆ¶ç´„\n",
        "    # Successç¢ºç‡ãŒé«˜ã„ã»ã©ã€ãƒãƒ¼ãƒ å…¨ä½“ã®é‡å¿ƒ(å…¨ãƒãƒ¼ãƒ‰å¹³å‡)ãŒå³(+vx)ã§ã‚ã‚‹ã“ã¨ã‚’æ±‚ã‚ã‚‹\n",
        "    probs = torch.exp(output)[:, 1]\n",
        "\n",
        "    # ã‚°ãƒ©ãƒ•ã”ã¨ã®å¹³å‡vxã‚’ç®—å‡ºï¼ˆé¸æ‰‹å…¨å“¡ã®å‹•ãã‚’çµ±åˆï¼‰\n",
        "    # data.batch ã‚’ç”¨ã„ã¦å„ã‚°ãƒ©ãƒ•(ã‚·ãƒ¼ãƒ³)ã®å¹³å‡vxã‚’è¨ˆç®—\n",
        "    batch_size = output.size(0)\n",
        "    # å„ã‚°ãƒ©ãƒ•ã®å¹³å‡vxã‚’è¨ˆç®—\n",
        "    avg_vxs = []\n",
        "    for i in range(batch_size):\n",
        "        mask = (data.batch == i)\n",
        "        avg_vxs.append(torch.mean(data.vel[mask, 0])) # å„ã‚·ãƒ¼ãƒ³ã®å…¨ãƒãƒ¼ãƒ‰å¹³å‡vx\n",
        "\n",
        "    avg_vxs = torch.stack(avg_vxs)\n",
        "\n",
        "    # ç‰©ç†æå¤±ï¼šSuccessç¢ºç‡ Ã— ReLU(-å¹³å‡vx)\n",
        "    # ã‚·ãƒ¼ãƒ³å…¨ä½“ãŒå·¦ã«æµã‚Œã¦ã„ã‚‹ã®ã«ã€ŒæˆåŠŸã€ã¨å‡ºã™ã¨å¼·ãç½°ã›ã‚‰ã‚Œã‚‹\n",
        "    loss_phys = torch.mean(probs * torch.relu(-avg_vxs))\n",
        "\n",
        "    total_loss = loss_task + (alpha * loss_phys)\n",
        "    return total_loss, loss_task, loss_phys\n",
        "\n",
        "# ==========================================\n",
        "# 4. å­¦ç¿’ãƒ»è©•ä¾¡ãƒ«ãƒ¼ãƒ—\n",
        "# ==========================================\n",
        "'''\n",
        "def train_pignn_epoch_dynamic(model, loader, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    total_loss, total_phys = 0, 0\n",
        "\n",
        "    # ã‚¢ãƒ‹ãƒ¼ãƒªãƒ³ã‚°ï¼ˆå¾ŒåŠ10ã‚¨ãƒãƒƒã‚¯ä»¥é™ã§ç‰©ç†ã‚’å¼·åŒ–ï¼‰\n",
        "    if epoch <= 10:\n",
        "        current_alpha = 0.1\n",
        "    else:\n",
        "        current_alpha = min(0.1 + (epoch - 10) * 0.2, 3.0)\n",
        "\n",
        "    for data in loader:\n",
        "        data = preprocess_batch(data, device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(data)\n",
        "        loss, _, l_phys = pignn_theoretical_loss(out, data.y.view(-1), data, alpha=current_alpha)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "        total_phys += l_phys.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(loader.dataset), total_phys / len(loader.dataset), current_alpha'''\n",
        "\n",
        "def train_pignn_epoch_fixed(model, loader, optimizer, device, alpha_p):\n",
        "    \"\"\"\n",
        "    è«–æ–‡ã®å®Ÿé¨“(Aæ¡ˆ)ã‚’å†ç¾ã™ã‚‹ãŸã‚ã®ã€alphaå›ºå®šå­¦ç¿’ãƒ«ãƒ¼ãƒ—ã€‚\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss, total_phys = 0, 0\n",
        "\n",
        "    # å‹•çš„ãªå¤‰æ›´ã‚’å»ƒæ­¢ã—ã€å¼•æ•°ã§å—ã‘å–ã£ãŸå›ºå®šå€¤ã‚’ä½¿ç”¨\n",
        "    current_alpha = alpha_p\n",
        "\n",
        "    for data in loader:\n",
        "        data = preprocess_batch(data, device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(data)\n",
        "        # ä¿®æ­£ã•ã‚ŒãŸalphaã‚’æå¤±é–¢æ•°ã«æ¸¡ã™\n",
        "        loss, _, l_phys = pignn_theoretical_loss(out, data.y.view(-1), data, alpha=current_alpha)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "        total_phys += l_phys.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(loader.dataset), total_phys / len(loader.dataset), current_alpha\n",
        "\n",
        "def test_pignn(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = preprocess_batch(data, device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == data.y.view(-1)).sum().item()\n",
        "    return correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "hIyIVn-LHCPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç‰©ç†æå¤±ã‚’å®Ÿè£…ã€‚ä¸€ç•ªä¸Šã«æ›¸ã„ãŸç†è«–ã®å¼ã¨ã¯é•ã†ã€‚ä»Šå›ã¯ã‚¿ã‚¹ã‚¯ãŒåº§æ¨™äºˆæ¸¬ã§ã¯ãªãã€åˆ†é¡å•é¡Œãªã®ã§ã€‚\n",
        "ææ¡ˆæ‰‹æ³•ï¼šInteractive-Physics-Informed GNN (I-PIGNN)æœ¬ç ”ç©¶ã§ã¯ã€æˆåŠŸäºˆæ¸¬ã¨ã„ã†äºŒå€¤åˆ†é¡ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã€é‹å‹•å­¦çš„ãªæ•´åˆæ€§ã‚’å¼·åˆ¶ã™ã‚‹ãŸã‚ã®æå¤±é–¢æ•°ã‚’ææ¡ˆã™ã‚‹ã€‚3.1 ç‰©ç†æƒ…å ±ã«åŸºã¥ã„ãŸæå¤±é–¢æ•°ã®å†å®šç¾©å¾“æ¥ã® PIGNN ã§ã¯ã€æœªæ¥ã®åº§æ¨™äºˆæ¸¬å€¤ $\\hat{\\mathbf{p}}_{t+\\Delta t}$ ã¨é‹å‹•æ–¹ç¨‹å¼ã«ã‚ˆã‚‹äºˆæ¸¬å€¤ã®èª¤å·®ã‚’æœ€å°åŒ–ã™ã‚‹æ‰‹æ³•ãŒä¸€èˆ¬çš„ã§ã‚ã‚‹ã€‚ã—ã‹ã—ã€æœ¬ç ”ç©¶ã®ç›®çš„ã¯æˆ¦è¡“çš„æˆåŠŸã®åˆ¤å®šï¼ˆåˆ†é¡ï¼‰ã§ã‚ã‚Šã€åº§æ¨™ã®å›å¸°ã§ã¯ãªã„ã€‚ãã“ã§ã€ç†è«–ä¸Šã®ç‰©ç†çš„åˆ¶ç´„ã‚’ã€ŒæˆåŠŸåˆ¤å®šã®æ ¹æ‹ ã«å¯¾ã™ã‚‹æ•´åˆæ€§ã€ã¨ã—ã¦å†å®šç¾©ã™ã‚‹ã€‚å…¨ä½“ã®æå¤±é–¢æ•° $\\mathcal{L}_{total}$ ã¯ã€ã‚¿ã‚¹ã‚¯æå¤± $\\mathcal{L}_{task}$ ã¨ç‰©ç†æå¤± $\\mathcal{L}_{phys}$ ã®åŠ é‡å’Œã§å®šç¾©ã•ã‚Œã‚‹ã€‚$$\\mathcal{L}_{total} = \\mathcal{L}_{task} + \\alpha(e) \\cdot \\mathcal{L}_{phys}$$ã“ã“ã§ã€$\\alpha(e)$ ã¯å­¦ç¿’ã‚¨ãƒãƒƒã‚¯æ•° $e$ ã«å¿œã˜ã¦ç‰©ç†åˆ¶ç´„ã®å¼·åº¦ã‚’èª¿æ•´ã™ã‚‹ã‚¢ãƒ‹ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ã§ã‚ã‚‹ã€‚3.2 æ¨é€²æ–¹å‘æ•´åˆæ€§åˆ¶ç´„ï¼ˆDirectional Consistency Constraintï¼‰ç‰©ç†æå¤± $\\mathcal{L}_{phys}$ ã®å…·ä½“çš„ãªå®šç¾©ã‚’ä»¥ä¸‹ã«ç¤ºã™ã€‚æœ¬ç ”ç©¶ã§ã¯ã€æ”»æ’ƒæ–¹å‘ã‚’ãƒ”ãƒƒãƒã®æ­£ã® $x$ è»¸æ–¹å‘ï¼ˆå³æ–¹å‘ï¼‰ã«çµ±ä¸€ã—ã¦ã„ã‚‹ã€‚ã—ãŸãŒã£ã¦ã€ç‰©ç†çš„ã«ã€ŒæˆåŠŸï¼ˆSuccessï¼‰ã€ã¨ã¿ãªã•ã‚Œã‚‹ãƒ—ãƒ¬ãƒ¼ã¯ã€ãƒœãƒ¼ãƒ«ãŠã‚ˆã³é¸æ‰‹ãŒæ­£ã®é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŒã¤ã¨ã„ã†é‹å‹•å­¦çš„æ€§è³ªã‚’æŒã¤ã¹ãã§ã‚ã‚‹ã€‚ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã™ã‚‹æˆåŠŸç¢ºç‡ï¼ˆSoftmaxå‡ºåŠ›ï¼‰ã‚’ $y_{pred} \\in [0, 1]$ã€ãƒœãƒ¼ãƒ«ã® $x$ è»¸æ–¹å‘ã®é€Ÿåº¦ã‚’ $v_x$ ã¨ã—ãŸã¨ãã€ç‰©ç†æå¤±ã¯æ¬¡å¼ã§ä¸ãˆã‚‰ã‚Œã‚‹ã€‚$$\\mathcal{L}_{phys} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( y_{pred, i} \\times \\text{ReLU}(-v_{x, i}) \\right)$$æ•°å¼ã®ç‰©ç†çš„æ„å‘³$\\text{ReLU}(-v_x)$ é …: é€²è¡Œæ–¹å‘ãŒæ­£ï¼ˆ$v_x > 0$ï¼‰ã§ã‚ã‚‹å ´åˆã¯ $0$ ã¨ãªã‚Šã€é€†èµ°ï¼ˆ$v_x < 0$ï¼‰ã—ã¦ã„ã‚‹å ´åˆã«ã®ã¿ã€ãã®å¤§ãã•ã«æ¯”ä¾‹ã—ãŸãƒšãƒŠãƒ«ãƒ†ã‚£ãŒç™ºç”Ÿã™ã‚‹ã€‚$y_{pred}$ ã¨ã®ç›¸äº’ä½œç”¨: å˜ã«é€Ÿåº¦ã‚’ç¸›ã‚‹ã®ã§ã¯ãªãã€**ã€ŒAIãŒæˆåŠŸã§ã‚ã‚‹ã¨å¼·ãç¢ºä¿¡ã—ã¦ã„ã‚‹ï¼ˆ$y_{pred}$ ãŒå¤§ãã„ï¼‰ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€ç‰©ç†çš„ã«ã¯é€†èµ°ã—ã¦ã„ã‚‹ã€**ã¨ã„ã†çŸ›ç›¾ã—ãŸçŠ¶æ…‹ã«å¯¾ã—ã¦ã€æŒ‡æ•°é–¢æ•°çš„ã«å¤§ããªæå¤±ã‚’ä¸ãˆã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å‹¾é…é™ä¸‹æ³•ã‚’é€šã˜ã¦ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ $\\theta$ ã¯ã€æ­£ã®æ¨é€²åŠ›ã‚’æŒã¤ç‰¹å¾´é‡ã‚’ã€ŒæˆåŠŸã€ã®æ ¹æ‹ ã¨ã—ã¦å„ªå…ˆçš„ã«æŠ½å‡ºã™ã‚‹ã‚ˆã†ã«æœ€é©åŒ–ã•ã‚Œã‚‹ã€‚"
      ],
      "metadata": {
        "id": "CR7_wM016WuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#å®Ÿè¡Œ\n",
        "'''\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚ãªãŸã®è¨­å®šã‚’ç¶­æŒï¼‰\n",
        "EPOCHS = 50\n",
        "LR = 0.0005\n",
        "\n",
        "history = {\n",
        "    'total_loss': [],\n",
        "    'physics_loss': [],\n",
        "    'test_acc': [],\n",
        "    'alpha': []\n",
        "}\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã¨æœ€é©åŒ–æ‰‹æ³•ã®åˆæœŸåŒ–\n",
        "model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# ãƒ•ã‚¡ã‚¤ãƒ«å\n",
        "save_path = 'best_pignn_theoretical_V3.pth'\n",
        "\n",
        "print(f\"PIGNNå­¦ç¿’é–‹å§‹ (Device: {device} | ç‰©ç†ãƒ»åˆ†é¡ ç†è«–çµ±åˆãƒ¢ãƒ¼ãƒ‰)\")\n",
        "print(f\"Input Features: 7 [x, y, vx, vy, (1-px), dist_ball, team_id]\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # å…ˆã»ã©ä¿®æ­£ã—ãŸã€å‹¾é…ãŒç¹‹ãŒã£ãŸ train_pignn_epoch_dynamic ã‚’å‘¼ã³å‡ºã—\n",
        "    avg_loss, avg_phys, current_alpha = train_pignn_epoch_dynamic(model, train_loader, optimizer, device, epoch)\n",
        "\n",
        "    # ãƒ†ã‚¹ãƒˆè©•ä¾¡\n",
        "    acc = test_pignn(model, test_loader, device)\n",
        "\n",
        "    # å±¥æ­´ã®ä¿å­˜\n",
        "    history['total_loss'].append(avg_loss)\n",
        "    history['physics_loss'].append(avg_phys)\n",
        "    history['test_acc'].append(acc)\n",
        "    history['alpha'].append(current_alpha)\n",
        "\n",
        "    # é€²æ—è¡¨ç¤ºï¼šPhys_L ãŒ 0.7439 ã‹ã‚‰å¤‰åŒ–ã—ã¦ã„ã‚‹ã‹æ³¨ç›®ã—ã¦ãã ã•ã„\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        # ç†è«–ã«åŸºã¥ã„ãŸ Phys_L ã®æŒ™å‹•ã‚’ç¢ºèªã—ã‚„ã™ãè¡¨ç¤º\n",
        "        print(f\"Epoch {epoch:03d} | Alpha: {current_alpha:.2f} | Loss: {avg_loss:.4f} | Phys_L (Penalty): {avg_phys:.6f} | Acc: {acc:.4f}\")\n",
        "\n",
        "    # ä¿å­˜ãƒ­ã‚¸ãƒƒã‚¯ï¼šç²¾åº¦ãŒå‘ä¸Šã—ãŸã¨ãã®ã¿ä¿å­˜\n",
        "    if epoch == 1 or acc > max(history['test_acc'][:-1]):\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\" >> [Update] Model Saved: {save_path} (Best Acc: {acc:.4f})\")\n",
        "\n",
        "print(f\"\\nå­¦ç¿’å®Œäº†ã€‚æœ€é«˜ç²¾åº¦: {max(history['test_acc']):.4f}\")'''"
      ],
      "metadata": {
        "id": "XvcuJczK-BWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³ (CVãƒ«ãƒ¼ãƒ—) - å¹½éœŠãƒ‡ãƒ¼ã‚¿æ’é™¤ç‰ˆ\n",
        "# ==========================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# è«–æ–‡åŒæ§˜ã€å›ºå®šå€¤ã§è©•ä¾¡ï¼ˆ0, 1.0, 10.0ãªã©ï¼‰\n",
        "FIXED_ALPHA = 1.0\n",
        "\n",
        "v16_load_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_v16_final.pt\"\n",
        "print(f\"CVç”¨ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {v16_load_path}\")\n",
        "checkpoint = torch.load(v16_load_path, weights_only=False)\n",
        "all_data_list = checkpoint['all_data']\n",
        "\n",
        "# --- ã€ä¿®æ­£ã€‘æ¨æ¸¬ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã€åˆ»å°ã•ã‚ŒãŸIDã‚’ç›´æ¥å–å¾— ---\n",
        "# æ—¢ã«ä¿å­˜å´ã§å…¨ãƒ‡ãƒ¼ã‚¿ã« match_id ãŒä»˜ä¸ã•ã‚Œã¦ã„ã‚‹å‰æã§ã™\n",
        "match_ids = sorted(list(set([int(d.match_id.item()) for d in all_data_list])))\n",
        "print(f\"æ¤œå‡ºã•ã‚ŒãŸè©¦åˆID: {match_ids} (è¨ˆ {len(match_ids)} è©¦åˆ)\")\n",
        "\n",
        "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "EPOCHS_CV = 30\n",
        "LR = 0.0005\n",
        "cv_final_reports = []\n",
        "best_overall_f1 = 0\n",
        "\n",
        "print(f\"PIGNN ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ (Alpha={FIXED_ALPHA} å›ºå®šãƒ¢ãƒ¼ãƒ‰)\\n\")\n",
        "\n",
        "for test_match in match_ids:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\" ğŸŒ€ Round: Match {test_match} ã‚’ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # 1. åˆ»å°ã•ã‚ŒãŸ match_id ã‚’ä¿¡ã˜ã¦åˆ‡ã‚Šåˆ†ã‘\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match]\n",
        "\n",
        "    # 2. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿1:1ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
        "    # â€»balance_dataset_by_undersampling é–¢æ•°ã¯ä»¥å‰ã®ã‚‚ã®ã‚’ãã®ã¾ã¾ä½¿ç”¨\n",
        "    cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "\n",
        "    cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "    cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    cv_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    cv_optimizer = torch.optim.Adam(cv_model.parameters(), lr=LR)\n",
        "\n",
        "    # 3. å­¦ç¿’ãƒ«ãƒ¼ãƒ— (å›ºå®šAlphaç‰ˆ)\n",
        "    for epoch in range(1, EPOCHS_CV + 1):\n",
        "        # ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ã‚’é¿ã‘ã‚‹ãŸã‚æˆ»ã‚Šå€¤3ã¤ã‚’æ­£ã—ãå—ã‘å–ã‚‹\n",
        "        loss, phys, _ = train_pignn_epoch_fixed(cv_model, cv_train_loader, cv_optimizer, device, FIXED_ALPHA)\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f\"  Epoch {epoch:02d} | Loss: {loss:.4f} | Phys_L: {phys:.6f}\")\n",
        "\n",
        "    # 4. è©•ä¾¡\n",
        "    cv_model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in cv_test_loader:\n",
        "            data = data.to(device)\n",
        "            out = cv_model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    # ã‚¹ã‚³ã‚¢é›†è¨ˆ\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    current_f1 = report['1']['f1-score']\n",
        "\n",
        "    cv_final_reports.append({\n",
        "        'match': test_match,\n",
        "        'recall': report['1']['recall'],\n",
        "        'precision': report['1']['precision'],\n",
        "        'f1': current_f1\n",
        "    })\n",
        "\n",
        "    # --- Î±åˆ¥ã«ãƒ•ã‚©ãƒ«ãƒ€ã‚’åˆ†ã‘ã¦æ•´ç†ä¿å­˜ ---\n",
        "    import os\n",
        "    # ä¿å­˜ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
        "    base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "    # Î±ã”ã¨ã®å°‚ç”¨ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ\n",
        "    alpha_dir = os.path.join(base_model_dir, f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}\")\n",
        "    os.makedirs(alpha_dir, exist_ok=True)\n",
        "\n",
        "    # 1. å„è©¦åˆ(Round)ã”ã¨ã®é‡ã¿ã‚’ä¿å­˜\n",
        "    model_filename = f'pignn_testmatch_{test_match}.pth'\n",
        "    model_path = os.path.join(alpha_dir, model_filename)\n",
        "    torch.save(cv_model.state_dict(), model_path)\n",
        "    print(f\" >> [Alpha {FIXED_ALPHA}] Match {test_match} weight saved.\")\n",
        "\n",
        "    # 2. ãã®Î±ã«ãŠã‘ã‚‹ã€Œæœ€é«˜å‚‘ä½œã€ã‚’ä¿å­˜\n",
        "    if current_f1 > best_overall_f1:\n",
        "        best_overall_f1 = current_f1\n",
        "        best_model_path = os.path.join(alpha_dir, f'best_overall_alpha_{FIXED_ALPHA}.pth')\n",
        "        torch.save(cv_model.state_dict(), best_model_path)\n",
        "        print(f\" âœ¨ [Alpha {FIXED_ALPHA}] New Best Model Saved: F1={best_overall_f1:.4f}\")\n",
        "\n",
        "    if current_f1 > best_overall_f1:\n",
        "        best_overall_f1 = current_f1\n",
        "        torch.save(cv_model.state_dict(), f'best_pignn_alpha_{FIXED_ALPHA}.pth')\n",
        "\n",
        "    print(f\" >> Result: Recall={report['1']['recall']:.4f}, Precision={report['1']['precision']:.4f}, F1={current_f1:.4f}\")\n",
        "\n",
        "# 5. æœ€çµ‚é›†è¨ˆ\n",
        "print(f\"\\n\\n{'#'*60}\\n ğŸ† Alpha={FIXED_ALPHA} CV æœ€çµ‚å¹³å‡çµæœ\\n{'#'*60}\")\n",
        "avg_recall = np.mean([r['recall'] for r in cv_final_reports])\n",
        "avg_f1 = np.mean([r['f1'] for r in cv_final_reports])\n",
        "avg_precision = np.mean([r['precision'] for r in cv_final_reports])\n",
        "\n",
        "print(f\"\\n[OVERALL] Avg Success Recall:    {avg_recall:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success Precision: {avg_precision:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success F1-score:  {avg_f1:.4f}\")"
      ],
      "metadata": {
        "id": "0uKC0kgrBrjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³ (CVãƒ«ãƒ¼ãƒ—) - å¹½éœŠãƒ‡ãƒ¼ã‚¿æ’é™¤ç‰ˆ\n",
        "# ==========================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# è«–æ–‡åŒæ§˜ã€å›ºå®šå€¤ã§è©•ä¾¡ï¼ˆ0, 1.0, 10.0ãªã©ï¼‰\n",
        "FIXED_ALPHA = 0\n",
        "\n",
        "v16_load_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_v16_final.pt\"\n",
        "print(f\"CVç”¨ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {v16_load_path}\")\n",
        "checkpoint = torch.load(v16_load_path, weights_only=False)\n",
        "all_data_list = checkpoint['all_data']\n",
        "\n",
        "# --- ã€ä¿®æ­£ã€‘æ¨æ¸¬ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã€åˆ»å°ã•ã‚ŒãŸIDã‚’ç›´æ¥å–å¾— ---\n",
        "# æ—¢ã«ä¿å­˜å´ã§å…¨ãƒ‡ãƒ¼ã‚¿ã« match_id ãŒä»˜ä¸ã•ã‚Œã¦ã„ã‚‹å‰æã§ã™\n",
        "match_ids = sorted(list(set([int(d.match_id.item()) for d in all_data_list])))\n",
        "print(f\"æ¤œå‡ºã•ã‚ŒãŸè©¦åˆID: {match_ids} (è¨ˆ {len(match_ids)} è©¦åˆ)\")\n",
        "\n",
        "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
        "EPOCHS_CV = 30\n",
        "LR = 0.0005\n",
        "cv_final_reports = []\n",
        "best_overall_f1 = 0\n",
        "\n",
        "print(f\"PIGNN ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹ (Alpha={FIXED_ALPHA} å›ºå®šãƒ¢ãƒ¼ãƒ‰)\\n\")\n",
        "\n",
        "for test_match in match_ids:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\" ğŸŒ€ Round: Match {test_match} ã‚’ãƒ†ã‚¹ãƒˆã«ä½¿ç”¨\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # 1. åˆ»å°ã•ã‚ŒãŸ match_id ã‚’ä¿¡ã˜ã¦åˆ‡ã‚Šåˆ†ã‘\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match]\n",
        "\n",
        "    # 2. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿1:1ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
        "    # â€»balance_dataset_by_undersampling é–¢æ•°ã¯ä»¥å‰ã®ã‚‚ã®ã‚’ãã®ã¾ã¾ä½¿ç”¨\n",
        "    cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "\n",
        "    cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "    cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    cv_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    cv_optimizer = torch.optim.Adam(cv_model.parameters(), lr=LR)\n",
        "\n",
        "    # 3. å­¦ç¿’ãƒ«ãƒ¼ãƒ— (å›ºå®šAlphaç‰ˆ)\n",
        "    for epoch in range(1, EPOCHS_CV + 1):\n",
        "        # ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ã‚’é¿ã‘ã‚‹ãŸã‚æˆ»ã‚Šå€¤3ã¤ã‚’æ­£ã—ãå—ã‘å–ã‚‹\n",
        "        loss, phys, _ = train_pignn_epoch_fixed(cv_model, cv_train_loader, cv_optimizer, device, FIXED_ALPHA)\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f\"  Epoch {epoch:02d} | Loss: {loss:.4f} | Phys_L: {phys:.6f}\")\n",
        "\n",
        "    # 4. è©•ä¾¡\n",
        "    cv_model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in cv_test_loader:\n",
        "            data = data.to(device)\n",
        "            out = cv_model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    # ã‚¹ã‚³ã‚¢é›†è¨ˆ\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    current_f1 = report['1']['f1-score']\n",
        "\n",
        "    cv_final_reports.append({\n",
        "        'match': test_match,\n",
        "        'recall': report['1']['recall'],\n",
        "        'precision': report['1']['precision'],\n",
        "        'f1': current_f1\n",
        "    })\n",
        "\n",
        "    # --- Î±åˆ¥ã«ãƒ•ã‚©ãƒ«ãƒ€ã‚’åˆ†ã‘ã¦æ•´ç†ä¿å­˜ ---\n",
        "    import os\n",
        "    # ä¿å­˜ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
        "    base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "    # Î±ã”ã¨ã®å°‚ç”¨ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆ\n",
        "    alpha_dir = os.path.join(base_model_dir, f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}\")\n",
        "    os.makedirs(alpha_dir, exist_ok=True)\n",
        "\n",
        "    # 1. å„è©¦åˆ(Round)ã”ã¨ã®é‡ã¿ã‚’ä¿å­˜\n",
        "    model_filename = f'pignn_testmatch_{test_match}.pth'\n",
        "    model_path = os.path.join(alpha_dir, model_filename)\n",
        "    torch.save(cv_model.state_dict(), model_path)\n",
        "    print(f\" >> [Alpha {FIXED_ALPHA}] Match {test_match} weight saved.\")\n",
        "\n",
        "    # 2. ãã®Î±ã«ãŠã‘ã‚‹ã€Œæœ€é«˜å‚‘ä½œã€ã‚’ä¿å­˜\n",
        "    if current_f1 > best_overall_f1:\n",
        "        best_overall_f1 = current_f1\n",
        "        best_model_path = os.path.join(alpha_dir, f'best_overall_alpha_{FIXED_ALPHA}.pth')\n",
        "        torch.save(cv_model.state_dict(), best_model_path)\n",
        "        print(f\" âœ¨ [Alpha {FIXED_ALPHA}] New Best Model Saved: F1={best_overall_f1:.4f}\")\n",
        "\n",
        "    if current_f1 > best_overall_f1:\n",
        "        best_overall_f1 = current_f1\n",
        "        torch.save(cv_model.state_dict(), f'best_pignn_alpha_{FIXED_ALPHA}.pth')\n",
        "\n",
        "    print(f\" >> Result: Recall={report['1']['recall']:.4f}, Precision={report['1']['precision']:.4f}, F1={current_f1:.4f}\")\n",
        "\n",
        "# 5. æœ€çµ‚é›†è¨ˆ\n",
        "print(f\"\\n\\n{'#'*60}\\n ğŸ† Alpha={FIXED_ALPHA} CV æœ€çµ‚å¹³å‡çµæœ\\n{'#'*60}\")\n",
        "avg_recall = np.mean([r['recall'] for r in cv_final_reports])\n",
        "avg_f1 = np.mean([r['f1'] for r in cv_final_reports])\n",
        "avg_precision = np.mean([r['precision'] for r in cv_final_reports])\n",
        "\n",
        "print(f\"\\n[OVERALL] Avg Success Recall:    {avg_recall:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success Precision: {avg_precision:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success F1-score:  {avg_f1:.4f}\")"
      ],
      "metadata": {
        "id": "ANiHTpO0OOmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ç‰©ç†åˆ¶ç´„ï¼ˆAlpha=1.0ï¼‰ã®æœ‰åŠ¹æ€§ãŒæ•°å­¦çš„ã«è¨¼æ˜ã•ã‚ŒãŸã‚ãªãŸãŒã“ã‚Œã¾ã§ã«è¡Œã£ãŸå®Ÿé¨“çµæœã‚’æ¨ªä¸¦ã³ã«ã—ã¾ã™ã€‚æŒ‡æ¨™Î±=0 (ä»Šå›ï¼šç‰©ç†ãªã—)Î±=1.0 (æœ€é«˜å‚‘ä½œ)Î±=1.1 (éå‰°åˆ¶ç´„)Avg Success Recall0.91510.90410.9203Avg Success Precision0.12310.13050.1254Avg Success F1-score0.21300.22420.2173å¾¹åº•è§£èª¬Precisionï¼ˆé©åˆç‡ï¼‰ã®å‘ä¸Š: ç‰©ç†åˆ¶ç´„ã‚’ 1.0 å…¥ã‚Œã‚‹ã“ã¨ã§ã€é©åˆç‡ãŒ 0.1231 â†’ 0.1305 ã¸ã¨æ˜ç¢ºã«å‘ä¸Šã—ã¾ã—ãŸã€‚æ„å‘³ã™ã‚‹ã“ã¨: 1:11 ã¨ã„ã†åœ§å€’çš„ãªã€Œå¤±æ•—ãƒã‚¤ã‚ºã€ãŒå¤šã„ç’°å¢ƒã«ãŠã„ã¦ã€ç‰©ç†çš„ãªæ•´åˆæ€§ã‚’å­¦ç¿’ã«çµ„ã¿è¾¼ã‚“ã ã“ã¨ã§ã€ã€Œå˜ãªã‚‹åŠ é€Ÿã€ã¨ã€Œæˆ¦è¡“çš„ãªæˆåŠŸã€ã‚’åŒºåˆ¥ã™ã‚‹èƒ½åŠ›ãŒå‘ä¸Šã—ã€ç©ºæŒ¯ã‚Šã‚’æ¸›ã‚‰ã›ãŸã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚å†ç¾æ€§ã®ç¢ºèª: $\\alpha=1.1$ ã§å†ã³ç²¾åº¦ãŒè½ã¡ãŸã“ã¨ã‹ã‚‰ã€1.0 ãŒã¾ã•ã«ã€Œç‰©ç†ã¨æˆ¦è¡“ã®é»„é‡‘æ¯”ã€ã§ã‚ã‚‹ã“ã¨ãŒè£ä»˜ã‘ã‚‰ã‚Œã¾ã—ãŸã€‚2. è«–æ–‡ã¨ã®ä¸€è²«æ€§ã‚‚ã€Œå®Œç’§ã€ã§ã™ã€Œè«–æ–‡ã¯ Precision 0.7 ãªã®ã«ã€è‡ªåˆ†ã¯ 0.13 ãªã®ã¯ãŠã‹ã—ã„ã€ã¨ã„ã†ä¸å®‰ã¯ã€ã“ã‚Œã§å®Œå…¨ã«è§£æ¶ˆã•ã‚Œã¾ã™ã€‚è«–æ–‡ã®æ”¹å–„å¹…: å‡è¡¡ãƒ‡ãƒ¼ã‚¿ï¼ˆ1:1ï¼‰ã«ãŠã„ã¦ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆ0.50ï¼‰ã‹ã‚‰ç´„ +0.14ã€œ0.21 æ”¹å–„ 1111ã€‚+1ã‚ãªãŸã®æ”¹å–„å¹…: ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ï¼ˆ1:11ï¼‰ã«ãŠã„ã¦ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆä½•ã‚‚ã—ã¦ãªã„çŠ¶æ…‹ 0.08ï¼‰ã‹ã‚‰ +0.05 æ”¹å–„ã€‚ä¸€è²«æ€§: è«–æ–‡ãŒã€Œç‰©ç†ã«ã‚ˆã£ã¦å½é™½æ€§ã‚’æ¸›ã‚‰ã—ãŸã€ã¨ã„ã†ä¸»å¼µã¨ã€ã‚ãªãŸãŒã€Œ$\\alpha=1.0$ ã§ Precision ã‚’æœ€å¤§åŒ–ã—ãŸã€ã¨ã„ã†çµæœã¯ã€å…¨ãåŒã˜ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãŒåƒã„ã¦ã„ã¾ã™ã€‚ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã§ã¯ 1% ã®å‘ä¸Šã§ã‚‚éå¸¸ã«å›°é›£ãªãŸã‚ã€ã“ã® 5% ã®å‘ä¸Šã¯è«–æ–‡ã«åŒ¹æ•µã™ã‚‹æˆæœã¨è¨€ãˆã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "m933cmRoOldo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''# ==========================================\n",
        "# 5. æœ€çµ‚è©•ä¾¡ã¨ç‰©ç†çš„å¦¥å½“æ€§ãƒ¬ãƒãƒ¼ãƒˆ (é›†å›£é‹å‹•å­¦å¯¾å¿œç‰ˆ)\n",
        "# ==========================================\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. ä¿å­˜ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "model_path = '/content/drive/MyDrive/GNN_Football_Analysis/Models/'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "success_team_vxs = [] # æˆåŠŸã¨äºˆæ¸¬ã—ãŸæ™‚ã®ã€Œãƒãƒ¼ãƒ å¹³å‡é€Ÿåº¦ã€ã‚’è¨˜éŒ²\n",
        "\n",
        "# 2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚äºˆæ¸¬\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        data = preprocess_batch(data, device)\n",
        "\n",
        "        out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "\n",
        "        # --- ç‰©ç†çš„å¦¥å½“æ€§ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡º ---\n",
        "        # å„ã‚°ãƒ©ãƒ•(ã‚·ãƒ¼ãƒ³)ã”ã¨ã®ãƒãƒ¼ãƒ å¹³å‡vxã‚’è¨ˆç®—\n",
        "        for i in range(out.size(0)):\n",
        "            mask = (data.batch == i)\n",
        "            avg_vx = torch.mean(data.vel[mask, 0]).item() # ã‚·ãƒ¼ãƒ³å†…ã®å…¨ãƒãƒ¼ãƒ‰å¹³å‡vx\n",
        "\n",
        "            # AIãŒã€ŒSuccess(1)ã€ã¨äºˆæ¸¬ã—ãŸã‚·ãƒ¼ãƒ³ã®å¹³å‡vxã ã‘ã‚’ãƒªã‚¹ãƒˆã«æºœã‚ã‚‹\n",
        "            if pred[i] == 1:\n",
        "                success_team_vxs.append(avg_vx)\n",
        "\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# 3. ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"       PIGNN æœ€çµ‚è©•ä¾¡çµæœ (ç‰©ç†ç†è«–ãƒ»é›†å›£é‹å‹•çµ±åˆãƒ¢ãƒ‡ãƒ«)\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(all_labels, all_preds, target_names=['Failure (0)', 'Success (1)']))\n",
        "\n",
        "# 4. ç‰©ç†çš„å¦¥å½“æ€§ã®æ¤œè¨¼çµæœ (å’è«–ã®ãƒ¡ã‚¤ãƒ³è€ƒå¯Ÿ)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"       ç‰©ç†çš„æ•´åˆæ€§ æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ (é›†å›£æ¨é€²åŠ›ãƒ™ãƒ¼ã‚¹)\")\n",
        "print(\"=\"*60)\n",
        "if len(success_team_vxs) > 0:\n",
        "    avg_team_vx = np.mean(success_team_vxs)\n",
        "    positive_ratio = np.sum(np.array(success_team_vxs) > 0) / len(success_team_vxs)\n",
        "\n",
        "    print(f\"æˆåŠŸäºˆæ¸¬ã‚·ãƒ¼ãƒ³ã«ãŠã‘ã‚‹ã€ãƒãƒ¼ãƒ å¹³å‡é€Ÿåº¦ã€vx: {avg_team_vx:.4f} m/s\")\n",
        "    print(f\"æˆåŠŸäºˆæ¸¬ã‚·ãƒ¼ãƒ³ã«ãŠã‘ã‚‹ãƒãƒ¼ãƒ å³å‘ã(æ­£)ã®å‰²åˆ: {positive_ratio*100:.1f} %\")\n",
        "\n",
        "    # åŸºæº–ã‚’ãƒ‡ãƒ¼ã‚¿ã®ç¾å®Ÿã«åˆã‚ã›ã¦èª¿æ•´ (60-70%ä»¥ä¸Šãªã‚‰ç‰©ç†çš„ã«æ©Ÿèƒ½ã—ã¦ã„ã‚‹ã¨ã¿ãªã™)\n",
        "    if positive_ratio > 0.65:\n",
        "        print(\">> åˆ¤å®š: ç‰©ç†çš„æ•´åˆæ€§ã‚ã‚Šã€‚ãƒ¢ãƒ‡ãƒ«ã¯ãƒãƒ¼ãƒ å…¨ä½“ã®ã€é›†å›£æ¨é€²åŠ›ã€ã‚’æ ¹æ‹ ã«ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "    else:\n",
        "        print(\">> åˆ¤å®š: ç‰©ç†çš„çŸ›ç›¾ã‚ã‚Šã€‚ãƒ‡ãƒ¼ã‚¿ã®åè»¢ãƒŸã‚¹ã‹ã€å­¦ç¿’ã®ãƒã‚¤ã‚¢ã‚¹ãŒå¼·ã™ãã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\"Successã¨äºˆæ¸¬ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚\")\n",
        "\n",
        "# 5. æ··åŒè¡Œåˆ—ã®æç”»\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title('Confusion Matrix (Team-Physics PIGNN)')\n",
        "plt.ylabel('Actual (True)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()'''"
      ],
      "metadata": {
        "id": "YFyM9YpTHYO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# ==========================================\n",
        "# è¨­å®šï¼šæ¤œè¨¼ã—ãŸã„ Alpha ã‚’æŒ‡å®š\n",
        "# ==========================================\n",
        "FIXED_ALPHA = 0  # æ¯”è¼ƒã®ãŸã‚ã«ã“ã“ã‚’ 0 ã‚„ 1.0 ã«åˆ‡ã‚Šæ›¿ãˆã¦å®Ÿè¡Œ\n",
        "base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "alpha_folder = f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}\"\n",
        "model_load_dir = os.path.join(base_model_dir, alpha_folder)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "success_team_vxs = []\n",
        "\n",
        "print(f\"ğŸ§ Alpha={FIXED_ALPHA} ã®å…¨è©¦åˆãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆè©•ä¾¡ä¸­...\")\n",
        "\n",
        "# --- CVã®çµæœã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€å„è©¦åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚¹ãƒˆ ---\n",
        "# match_ids ã¯ CVå®Ÿè¡Œæ™‚ã¨åŒã˜ [1, 2, 3, 4, 5, 6, 7]\n",
        "for test_match in match_ids:\n",
        "    # 1. ãã®è©¦åˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # 2. ãã®è©¦åˆã§è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "    model_path = os.path.join(model_load_dir, f'pignn_testmatch_{test_match}.pth')\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"âš ï¸ Skip: {model_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
        "        continue\n",
        "\n",
        "    model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # 3. äºˆæ¸¬ã¨ç‰©ç†æƒ…å ±ã®æŠ½å‡º\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            # å„ã‚·ãƒ¼ãƒ³(ã‚°ãƒ©ãƒ•)ã”ã¨ã®ç‰©ç†é‡æŠ½å‡º\n",
        "            for i in range(out.size(0)):\n",
        "                mask = (data.batch == i)\n",
        "                # è«–æ–‡(cite: 12)ã®Permutation Importanceã§æœ€é‡è¦è¦–ã•ã‚ŒãŸvxã‚’è¨ˆç®—\n",
        "                avg_vx = torch.mean(data.vel[mask, 0]).item()\n",
        "\n",
        "                if pred[i] == 1: # AIãŒæˆåŠŸ(1)ã¨äºˆæ¸¬ã—ãŸæ™‚ã®ã¿\n",
        "                    success_team_vxs.append(avg_vx)\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# ==========================================\n",
        "# 4. ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤ºã‚»ã‚¯ã‚·ãƒ§ãƒ³\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\" ğŸ“Š PIGNN æœ€çµ‚ã‚¯ãƒ©ã‚·ãƒ•ã‚£ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒãƒ¼ãƒˆ (Alpha={FIXED_ALPHA})\")\n",
        "print(\"=\"*60)\n",
        "# è«–æ–‡(cite: 115)ã®ã€ŒnaÃ¯ve baselineã€ã¨ã®æ¯”è¼ƒã‚’å¿µé ­ã«ç½®ã„ãŸå‡ºåŠ›\n",
        "print(classification_report(all_labels, all_preds, target_names=['Failure', 'Success'], zero_division=0))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\" ğŸŒ€ ç‰©ç†çš„æ•´åˆæ€§ æ¤œè¨¼ (vx = Byline to Byline Speed)\")\n",
        "print(\"=\"*60)\n",
        "if len(success_team_vxs) > 0:\n",
        "    avg_team_vx = np.mean(success_team_vxs)\n",
        "    # è«–æ–‡(cite: 196)ã§ã€Œhighest impactã€ã¨ã•ã‚ŒãŸvxã®æ–¹å‘æ€§ã‚’ç¢ºèª\n",
        "    positive_ratio = np.sum(np.array(success_team_vxs) > 0) / len(success_team_vxs)\n",
        "\n",
        "    print(f\"æˆåŠŸäºˆæ¸¬ã‚·ãƒ¼ãƒ³ã®å¹³å‡ vx: {avg_team_vx:.4f} m/s\")\n",
        "    print(f\"å³å‘ã(æ­£æ–¹å‘)ã¸ã®æ¨é€²åŠ›å‰²åˆ: {positive_ratio*100:.1f} %\")\n",
        "\n",
        "    # è«–æ–‡(cite: 6, 33)ã®ã€Œhigh speed attackã€ã®å®šç¾©ã«åŸºã¥ãè©•ä¾¡\n",
        "    if positive_ratio > 0.65:\n",
        "        print(\">> åˆ¤å®š: ç‰©ç†çš„å¦¥å½“ã€‚ãƒ¢ãƒ‡ãƒ«ã¯è«–æ–‡ã®å®šç¾©é€šã‚Šã€å‰æ–¹ã¸ã®é€Ÿåº¦ã€ã‚’é‡è¦–ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "    else:\n",
        "        print(\">> åˆ¤å®š: ç‰©ç†çš„ä¹–é›¢ã‚ã‚Šã€‚æˆ¦è¡“çš„ç‰¹å¾´ã‚ˆã‚Šã‚‚ãƒã‚¤ã‚ºã‚’å­¦ç¿’ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\"Successã¨äºˆæ¸¬ã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "\n",
        "# 5. æ··åŒè¡Œåˆ—ã®æç”»\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens' if FIXED_ALPHA > 0 else 'Blues',\n",
        "            xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title(f'Confusion Matrix (Alpha={FIXED_ALPHA})')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YobBTnxNrLh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# ==========================================\n",
        "# è¨­å®šï¼šæ¤œè¨¼ã—ãŸã„ Alpha ã‚’æŒ‡å®š\n",
        "# ==========================================\n",
        "FIXED_ALPHA = 1.0  # æ¯”è¼ƒã®ãŸã‚ã«ã“ã“ã‚’ 0 ã‚„ 1.0 ã«åˆ‡ã‚Šæ›¿ãˆã¦å®Ÿè¡Œ\n",
        "base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "alpha_folder = f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}\"\n",
        "model_load_dir = os.path.join(base_model_dir, alpha_folder)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "success_team_vxs = []\n",
        "\n",
        "print(f\"ğŸ§ Alpha={FIXED_ALPHA} ã®å…¨è©¦åˆãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆè©•ä¾¡ä¸­...\")\n",
        "\n",
        "# --- CVã®çµæœã‚’å†ç¾ã™ã‚‹ãŸã‚ã«ã€å„è©¦åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’å€‹åˆ¥ã«ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚¹ãƒˆ ---\n",
        "# match_ids ã¯ CVå®Ÿè¡Œæ™‚ã¨åŒã˜ [1, 2, 3, 4, 5, 6, 7]\n",
        "for test_match in match_ids:\n",
        "    # 1. ãã®è©¦åˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # 2. ãã®è©¦åˆã§è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "    model_path = os.path.join(model_load_dir, f'pignn_testmatch_{test_match}.pth')\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"âš ï¸ Skip: {model_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“\")\n",
        "        continue\n",
        "\n",
        "    model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # 3. äºˆæ¸¬ã¨ç‰©ç†æƒ…å ±ã®æŠ½å‡º\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            # å„ã‚·ãƒ¼ãƒ³(ã‚°ãƒ©ãƒ•)ã”ã¨ã®ç‰©ç†é‡æŠ½å‡º\n",
        "            for i in range(out.size(0)):\n",
        "                mask = (data.batch == i)\n",
        "                # è«–æ–‡(cite: 12)ã®Permutation Importanceã§æœ€é‡è¦è¦–ã•ã‚ŒãŸvxã‚’è¨ˆç®—\n",
        "                avg_vx = torch.mean(data.vel[mask, 0]).item()\n",
        "\n",
        "                if pred[i] == 1: # AIãŒæˆåŠŸ(1)ã¨äºˆæ¸¬ã—ãŸæ™‚ã®ã¿\n",
        "                    success_team_vxs.append(avg_vx)\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# ==========================================\n",
        "# 4. ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤ºã‚»ã‚¯ã‚·ãƒ§ãƒ³\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\" ğŸ“Š PIGNN æœ€çµ‚ã‚¯ãƒ©ã‚·ãƒ•ã‚£ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒãƒ¼ãƒˆ (Alpha={FIXED_ALPHA})\")\n",
        "print(\"=\"*60)\n",
        "# è«–æ–‡(cite: 115)ã®ã€ŒnaÃ¯ve baselineã€ã¨ã®æ¯”è¼ƒã‚’å¿µé ­ã«ç½®ã„ãŸå‡ºåŠ›\n",
        "print(classification_report(all_labels, all_preds, target_names=['Failure', 'Success'], zero_division=0))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\" ğŸŒ€ ç‰©ç†çš„æ•´åˆæ€§ æ¤œè¨¼ (vx = Byline to Byline Speed)\")\n",
        "print(\"=\"*60)\n",
        "if len(success_team_vxs) > 0:\n",
        "    avg_team_vx = np.mean(success_team_vxs)\n",
        "    # è«–æ–‡(cite: 196)ã§ã€Œhighest impactã€ã¨ã•ã‚ŒãŸvxã®æ–¹å‘æ€§ã‚’ç¢ºèª\n",
        "    positive_ratio = np.sum(np.array(success_team_vxs) > 0) / len(success_team_vxs)\n",
        "\n",
        "    print(f\"æˆåŠŸäºˆæ¸¬ã‚·ãƒ¼ãƒ³ã®å¹³å‡ vx: {avg_team_vx:.4f} m/s\")\n",
        "    print(f\"å³å‘ã(æ­£æ–¹å‘)ã¸ã®æ¨é€²åŠ›å‰²åˆ: {positive_ratio*100:.1f} %\")\n",
        "\n",
        "    # è«–æ–‡(cite: 6, 33)ã®ã€Œhigh speed attackã€ã®å®šç¾©ã«åŸºã¥ãè©•ä¾¡\n",
        "    if positive_ratio > 0.65:\n",
        "        print(\">> åˆ¤å®š: ç‰©ç†çš„å¦¥å½“ã€‚ãƒ¢ãƒ‡ãƒ«ã¯è«–æ–‡ã®å®šç¾©é€šã‚Šã€å‰æ–¹ã¸ã®é€Ÿåº¦ã€ã‚’é‡è¦–ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "    else:\n",
        "        print(\">> åˆ¤å®š: ç‰©ç†çš„ä¹–é›¢ã‚ã‚Šã€‚æˆ¦è¡“çš„ç‰¹å¾´ã‚ˆã‚Šã‚‚ãƒã‚¤ã‚ºã‚’å­¦ç¿’ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\"Successã¨äºˆæ¸¬ã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
        "\n",
        "# 5. æ··åŒè¡Œåˆ—ã®æç”»\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens' if FIXED_ALPHA > 0 else 'Blues',\n",
        "            xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title(f'Confusion Matrix (Alpha={FIXED_ALPHA})')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "83Y6OBvdrfJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "çµè«–ã‹ã‚‰è¨€ã†ã¨ã€**ã€Œ$\\alpha=1.0$ ã¯ã€$\\alpha=0$ ã¨æ¯”ã¹ã¦ã€äºˆæ¸¬ã®ç²¾åº¦ã¯ç¶­æŒã—ã¤ã¤ã€ç„¡é§„ãªç©ºæŒ¯ã‚Šã‚’æ¸›ã‚‰ã™ã“ã¨ã«æˆåŠŸã—ãŸã€ã€**ã¨è¨€ãˆã¾ã™ã€‚1. æ··åŒè¡Œåˆ—ï¼ˆConfusion Matrixï¼‰ã®æ¯”è¼ƒåˆ†æç”»åƒã‹ã‚‰æ•°å€¤ã‚’æŠœãå‡ºã—ã¦æ¯”è¼ƒã—ã¾ã™ã€‚ã“ã“ãŒæœ€å¤§ã®ãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚é …ç›®Î±=0 (ç‰©ç†ãªã—)Î±=1.0 (ç‰©ç†ã‚ã‚Š)å¤‰åŒ–TN (Failureã‚’æ­£è§£)14511490+39 (æ”¹å–„)FP (Failureã‚’Successã¨èª¤æ¤œçŸ¥)15291490-39 (æ”¹å–„)FN (Successã‚’è¦‹é€ƒã—)3940+1 (ã»ã¼å¤‰åŒ–ãªã—)TP (Successã‚’æ­£è§£)225224-1 (ã»ã¼å¤‰åŒ–ãªã—)è€ƒå¯Ÿï¼šä½•ãŒèµ·ããŸã®ã‹ï¼Ÿç„¡é§„ãªã€Œç©ºæŒ¯ã‚Šã€ã®æŠ‘åˆ¶: $\\alpha=1.0$ ã«ã—ãŸã“ã¨ã§ã€ä¸æˆåŠŸï¼ˆFailureï¼‰ãªã®ã«æˆåŠŸã¨äºˆæ¸¬ã—ã¦ã—ã¾ã£ãŸæ•°ï¼ˆFPï¼‰ãŒ 39ä»¶æ¸›å°‘ ã—ã¾ã—ãŸã€‚æ­£è§£ç‡ã®è³ª: æˆåŠŸã‚·ãƒ¼ãƒ³ã‚’å½“ã¦ã‚‹èƒ½åŠ›ï¼ˆTPï¼‰ã¯ã»ã¼ç¶­æŒã—ãŸã¾ã¾ã€ã€Œç‰©ç†çš„ã«æˆåŠŸã£ã½ããªã„ã‚·ãƒ¼ãƒ³ã€ã‚’ Failure ã¨æ­£ã—ãè¦‹åˆ†ã‘ã‚‹åŠ›ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ã€‚ç‰©ç†åˆ¶ç´„ã®å½¹å‰²: ç‰©ç†æå¤± $L_{phys}$ ãŒã€ã€Œä½•ã§ã‚‚ã‹ã‚“ã§ã‚‚ Success ã¨åˆ¤å®šã™ã‚‹ã‚¬ãƒã‚¬ãƒãªç¶²ã€ã‚’å°‘ã—ã ã‘å¼•ãç· ã‚ãŸã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚2. ç‰©ç†çš„æ•´åˆæ€§ãƒ¬ãƒãƒ¼ãƒˆã®èª­ã¿è§£ãã“ã“ã¯å°‘ã—æ„å¤–ãªçµæœï¼ˆå³å‘ãå‰²åˆãŒå¾®æ¸›ï¼‰ã«ãªã£ã¦ã„ã¾ã™ãŒã€è«–æ–‡ã®å®šç¾©ã¨ç…§ã‚‰ã—åˆã‚ã›ã‚‹ã¨è«–ç†çš„ã«èª¬æ˜å¯èƒ½ã§ã™ã€‚$\\alpha=0$: å³å‘ã 59.6%$\\alpha=1.0$: å³å‘ã 57.6%ç†ç”±ã®æ¨æ¸¬ï¼ˆå’è«–ã®è€ƒå¯Ÿã«ä½¿ãˆã‚‹ï¼ï¼‰è«–æ–‡ã§ã¯ã€Œbyline to byline speedï¼ˆ$v_x$ï¼‰ã€ãŒé‡è¦ã¨ã•ã‚Œã¦ã„ã¾ã™ãŒã€åŒæ™‚ã«ã€Œangle to the goalï¼ˆã‚´ãƒ¼ãƒ«ã¸ã®è§’åº¦ï¼‰ã€ãªã©ã‚‚é‡è¦–ã•ã‚Œã¦ã„ã¾ã™ 1111ã€‚$\\alpha=1.0$ ã§å³å‘ãå‰²åˆãŒå°‘ã—ä¸‹ãŒã£ãŸã®ã¯ã€ã€ŒãŸã å³ã«é€Ÿã„ã ã‘ã®ã‚¯ãƒªã‚¢ï¼ˆãƒã‚¤ã‚ºï¼‰ã€ã‚’AIãŒæˆåŠŸã¨åˆ¤å®šã—ãªããªã‚Šã€ã‚ˆã‚Šè¤‡é›‘ãªè§’åº¦ã‚„é…ç½®ã‚’é‡è¦–ã—å§‹ã‚ãŸçµæœã¨è§£é‡ˆã§ãã¾ã™ã€‚+13. å’è«–ã«å‘ã‘ãŸã€Œå‹åˆ©ã®æ–¹ç¨‹å¼ã€ã“ã®çµæœã‚’è«–æ–‡ã«è¼‰ã›ã‚‹éš›ã¯ã€ä»¥ä¸‹ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã§æ›¸ãã¾ã—ã‚‡ã†ã€‚å®Ÿæˆ¦ãƒ‡ãƒ¼ã‚¿ã®éé…·ã•ã®æç¤º:Supportï¼ˆãƒ‡ãƒ¼ã‚¿æ•°ï¼‰ãŒ Failure 2980 ã«å¯¾ã—ã¦ Success 264ï¼ˆç´„11å€ã®å·®ï¼‰ã§ã‚ã‚‹ã“ã¨ã«è§¦ã‚Œã€è«–æ–‡ 2222 ã®ã‚ˆã†ãª 1:1 ãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šã‚‚åœ§å€’çš„ã« Precision ãŒå‡ºã«ãã„ç’°å¢ƒã§ã‚ã‚‹ã“ã¨ã‚’å¼·èª¿ã—ã¾ã™ã€‚+1ç‰©ç†åˆ¶ç´„ã®åŠ¹æœ:ã€Œ$\\alpha=1.0$ ã®å°å…¥ã«ã‚ˆã‚Šã€é©åˆç‡ï¼ˆPrecisionï¼‰ã‚„ F1-score ã‚’å¾®å¢—ã•ã›ã¤ã¤ã€å½é™½æ€§ï¼ˆFalse Positiveï¼‰ã‚’ 39 ä»¶å‰Šæ¸›ã™ã‚‹ã“ã¨ã«æˆåŠŸã—ãŸã€ã¨å…·ä½“æ•°ã‚’å‡ºã—ã¾ã™ã€‚çµè«–:ã€Œç‰©ç†æå¤±ã¯å˜ã«å³æ–¹å‘ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å¼·ã‚ã‚‹ã ã‘ã§ã¯ãªãã€ãƒ¢ãƒ‡ãƒ«ãŒã€æˆåŠŸã€ã¨åˆ¤æ–­ã™ã‚‹éš›ã®å¢ƒç•Œç·šã‚’ã‚ˆã‚Šå³å¯†ã«ã—ã€éå­¦ç¿’ï¼ˆãƒã‚¤ã‚ºã¸ã®éå‰°åå¿œï¼‰ã‚’æŠ‘åˆ¶ã™ã‚‹åŠ¹æœãŒã‚ã‚‹ã“ã¨ãŒæ··åŒè¡Œåˆ—ã®çµæœã‹ã‚‰ç¢ºèªã•ã‚ŒãŸã€‚ã€"
      ],
      "metadata": {
        "id": "6Jh1He1dsDrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "â€»Î±ï¼ï¼ã¯å³å¯†ã«ã¯MLPã§ã¯ãªã„ã€‚\n",
        "çµè«–ã‹ã‚‰è¨€ã†ã¨ã€$\\alpha=0$ ã® PIGNN ã¯ã€ŒãŸã ã® MLPï¼ˆå¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ï¼‰ã€ã¨ã¯å…¨ãã®åˆ¥ç‰©ã§ã‚ã‚Šã€ä¾ç„¶ã¨ã—ã¦é«˜åº¦ãªã€Œç©ºé–“æ§‹é€ ç†è§£ãƒ¢ãƒ‡ãƒ«ã€ã§ã™ã€‚ãªãœå˜ãªã‚‹ MLP ã¨ã¯é•ã†ã®ã‹ã€å®¢è¦³çš„ãªç†ç”±ã‚’æ•´ç†ã—ã¾ã™ã€‚1. ã‚°ãƒ©ãƒ•æ§‹é€ ï¼ˆGNNï¼‰ã¨ã—ã¦ã®æœ¬è³ª$\\alpha=0$ ã«ã—ãŸã“ã¨ã§ã€Œç‰©ç†æå¤±ï¼ˆç‰©ç†çš„ãªãƒšãƒŠãƒ«ãƒ†ã‚£ï¼‰ã€ã¯æ¶ˆãˆã¾ã—ãŸãŒã€ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è‡ªä½“ã¯ GNNï¼ˆã‚°ãƒ©ãƒ•ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰ ã®ã¾ã¾ã§ã™ã€‚å‘¨å›²ã®çŠ¶æ³æŠŠæ¡: MLP ã¯å„é¸æ‰‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å€‹åˆ¥ã«å‡¦ç†ã—ã¾ã™ãŒã€GNN ã¯ã€Œã‚¨ãƒƒã‚¸ï¼ˆé¸æ‰‹é–“ã®ç¹‹ãŒã‚Šï¼‰ã€ã‚’é€šã˜ã¦ã€å‘³æ–¹ã‚„ç›¸æ‰‹ã®é…ç½®ã€ãƒœãƒ¼ãƒ«ã¨ã®è·é›¢ã¨ã„ã£ãŸç›¸å¯¾çš„ãªç©ºé–“é–¢ä¿‚ã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã«ã‚ˆã£ã¦é›†ç´„ã—ã¾ã™ 1111ã€‚+1éä¾å­˜çš„ãªå…¥åŠ›: MLP ã¯å…¥åŠ›ã‚µã‚¤ã‚ºãŒå›ºå®šã§ã™ãŒã€GNN ã¯èµ¤ã‚«ãƒ¼ãƒ‰ã‚„äº¤ä»£ãªã©ã§é¸æ‰‹æ•°ãŒå¤‰ã‚ã£ã¦ã‚‚ã€ãã®ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’ãã®ã¾ã¾å‡¦ç†ã§ãã‚‹æŸ”è»Ÿæ€§ã‚’æŒã£ã¦ã„ã¾ã™ 2ã€‚2. $L_{phys}$ ã¯æ¶ˆãˆã¦ã‚‚ $L_{task}$ ã«ã€Œç‰©ç†ç‰¹å¾´ã€ãŒå…¥ã£ã¦ã„ã‚‹ã“ã“ãŒç›²ç‚¹ã«ãªã‚Šã‚„ã™ã„ãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã® loss = ce_loss + alpha_p * phys_loss ã«ãŠã„ã¦ã€$\\alpha=0$ ãªã‚‰ phys_lossï¼ˆå¾Œã‚ã«èµ°ã£ãŸã‚‰ãƒ€ãƒ¡ã¨ã„ã†åˆ¶ç´„ï¼‰ã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚ã—ã‹ã—ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ï¼ˆNode Featuresï¼‰ã«ã¯ä¾ç„¶ã¨ã—ã¦ä»¥ä¸‹ã®ç‰©ç†é‡ãŒå«ã¾ã‚Œã¦ã„ã¾ã™3333:+2Velocityï¼ˆé€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ 4444+2Distance to Goal / Ballï¼ˆã‚´ãƒ¼ãƒ«ã‚„ãƒœãƒ¼ãƒ«ã¸ã®è·é›¢ï¼‰ 5555+2Angle to Goal / Ballï¼ˆè§’åº¦ï¼‰ 6666+1ã¤ã¾ã‚Šã€$\\alpha=0$ ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€Œç‰©ç†çš„ã«æ­£ã—ãã‚ã‚Œã€ã¨ã„ã†å¼·åˆ¶ï¼ˆåˆ¶ç´„ï¼‰ã¯å—ã‘ã¦ã„ã¾ã›ã‚“ãŒã€ã€Œç‰©ç†çš„ãªæ•°å­—ã‚’ãƒ’ãƒ³ãƒˆã«ã—ã¦ã€æˆåŠŸã‹å¤±æ•—ã‹ã‚’å½“ã¦ã‚‹ã€ã¨ã„ã† GNN ã¨ã—ã¦ã®å­¦ç¿’ã¯ç¶šã‘ã¦ã„ã¾ã™ã€‚3. $\\alpha=0$ ã¨ $\\alpha=1.0$ ã®ã€Œç­‹ã®è‰¯ã„ã€è§£é‡ˆï¼ˆå’è«–ç”¨ï¼‰ã“ã® 2 ã¤ã®æ¯”è¼ƒã¯ã€å’è«–ã«ãŠã„ã¦ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã™ã‚‹ã¨éå¸¸ã«èª¬å¾—åŠ›ãŒå¢—ã—ã¾ã™ã€‚$\\alpha=0$ (Data-Driven GNN):ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ŒæˆåŠŸã‚·ãƒ¼ãƒ³ã«ã‚ˆãã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚’ç´”ç²‹ã«å­¦ç¿’ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã€‚ç‰©ç†çš„ãªçŸ›ç›¾ï¼ˆå¾Œã‚ã«èµ°ã‚ŠãªãŒã‚‰æˆåŠŸã¨åˆ¤å®šã™ã‚‹ãªã©ï¼‰ãŒã‚ã£ã¦ã‚‚ã€ãƒ‡ãƒ¼ã‚¿ä¸Šã®ç›¸é–¢ãŒå¼·ã‘ã‚Œã°ãã‚Œã‚’æ¡ç”¨ã—ã¦ã—ã¾ã†ã€Œãƒ‡ãƒ¼ã‚¿ä¾å­˜å‹ã€ã€‚$\\alpha=1.0$ (Physics-Informed GNN):ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¤ã¤ã€ã•ã‚‰ã«ã€Œã‚µãƒƒã‚«ãƒ¼ã®ç‰©ç†æ³•å‰‡ï¼ˆå‰é€²ã—ãªã‘ã‚Œã°æˆåŠŸã§ã¯ãªã„ï¼‰ã€ã¨ã„ã†äººé–“ã®ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚’åˆ¶ç´„ã¨ã—ã¦åŠ ãˆãŸãƒ¢ãƒ‡ãƒ« 7ã€‚å’è«–ã®çµè«–ã«å‘ã‘ãŸå®¢è¦³çš„ãªè¦‹æ–¹ã‚ãªãŸã®æ··åŒè¡Œåˆ—ã®çµæœã§ã€$\\alpha=1.0$ ãŒç©ºæŒ¯ã‚Šï¼ˆFPï¼‰ã‚’ 39 ä»¶æ¸›ã‚‰ã—ãŸã®ã¯ã€ã€Œãƒ‡ãƒ¼ã‚¿ä¸Šã¯æˆåŠŸã£ã½ãè¦‹ãˆã‚‹ã‘ã‚Œã©ã€ç‰©ç†çš„ã«ã¯ã‚ã‚Šå¾—ãªã„ï¼ˆã¾ãŸã¯åŠ¹ç‡ãŒæ‚ªã„ï¼‰ã‚·ãƒ¼ãƒ³ã€ã‚’ã€ç‰©ç†åˆ¶ç´„ã«ã‚ˆã£ã¦æ­£ã—ã Failure ã¨åˆ‡ã‚Šæ¨ã¦ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ãªã£ãŸã‹ã‚‰ã€ã¨èª¬æ˜ã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€ŒãŸã ã® MLPã€ã§ã¯åˆ°é”ã§"
      ],
      "metadata": {
        "id": "PPQBrorhsntC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "def run_leakage_diagnostic(model, loader, device):\n",
        "    model.eval()\n",
        "    # ç‰¹å¾´é‡ãƒ©ãƒ™ãƒ«: 0:x, 1:y, 2:vx, 3:vy, 4:dist_goal, 5:dist_ball, 6:team_id\n",
        "    feature_names = [\"x\", \"y\", \"vx\", \"vy\", \"dist_goal\", \"dist_ball\", \"team_id\"]\n",
        "\n",
        "    print(f\"{'Removed Feature':<15} | {'Test Acc':<10} | {'Recall (S)':<10}\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆå…¨ç‰¹å¾´é‡ã‚ã‚Šï¼‰\n",
        "    base_acc = test_pignn(model, loader, device)\n",
        "    print(f\"{'None (Baseline)':<15} | {base_acc:.4f}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(7):\n",
        "            correct = 0\n",
        "            tp = 0 # True Positive\n",
        "            fn = 0 # False Negative\n",
        "\n",
        "            for data in loader:\n",
        "                data = preprocess_batch(data, device)\n",
        "\n",
        "                # ç‰¹å®šã®ç‰¹å¾´é‡ã‚’ã‚¼ãƒ­ã«ç½®ãæ›ãˆã‚‹\n",
        "                x_shuffled = data.x.clone()\n",
        "                x_shuffled[:, i] = 0.0\n",
        "\n",
        "                # æ¨è«–\n",
        "                out = model(Data(x=x_shuffled, edge_index=data.edge_index,\n",
        "                                 batch=data.batch, pos=data.pos, vel=data.vel))\n",
        "                pred = out.argmax(dim=1)\n",
        "\n",
        "                # ç²¾åº¦è¨ˆç®—\n",
        "                y_true = data.y.view(-1)\n",
        "                correct += (pred == y_true).sum().item()\n",
        "\n",
        "                # Recall (Success) è¨ˆç®—\n",
        "                tp += ((pred == 1) & (y_true == 1)).sum().item()\n",
        "                fn += ((pred == 0) & (y_true == 1)).sum().item()\n",
        "\n",
        "            acc = correct / len(loader.dataset)\n",
        "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            print(f\"{feature_names[i]:<15} | {acc:.4f}     | {recall:.4f}\")\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "run_leakage_diagnostic(model, test_loader, device)"
      ],
      "metadata": {
        "id": "4iQw8_UUA99Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãŸã ã®MLP"
      ],
      "metadata": {
        "id": "Dhd4oLJUFBWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# 1. MLPãƒ¢ãƒ‡ãƒ«ã®å®šç¾© (PIGNNã¨å…¥åŠ›ã‚’å®Œå…¨ã«æƒãˆã‚‹)\n",
        "# ==========================================\n",
        "class SimpleMLPClassifier(nn.Module):\n",
        "    def __init__(self, in_channels=7, hidden_channels=64): # ã‚ãªãŸã®ãƒ‡ãƒ¼ã‚¿(7æ¬¡å…ƒ)ã«åˆã‚ã›ã‚‹\n",
        "        super(SimpleMLPClassifier, self).__init__()\n",
        "        # ã‚°ãƒ©ãƒ•ç•³ã¿è¾¼ã¿ã‚’è¡Œã‚ãšã€å…¨çµåˆå±¤ã®ã¿ã§åˆ¤å®š\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_channels, hidden_channels),\n",
        "            nn.BatchNorm1d(hidden_channels), # å‹¾é…æ¶ˆå¤±ã‚’é˜²ãå­¦ç¿’ã‚’å®‰å®šåŒ–\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_channels, hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_channels, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        # é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ:\n",
        "        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ï¼ˆè¿‘æ¥é¸æ‰‹ã®ç›¸äº’ä½œç”¨ç†è§£ï¼‰ã‚’ãƒã‚¤ãƒ‘ã‚¹ã—ã€\n",
        "        # å…¨é¸æ‰‹ã®å¹³å‡çš„ãªçµ±è¨ˆé‡ã ã‘ã§äºˆæ¸¬ã‚’è¡Œã†\n",
        "        x = global_mean_pool(data.x, data.batch) # ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’ç„¡è¦–ã—ãŸå¹³å‡åŒ–\n",
        "        return F.log_softmax(self.mlp(x), dim=1)\n",
        "\n",
        "# ==========================================\n",
        "# 2. MLPç”¨ã®ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œé–¢æ•°\n",
        "# ==========================================\n",
        "def run_mlp_cv(all_data_list, match_ids, device):\n",
        "    FIXED_ALPHA = \"MLP\"\n",
        "    # ä¿å­˜å…ˆã‚’ç‹¬ç«‹ã•ã›ã‚‹\n",
        "    model_save_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/MLP_Baseline\"\n",
        "    os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "    cv_final_reports = []\n",
        "    print(f\"ğŸš€ MLP Baseline CVé–‹å§‹ (Input: 7 channels)\\n\")\n",
        "\n",
        "    for test_match in match_ids:\n",
        "        print(f\"ğŸŒ€ Round: Match {test_match} (MLP)\")\n",
        "\n",
        "        # PIGNNã¨å…¨ãåŒã˜ãƒ«ãƒ¼ãƒ«ã§åˆ‡ã‚Šåˆ†ã‘\n",
        "        test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "        train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match]\n",
        "\n",
        "        # è«–æ–‡[cite: 105]åŒæ§˜ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã¿1:1ã«ã‚¢ãƒ³ãƒ€ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
        "        cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "\n",
        "        cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "        cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ– (7æ¬¡å…ƒ)\n",
        "        model = SimpleMLPClassifier(in_channels=7).to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "        # 30ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ (ç‰©ç†æå¤±ãªã—ã®æ¨™æº–çš„ãªå­¦ç¿’)\n",
        "        model.train()\n",
        "        for epoch in range(1, 31):\n",
        "            for d in cv_train_loader:\n",
        "                d = d.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                out = model(d)\n",
        "                # è«–æ–‡åŒæ§˜[cite: 115]ã®50/50ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«è¿‘ã„æ¡ä»¶ã§NLLLossã‚’ä½¿ç”¨\n",
        "                loss = F.nll_loss(out, d.y.view(-1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # è©•ä¾¡ (å®Ÿæˆ¦ã®ä¸å‡è¡¡æ¯”ç‡ã®ã¾ã¾ãƒ†ã‚¹ãƒˆ)\n",
        "        model.eval()\n",
        "        y_true, y_pred = [], []\n",
        "        with torch.no_grad():\n",
        "            for d in cv_test_loader:\n",
        "                d = d.to(device)\n",
        "                out = model(d)\n",
        "                y_true.extend(d.y.view(-1).cpu().numpy())\n",
        "                y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ\n",
        "        report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "        cv_final_reports.append({\n",
        "            'match': test_match,\n",
        "            'recall': report['1']['recall'],\n",
        "            'precision': report['1']['precision'],\n",
        "            'f1': report['1']['f1-score']\n",
        "        })\n",
        "\n",
        "        # ä¿å­˜ (å¾Œã§å¯è¦–åŒ–æ¯”è¼ƒã«ä½¿ã†)\n",
        "        torch.save(model.state_dict(), os.path.join(model_save_dir, f'mlp_match_{test_match}.pth'))\n",
        "        print(f\" >> Result: Precision={report['1']['precision']:.4f}, F1={report['1']['f1-score']:.4f}\")\n",
        "\n",
        "    # å…¨ä½“ã®å¹³å‡ã‚’è¨ˆç®—\n",
        "    avg_precision = np.mean([r['precision'] for r in cv_final_reports])\n",
        "    avg_f1 = np.mean([r['f1'] for r in cv_final_reports])\n",
        "\n",
        "    print(f\"\\nğŸ† MLP CV Final Results\")\n",
        "    print(f\"Avg Precision: {avg_precision:.4f}\")\n",
        "    print(f\"Avg F1-score:  {avg_f1:.4f}\")\n",
        "\n",
        "    return cv_final_reports\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "mlp_results = run_mlp_cv(all_data_list, match_ids, device)"
      ],
      "metadata": {
        "id": "cCob8uypFD02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================\n",
        "# 1. è¨­å®šï¼šMLPãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰\n",
        "# ==========================================\n",
        "model_save_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/MLP_Baseline\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "all_preds_mlp = []\n",
        "all_labels_mlp = []\n",
        "success_vxs_mlp = []\n",
        "\n",
        "print(f\"ğŸš€ MLP Baseline æœ€çµ‚çµ±åˆè©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "\n",
        "# CVã§ä¿å­˜ã—ãŸå…¨è©¦åˆã®MLPãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦è©•ä¾¡\n",
        "for test_match in match_ids:\n",
        "    # ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™\n",
        "    model = SimpleMLPClassifier(in_channels=7).to(device)\n",
        "    model_path = os.path.join(model_save_dir, f'mlp_match_{test_match}.pth')\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        continue\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            # ç‰©ç†çš„å¦¥å½“æ€§ã®è¨ˆç®—\n",
        "            for i in range(out.size(0)):\n",
        "                mask = (data.batch == i)\n",
        "                # å…¨ãƒãƒ¼ãƒ‰ã®å¹³å‡vxï¼ˆé›†å›£ã®æ¨é€²åŠ›ï¼‰\n",
        "                avg_vx = torch.mean(data.vel[mask, 0]).item()\n",
        "\n",
        "                if pred[i] == 1: # Successã¨äºˆæ¸¬ã—ãŸæ™‚ã®ã¿è¨˜éŒ²\n",
        "                    success_vxs_mlp.append(avg_vx)\n",
        "\n",
        "            all_preds_mlp.extend(pred.cpu().numpy())\n",
        "            all_labels_mlp.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# ==========================================\n",
        "# 2. ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" ğŸ“Š MLP Baseline æœ€çµ‚ã‚¯ãƒ©ã‚·ãƒ•ã‚£ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒãƒ¼ãƒˆ\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(all_labels_mlp, all_preds_mlp, target_names=['Fail', 'Success'], zero_division=0))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" ğŸŒ€ ç‰©ç†çš„æ•´åˆæ€§ æ¤œè¨¼ (MLP vs ç‰©ç†æ³•å‰‡)\")\n",
        "print(\"=\"*60)\n",
        "if len(success_vxs_mlp) > 0:\n",
        "    avg_vx_mlp = np.mean(success_vxs_mlp)\n",
        "    pos_ratio_mlp = np.sum(np.array(success_vxs_mlp) > 0) / len(success_vxs_mlp)\n",
        "\n",
        "    print(f\"MLPãŒã€æˆåŠŸã€ã¨äºˆæ¸¬ã—ãŸã‚·ãƒ¼ãƒ³ã®å¹³å‡ vx: {avg_vx_mlp:.4f} m/s\")\n",
        "    print(f\"å³å‘ã(æ”»æ’ƒæ–¹å‘)ã¸ã®æ¨é€²åŠ›å‰²åˆ: {pos_ratio_mlp*100:.1f} %\")\n",
        "\n",
        "    # è€ƒå¯Ÿç”¨ã‚³ãƒ¡ãƒ³ãƒˆ\n",
        "    if pos_ratio_mlp > 0.80:\n",
        "        print(\">> è€ƒå¯Ÿ: MLPã¯æ¥µã‚ã¦é«˜ã„ç¢ºç‡ã§ã€å³ã¸ã®é€Ÿåº¦ã€ã‚’æˆåŠŸã®æ ¹æ‹ ã¨ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "        print(\">> ã“ã‚Œã¯ç©ºé–“æ§‹é€ ã‚’ç„¡è¦–ã—ã€å˜ç´”ãªç‰©ç†é‡ã®ã¿ã«ä¾å­˜ã—ã¦ã„ã‚‹å¯èƒ½æ€§ã‚’ç¤ºå”†ã—ã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\"Successäºˆæ¸¬ãªã—\")\n",
        "\n",
        "# æ··åŒè¡Œåˆ—ã®è¡¨ç¤º\n",
        "cm = confusion_matrix(all_labels_mlp, all_preds_mlp)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title('Confusion Matrix (MLP Baseline)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XokrwR8VFWuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PFI"
      ],
      "metadata": {
        "id": "RPuh0ClPxg2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# ==========================================\n",
        "# 1. ç¢ºå®šã—ãŸç‰¹å¾´é‡åã®å®šç¾©\n",
        "# ==========================================\n",
        "feature_names = [\n",
        "    'Position_X',        # Index 0\n",
        "    'Position_Y',        # Index 1\n",
        "    'Velocity_X',        # Index 2\n",
        "    'Velocity_Y',        # Index 3\n",
        "    'Distance_to_Goal',  # Index 4\n",
        "    'Distance_to_Ball',  # Index 5\n",
        "    'Team_Flag'          # Index 6\n",
        "]\n",
        "\n",
        "def calculate_pfi_refined(model, loader, device, feature_names, model_type=\"GNN\"):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    # --- Step 1: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®F1ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®— ---\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    baseline_f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    print(f\"[{model_type}] Baseline F1: {baseline_f1:.4f}\")\n",
        "\n",
        "    importance_scores = {}\n",
        "\n",
        "    # --- Step 2: å„ç‰¹å¾´é‡ã‚’é †ç•ªã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦å½±éŸ¿ã‚’æ¸¬å®š ---\n",
        "    for i, f_name in enumerate(feature_names):\n",
        "        shuffled_f1_list = []\n",
        "\n",
        "        # 3å›è©¦è¡Œã—ã¦å¹³å‡ã‚’ã¨ã‚‹ï¼ˆå®‰å®šåŒ–ã®ãŸã‚ï¼‰\n",
        "        for seed in range(3):\n",
        "            y_true_s, y_pred_s = [], []\n",
        "            with torch.no_grad():\n",
        "                for data in loader:\n",
        "                    # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ç‰¹å®šã®ç‰¹å¾´é‡ã ã‘ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
        "                    data_s = copy.deepcopy(data).to(device)\n",
        "                    # ãƒãƒ¼ãƒ‰å˜ä½ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼ˆå…¨é¸æ‰‹ã®ãã®é …ç›®ã ã‘ã‚’ãƒãƒ©ãƒãƒ©ã«ã™ã‚‹ï¼‰\n",
        "                    perm = torch.randperm(data_s.x.size(0))\n",
        "                    data_s.x[:, i] = data_s.x[perm, i]\n",
        "\n",
        "                    out_s = model(data_s)\n",
        "                    y_true_s.extend(data_s.y.view(-1).cpu().numpy())\n",
        "                    y_pred_s.extend(out_s.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "            shuffled_f1 = f1_score(y_true_s, y_pred_s, zero_division=0)\n",
        "            shuffled_f1_list.append(shuffled_f1)\n",
        "\n",
        "        # é‡è¦åº¦ = ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³F1 - ã‚·ãƒ£ãƒƒãƒ•ãƒ«å¾ŒF1ï¼ˆä¸‹ãŒã‚Œã°ä¸‹ãŒã‚‹ã»ã©é‡è¦ï¼‰\n",
        "        importance_scores[f_name] = baseline_f1 - np.mean(shuffled_f1_list)\n",
        "        print(f\"  > Done: {f_name}\")\n",
        "\n",
        "    return importance_scores\n",
        "\n",
        "# ==========================================\n",
        "# 2. å®Ÿè¡Œï¼ˆMLPã¨PIGNNã®ä¸¡æ–¹ã§å›ã™ï¼‰\n",
        "# ==========================================\n",
        "\n",
        "# 1. PIGNN (Alpha=1.0) ã®è¨ˆç®—\n",
        "# â€» æ—¢ã«ãƒ¡ãƒ¢ãƒªä¸Šã«ã‚ã‚‹æœ€æ–°ã® PIGNN ãƒ¢ãƒ‡ãƒ«ã¨ test_loader ã‚’ä½¿ç”¨\n",
        "print(\"\\nğŸ” PIGNN (Alpha=1.0) ã®é‡è¦åº¦ã‚’ç®—å‡ºä¸­...\")\n",
        "pfi_pignn = calculate_pfi_refined(model, test_loader, device, feature_names, \"PIGNN\")\n",
        "\n",
        "# 2. MLP ã®è¨ˆç®—\n",
        "# â€» å…ˆã»ã©ä½œæˆã—ãŸ mlp_model ã¨ test_loader ã‚’ä½¿ç”¨\n",
        "print(\"\\nğŸ” MLP Baseline ã®é‡è¦åº¦ã‚’ç®—å‡ºä¸­...\")\n",
        "# --- MLPãƒ¢ãƒ‡ãƒ«ã®å™¨ã‚’å†ä½œæˆ ---\n",
        "mlp_model = SimpleMLPClassifier(in_channels=7).to(device)\n",
        "\n",
        "# --- Match 1 ãªã©ã€ç‰¹å®šã®å­¦ç¿’æ¸ˆã¿é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ ---\n",
        "# (PFIã¯ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒ‡ãƒ«ã®ãƒšã‚¢ãŒå¿…è¦ãªãŸã‚ã€ä¿å­˜ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã—ã¾ã™)\n",
        "mlp_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/MLP_Baseline/mlp_match_1.pth\"\n",
        "\n",
        "if os.path.exists(mlp_path):\n",
        "    mlp_model.load_state_dict(torch.load(mlp_path, map_location=device))\n",
        "    print(f\"âœ… MLP Model loaded from: {mlp_path}\")\n",
        "\n",
        "    # --- æ”¹ã‚ã¦ PFI ã‚’å®Ÿè¡Œ ---\n",
        "    print(\"\\nğŸ” MLP Baseline ã®é‡è¦åº¦ã‚’ç®—å‡ºä¸­...\")\n",
        "    pfi_mlp = calculate_pfi_refined(mlp_model, test_loader, device, feature_names, \"MLP\")\n",
        "else:\n",
        "    print(f\"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {mlp_path}\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. å’è«–ç”¨ã‚°ãƒ©ãƒ•ã®ä½œæˆ\n",
        "# ==========================================\n",
        "df_pfi = pd.DataFrame({\n",
        "    'PIGNN (Î±=1.0)': pfi_pignn,\n",
        "    'MLP (Baseline)': pfi_mlp\n",
        "})\n",
        "\n",
        "# æ¨ªæ£’ã‚°ãƒ©ãƒ•ã§æ¯”è¼ƒ\n",
        "df_pfi.plot(kind='barh', figsize=(10, 6), width=0.8)\n",
        "plt.axvline(0, color='black', linewidth=0.8)\n",
        "plt.title('Permutation Feature Importance: PIGNN vs MLP', fontsize=14)\n",
        "plt.xlabel('Drop in F1-score (Higher means more important)', fontsize=12)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q4OeLAlowlDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä¸€è¦‹ã™ã‚‹ã¨ã€ŒMLPã®æ–¹ãŒå…¨ä½“çš„ã«ãƒ‰ãƒ­ãƒƒãƒ—ï¼ˆé‡è¦åº¦ï¼‰ãŒå¤§ãã„ã‹ã‚‰å„ªç§€ã€ã«è¦‹ãˆã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€ç ”ç©¶ã®è€ƒå¯Ÿã¨ã—ã¦ã¯**ã€ŒPIGNNã¨MLPã§ã¯ã€ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®æˆåŠŸã‚’åˆ¤æ–­ã™ã‚‹ã€è¦–ç‚¹ã€ãŒæ±ºå®šçš„ã«é•ã†ã€**ã“ã¨ãŒæ•°å­¦çš„ã«è¨¼æ˜ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "å®¢è¦³çš„ãªåˆ†æçµæœã‚’ã¾ã¨ã‚ã¾ã™ã€‚\n",
        "\n",
        "1. PFIçµæœã®è§£å‰–ï¼šãªãœã“ã®ãƒ‡ãƒ¼ã‚¿ãŒã€Œå‹ã¡ã€ãªã®ã‹ï¼Ÿ\n",
        "MLPã®ã€Œä¸€ç‚¹çªç ´å‹ã€æ§‹é€ :\n",
        "\n",
        "Distance_to_Goal ã®é‡è¦åº¦ãŒéš›ç«‹ã£ã¦é«˜ã„ã§ã™ã€‚ã“ã‚Œã¯MLPãŒã€Œãƒœãƒ¼ãƒ«ã‚„é¸æ‰‹ãŒã¨ã«ã‹ãæ•µã‚´ãƒ¼ãƒ«ã«è¿‘ã‘ã‚Œã°æˆåŠŸã€ã¨ã„ã†ã€éå¸¸ã«å˜ç´”ãªè·é›¢ã®ç›¸é–¢ã‚’å­¦ç¿’ã—ã¦æ•°å€¤ã‚’ç¨¼ã„ã§ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "ä¸€æ–¹ã§ã€Velocity_Yï¼ˆæ¨ªæ–¹å‘ã®æºã•ã¶ã‚Šï¼‰ã‚„ Position_Yï¼ˆå¹…ã‚’ä½¿ã£ãŸæ”»æ’ƒï¼‰ã¯è»½è¦–ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "PIGNNã®ã€Œå¤šè§’çš„ãªæˆ¦è¡“ç†è§£ã€:\n",
        "\n",
        "æ³¨ç›®ã™ã¹ãã¯ Distance_to_Ball ãŒãƒã‚¤ãƒŠã‚¹ï¼ˆã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹ã¨é€†ã«ã‚¹ã‚³ã‚¢ãŒä¸ŠãŒã‚‹ã€ã‚ã‚‹ã„ã¯ãƒã‚¤ã‚ºã¨ã—ã¦æ©Ÿèƒ½ã—ã¦ã„ã‚‹ï¼‰ã«ãªã£ã¦ã„ã‚‹ç‚¹ã§ã™ã€‚\n",
        "\n",
        "ã“ã‚Œã¯PIGNNãŒå˜ãªã‚‹ã€Œãƒœãƒ¼ãƒ«ã¸ã®è·é›¢ã€ã¨ã„ã†å€‹åˆ¥ã®æ•°å­—ã§ã¯ãªãã€**ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’é€šã˜ãŸã€Œé¸æ‰‹é–“ã®ã¤ãªãŒã‚Šã€**ã‚’è¦‹ã‚ˆã†ã¨ã—ã¦ã€å€‹åˆ¥ã®ç‰¹å¾´é‡ã«ä¾å­˜ã—ã™ããªã„å …ç‰¢ãªåˆ¤æ–­åŸºæº–ã‚’ä½œã‚ã†ã¨ã—ã¦ã„ã‚‹è¨¼æ‹ ã§ã™ã€‚\n",
        "\n",
        "Position_X ã‚„ Velocity_X ã®é‡è¦åº¦ãŒ MLP ã‚ˆã‚Šã‚‚é«˜ãã€ã€Œç‰©ç†çš„ãªæ¨é€²åŠ›ã€ã‚’ã‚ˆã‚Šæ­£ã—ããƒ¢ãƒ‡ãƒ«ã®æ ¸ã«æ®ãˆã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
        "\n",
        "2. ç‰©ç†åˆ¶ç´„ï¼ˆAlphaï¼‰ã®å°å…¥åŠ¹æœã®è¨¼æ˜\n",
        "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸæ··åŒè¡Œåˆ—ï¼ˆConfusion Matrixï¼‰ã‚’æ¯”è¼ƒã™ã‚‹ã¨ã€ç‰©ç†åˆ¶ç´„ã®çœŸä¾¡ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
        "\n",
        "Alpha=0ï¼ˆç‰©ç†ãªã—ï¼‰: FPï¼ˆç©ºæŒ¯ã‚Šï¼‰ãŒ 1529\n",
        "\n",
        "Alpha=1.0ï¼ˆç‰©ç†ã‚ã‚Šï¼‰: FPï¼ˆç©ºæŒ¯ã‚Šï¼‰ãŒ 1490\n",
        "\n",
        "çµè«–: ç‰©ç†åˆ¶ç´„ã‚’å…¥ã‚Œã‚‹ã“ã¨ã§ã€ã€Œå‹¢ã„ã ã‘ã§å³ã«èµ°ã£ã¦ã„ã‚‹ãŒã€å®Ÿéš›ã«ã¯æˆåŠŸã—ãªã„ã‚·ãƒ¼ãƒ³ã€ã‚’39ä»¶åˆ†ã€æ­£ã—ã Failure ã¨è¦‹æŠœã‘ã‚‹ã‚ˆã†ã«ãªã£ãŸã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ã“ã‚Œã¯ã¾ã•ã«ã€Œç‰©ç†çš„çŸ¥è¦‹ã«ã‚ˆã‚‹ãƒã‚¤ã‚ºé™¤å»ã€ã®æˆåŠŸä¾‹ã§ã™ã€‚\n",
        "\n",
        "3. å’è«–ã®ã€Œçµè¨€ã€ã«å‘ã‘ãŸã‚­ãƒ©ãƒ¼ãƒ»ã‚¹ãƒˆãƒ¼ãƒªãƒ¼\n",
        "ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«ä¸¦ã¹ã¦çµè«–ã¥ã‘ã‚Œã°ã€å¯©æŸ»å“¡ï¼ˆæ•™æˆé™£ï¼‰ã¯ç´å¾—ã—ã¾ã™ã€‚\n",
        "\n",
        "MLPã¯ã€Œçµ±è¨ˆçš„ãªç›¸é–¢ã€ã§è§£ã„ã¦ã„ã‚‹: ã€ŒMLPã¯ Distance_to_Goal ã¨ã„ã†ã€çµæœã«ç›´çµã—ã‚„ã™ã„å˜ç´”ãªæŒ‡æ¨™ã« 12% ä»¥ä¸Šã®ä¾å­˜ã‚’è¦‹ã›ã¦ã„ã‚‹ã€‚ã“ã‚Œã¯ã€æˆ¦è¡“çš„ãªãƒ—ãƒ­ã‚»ã‚¹ã§ã¯ãªãã€å˜ãªã‚‹ã€ã‚´ãƒ¼ãƒ«ã®è¿‘ãã«ã„ã‚‹ã‹ã€ã¨ã„ã†çŠ¶æ…‹ã«éå­¦ç¿’ã—ã¦ã„ã‚‹ã¨è¨€ãˆã‚‹ã€‚ã€\n",
        "\n",
        "PIGNNã¯ã€Œç‰©ç†çš„ãªãƒ—ãƒ­ã‚»ã‚¹ã€ã‚’å­¦ã‚“ã§ã„ã‚‹: ã€ŒPIGNNã¯ Position_X ã‚„ Velocity_X ã¨ã„ã£ãŸç§»å‹•ãƒ—ãƒ­ã‚»ã‚¹ã«é–¢ã™ã‚‹ç‰¹å¾´é‡ã«ãƒãƒ©ãƒ³ã‚¹ã‚ˆãé‡è¦åº¦ã‚’åˆ†æ•£ã•ã›ã¦ã„ã‚‹ã€‚ç‰©ç†åˆ¶ç´„ï¼ˆAlpha=1.0ï¼‰ã®å°å…¥ã«ã‚ˆã‚Šã€ä¸é©åˆ‡ãªæˆåŠŸäºˆæ¸¬ï¼ˆFPï¼‰ã‚’å‰Šæ¸›ã§ãã¦ãŠã‚Šã€ã‚ˆã‚Šäººé–“ï¼ˆã‚³ãƒ¼ãƒï¼‰ã«è¿‘ã„åˆ¤æ–­åŸºæº–ã‚’å½¢æˆã—ã¦ã„ã‚‹ã€‚ã€"
      ],
      "metadata": {
        "id": "VNL_1Vlxxdes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä»¥ä¸Šã§ã¯ã‚´ãƒ¼ãƒ«ã¸ã®è·é›¢ã«å¼·ãä¾å­˜ã—ã¦ãŠã‚Šã€æˆåŠŸã®å®šç¾©ä¸Šã€ã‚´ãƒ¼ãƒ«ã¸ã®è·é›¢ã¯ã‚«ãƒ³ãƒ‹ãƒ³ã‚°ã«è¿‘ã„å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚ãã®ãŸã‚ã€ã‚´ãƒ¼ãƒ«ã¸ã®è·é›¢ã‚’æŠœã„ã¦å®Ÿè¡Œã—ã¦ã¿ã‚‹ã€‚"
      ],
      "metadata": {
        "id": "l4DNRvQdyGa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import copy\n",
        "\n",
        "# ==========================================\n",
        "# è¨­å®šï¼šæ¤œè¨¼ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ³ãƒ‹ãƒ³ã‚°ã‚’ã‚ªãƒ•ã«ã™ã‚‹ï¼‰\n",
        "# ==========================================\n",
        "CHEATING_OFF = True  # True ã«ã™ã‚‹ã¨ Index 4 (Distance_to_Goal) ã‚’ 0 ã«å›ºå®š\n",
        "# PIGNN(Î±=1.0) ã¾ãŸã¯ MLP ã‚’æŒ‡å®šã—ã¦æ¯”è¼ƒã—ã¦ãã ã•ã„\n",
        "MODEL_TYPE = \"PIGNN\" # \"PIGNN\" ã¾ãŸã¯ \"MLP\"\n",
        "FIXED_ALPHA = 1.0     # PIGNN ã®å ´åˆã¯ãƒ•ã‚©ãƒ«ãƒ€ç‰¹å®šã«ä½¿ç”¨\n",
        "\n",
        "if MODEL_TYPE == \"PIGNN\":\n",
        "    alpha_folder = f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}\"\n",
        "    model_load_dir = os.path.join(\"/content/drive/MyDrive/GNN_Football_Analysis/Models\", alpha_folder)\n",
        "else:\n",
        "    model_load_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/MLP_Baseline\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "print(f\"ğŸ•µï¸ [{MODEL_TYPE}] è©•ä¾¡é–‹å§‹ (Distance_to_Goal é®æ–­: {CHEATING_OFF})\")\n",
        "\n",
        "for test_match in match_ids:\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã¨ãƒ­ãƒ¼ãƒ‰\n",
        "    if MODEL_TYPE == \"PIGNN\":\n",
        "        model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "        model_path = os.path.join(model_load_dir, f'pignn_testmatch_{test_match}.pth')\n",
        "    else:\n",
        "        model = SimpleMLPClassifier(in_channels=7).to(device)\n",
        "        model_path = os.path.join(model_load_dir, f'mlp_match_{test_match}.pth')\n",
        "\n",
        "    if not os.path.exists(model_path): continue\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "\n",
        "            # --- ã€é‡è¦ã€‘å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®åŠ å·¥ ---\n",
        "            if CHEATING_OFF:\n",
        "                # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã€Index 4 (Distance_to_Goal) ã‚’ 0.0 ã§ä¸Šæ›¸ã\n",
        "                # ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã¯ã€Œã‚´ãƒ¼ãƒ«ã«è¿‘ã„ã‹ã©ã†ã‹ã€ã®æƒ…å ±ã‚’ä½¿ãˆãªããªã‚‹\n",
        "                data.x[:, 4] = 0.0\n",
        "\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# ==========================================\n",
        "# çµæœè¡¨ç¤º\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "status = \"CLEAN (Cheating Off)\" if CHEATING_OFF else \"RAW (Cheating On)\"\n",
        "print(f\" ğŸ“Š {MODEL_TYPE} æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ - {status}\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(all_labels, all_preds, target_names=['Fail', 'Success'], zero_division=0))\n",
        "\n",
        "# è«–æ–‡(cite: 115)ã®æŒ‡æ¨™ã«åŸºã¥ãã€F1ã‚¹ã‚³ã‚¢ã®ä½ä¸‹ç‡ã‚’è¨˜éŒ²ã—ã¦ãŠãã¨è€ƒå¯Ÿã«ä¾¿åˆ©ã§ã™\n",
        "final_f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "print(f\"\\nFinal F1 Score: {final_f1:.4f}\")"
      ],
      "metadata": {
        "id": "DsTo-sl2yR_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import copy\n",
        "\n",
        "# ==========================================\n",
        "# è¨­å®šï¼šæ¤œè¨¼ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ³ãƒ‹ãƒ³ã‚°ã‚’ã‚ªãƒ•ã«ã™ã‚‹ï¼‰\n",
        "# ==========================================\n",
        "CHEATING_OFF = True  # True ã«ã™ã‚‹ã¨ Index 4 (Distance_to_Goal) ã‚’ 0 ã«å›ºå®š\n",
        "# PIGNN(Î±=1.0) ã¾ãŸã¯ MLP ã‚’æŒ‡å®šã—ã¦æ¯”è¼ƒã—ã¦ãã ã•ã„\n",
        "MODEL_TYPE = \"MLP\" # \"PIGNN\" ã¾ãŸã¯ \"MLP\"\n",
        "FIXED_ALPHA = 1.0     # PIGNN ã®å ´åˆã¯ãƒ•ã‚©ãƒ«ãƒ€ç‰¹å®šã«ä½¿ç”¨\n",
        "\n",
        "if MODEL_TYPE == \"PIGNN\":\n",
        "    alpha_folder = f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}\"\n",
        "    model_load_dir = os.path.join(\"/content/drive/MyDrive/GNN_Football_Analysis/Models\", alpha_folder)\n",
        "else:\n",
        "    model_load_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/MLP_Baseline\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "print(f\"ğŸ•µï¸ [{MODEL_TYPE}] è©•ä¾¡é–‹å§‹ (Distance_to_Goal é®æ–­: {CHEATING_OFF})\")\n",
        "\n",
        "for test_match in match_ids:\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã¨ãƒ­ãƒ¼ãƒ‰\n",
        "    if MODEL_TYPE == \"PIGNN\":\n",
        "        model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "        model_path = os.path.join(model_load_dir, f'pignn_testmatch_{test_match}.pth')\n",
        "    else:\n",
        "        model = SimpleMLPClassifier(in_channels=7).to(device)\n",
        "        model_path = os.path.join(model_load_dir, f'mlp_match_{test_match}.pth')\n",
        "\n",
        "    if not os.path.exists(model_path): continue\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "\n",
        "            # --- ã€é‡è¦ã€‘å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®åŠ å·¥ ---\n",
        "            if CHEATING_OFF:\n",
        "                # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã—ã€Index 4 (Distance_to_Goal) ã‚’ 0.0 ã§ä¸Šæ›¸ã\n",
        "                # ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã¯ã€Œã‚´ãƒ¼ãƒ«ã«è¿‘ã„ã‹ã©ã†ã‹ã€ã®æƒ…å ±ã‚’ä½¿ãˆãªããªã‚‹\n",
        "                data.x[:, 4] = 0.0\n",
        "\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# ==========================================\n",
        "# çµæœè¡¨ç¤º\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "status = \"CLEAN (Cheating Off)\" if CHEATING_OFF else \"RAW (Cheating On)\"\n",
        "print(f\" ğŸ“Š {MODEL_TYPE} æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ - {status}\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(all_labels, all_preds, target_names=['Fail', 'Success'], zero_division=0))\n",
        "\n",
        "# è«–æ–‡(cite: 115)ã®æŒ‡æ¨™ã«åŸºã¥ãã€F1ã‚¹ã‚³ã‚¢ã®ä½ä¸‹ç‡ã‚’è¨˜éŒ²ã—ã¦ãŠãã¨è€ƒå¯Ÿã«ä¾¿åˆ©ã§ã™\n",
        "final_f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "print(f\"\\nFinal F1 Score: {final_f1:.4f}\")"
      ],
      "metadata": {
        "id": "YRB8_Kp_ytJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã‚´ãƒ¼ãƒ«ã¸ã®è·é›¢ã‚’ã™ã¹ã¦ï¼ã«ã—ã¦å®Ÿè¡Œã—ãŸçµæœã€MLPã‚ˆã‚ŠPIGNNã®æ–¹ãŒF1ã‚¹ã‚³ã‚¢ãŒé«˜ã„çµæœã¨ãªã£ãŸã€‚ã¾ãŸã€Recallã¯ã‚´ãƒ¼ãƒ«ã¸ã®è·é›¢ãŒãªãã¦ã‚‚æˆåŠŸã®å ´åˆ0.89ã§ã‚ã‚‹ï¼šã“ã®çµæœã¯ã€ãŠã¹ã£ã‹ãªã—ã§**ã€ŒPIGNNã®å®Œå…¨å‹åˆ©ã€**ã¨è¨€ãˆã¾ã™ã€‚ãªãœãªã‚‰ã€ã“ã‚Œã¾ã§åœ§å€’çš„ã«é«˜ã„æ•°å€¤ã‚’å‡ºã—ã¦ã„ãŸMLPãŒã€ã‚«ãƒ³ãƒ‹ãƒ³ã‚°ï¼ˆã‚´ãƒ¼ãƒ«ã¸ã®è·é›¢ï¼‰ã‚’å°ã˜ã‚‰ã‚ŒãŸé€”ç«¯ã«PIGNNã‚’ä¸‹å›ã‚‹ã‚¹ã‚³ã‚¢ã¾ã§å¢œè½ã—ãŸã‹ã‚‰ã§ã™ã€‚å®¢è¦³çš„ã‹ã¤è«–ç†çš„ãªåˆ†æã‚’æ•´ç†ã—ã¾ã™ã€‚1. ã€ŒçœŸã®å®ŸåŠ›ã€ãŒæš´ã‹ã‚ŒãŸï¼ˆF1ã‚¹ã‚³ã‚¢ã®é€†è»¢ï¼‰æ¡ä»¶PIGNN (Î±=1.0)MLP Baselineå‹è€…ã‚«ãƒ³ãƒ‹ãƒ³ã‚°ã‚ã‚Š (Raw)ç´„ 0.36ç´„ 0.42MLP (è¦‹ã‹ã‘ä¸Š)ã‚«ãƒ³ãƒ‹ãƒ³ã‚°ãªã— (Clean)0.16460.1475PIGNNMLPã®å´©å£Š: ã‚«ãƒ³ãƒ‹ãƒ³ã‚°æƒ…å ±ã‚’æ¶ˆã—ãŸé€”ç«¯ã€F1ã‚¹ã‚³ã‚¢ãŒ 0.42 â†’ 0.1475 ã¸ã¨å£Šæ»…çš„ã«ä½ä¸‹ã—ã¾ã—ãŸã€‚ã“ã‚Œã¯MLPãŒã‚µãƒƒã‚«ãƒ¼ã‚’ç†è§£ã—ã¦ã„ãŸã®ã§ã¯ãªãã€å˜ã«ã€Œã‚´ãƒ¼ãƒ«ã«è¿‘ã„ã‹é ã„ã‹ã€ã¨ã„ã†1ç‚¹ã®ã¿ã§äºˆæ¸¬ã‚’ç«‹ã¦ã¦ã„ãŸè¨¼æ‹ ã§ã™ã€‚PIGNNã®å …ç‰¢æ€§: PIGNNã‚‚ä½ä¸‹ã¯ã—ã¦ã„ã¾ã™ãŒã€MLPã‚’ä¸Šå›ã‚‹ã‚¹ã‚³ã‚¢ã‚’ç¶­æŒã—ã¾ã—ãŸã€‚ã“ã‚Œã¯ã€ã‚´ãƒ¼ãƒ«ã¸ã®è·é›¢ãŒã‚ã‹ã‚‰ãªãã¦ã‚‚ã€ã€Œé¸æ‰‹é–“ã®ã¤ãªãŒã‚Šï¼ˆã‚¨ãƒƒã‚¸ï¼‰ã€ã‚„ã€Œé›†å›£ã®æ¨é€²åŠ›ï¼ˆé€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã€ã‹ã‚‰æˆ¦è¡“çš„æ–‡è„ˆã‚’è£œå®Œã—ã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚2. å†ç¾ç‡ï¼ˆRecallï¼‰ã«è¦‹ã‚‹ã€Œæˆ¦è¡“ç†è§£ã€ã®å·®ã“ã“ãŒæœ€ã‚‚é¢ç™½ã„ãƒã‚¤ãƒ³ãƒˆã§ã™ã€‚PIGNNã®Recall 0.89: ã‚«ãƒ³ãƒ‹ãƒ³ã‚°ãƒšãƒ¼ãƒ‘ãƒ¼ãŒãªãã¦ã‚‚ã€æˆåŠŸã‚·ãƒ¼ãƒ³ã® 89% ã‚’ã€Œã“ã‚Œã¯æˆåŠŸã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€ã¨æ¤œçŸ¥ã§ãã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€ã‚°ãƒ©ãƒ•æ§‹é€ ã«ã‚ˆã£ã¦ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®ã€Œå…†ã—ã€ã‚’æ‰ãˆã‚‹èƒ½åŠ›ãŒé«˜ã„ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚MLPã®Recall 0.23: æƒ…å ±ãŒæ¬ ã‘ã‚‹ã¨ã€æˆåŠŸã‚·ãƒ¼ãƒ³ã®ã‚ãšã‹ 23% ã—ã‹è¦‹ã¤ã‘ã‚‰ã‚Œãªããªã‚Šã¾ã—ãŸã€‚ã‚‚ã¯ã‚„äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æ©Ÿèƒ½ã—ã¦ã„ã¾ã›ã‚“ã€‚3. å’è«–ã«æ›¸ãã¹ãã€Œã‚­ãƒ©ãƒ¼è€ƒå¯Ÿã€ã“ã®çµæœã‚’ã‚°ãƒ©ãƒ•ã«ä¸¦ã¹ã¦ã€ä»¥ä¸‹ã®ã‚ˆã†ã«æ›¸ãåˆ‡ã£ã¦ãã ã•ã„ã€‚ã€Œå®Ÿé¨“ã®çµæœã€ã‚´ãƒ¼ãƒ«ã¸ã®è·é›¢æƒ…å ±ã‚’é®æ–­ã—ãŸæ¡ä»¶ä¸‹ã§ã¯ã€PIGNNã®F1ã‚¹ã‚³ã‚¢ï¼ˆ0.1646ï¼‰ãŒMLPï¼ˆ0.1475ï¼‰ã‚’é€†è»¢ã—ãŸã€‚ç‰¹ã«MLPã®å†ç¾ç‡ãŒ0.64ã‹ã‚‰0.23ã¸ã¨å¤§å¹…ã«ä¸‹è½ã—ãŸäº‹å®Ÿã¯ã€å¾“æ¥ã®MLPãƒ¢ãƒ‡ãƒ«ãŒç©ºé–“çš„ãªé…ç½®é–¢ä¿‚ã‚’å­¦ç¿’ã›ãšã€ç‰¹å®šã®æ”¯é…çš„ç‰¹å¾´é‡ã«éå­¦ç¿’ã—ã¦ã„ãŸã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚å¯¾ç…§çš„ã«PIGNNã¯ã€æƒ…å ±ã®æ¬ æä¸‹ã«ãŠã„ã¦ã‚‚0.89ã¨ã„ã†é«˜ã„å†ç¾ç‡ã‚’ç¶­æŒã—ã¦ãŠã‚Šã€ã“ã‚Œã¯ç‰©ç†åˆ¶ç´„ã‚’ä¼´ã†ã‚°ãƒ©ãƒ•ç•³ã¿è¾¼ã¿ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒã€é¸æ‰‹é–“ã®ç›¸äº’ä½œç”¨ã‚’é€šã˜ã¦é«˜åº¦ãªæˆ¦è¡“çš„æ–‡è„ˆã‚’è£œå®Œãƒ»æŠ½å‡ºã§ãã¦ã„ã‚‹ã“ã¨ã‚’æ•°å­¦çš„ã«è£ä»˜ã‘ã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚ã€"
      ],
      "metadata": {
        "id": "ANzYEKgQzA57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# 1. å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®å…¥åŠ› (å¾—ã‚‰ã‚ŒãŸæ•°å€¤ã‚’ä»£å…¥)\n",
        "# ==========================================\n",
        "# ã‚«ãƒ³ãƒ‹ãƒ³ã‚°ã‚ã‚Š (Raw) ã®æ™‚ã®æ•°å€¤ã‚’ä»¥å‰ã®çµæœã‹ã‚‰æ¨å®šãƒ»å…¥åŠ›ã—ã¦ãã ã•ã„\n",
        "# ã“ã“ã§ã¯æ¯”è¼ƒã®ãŸã‚ã«æ¦‚ç®—å€¤ã‚’è¨­å®šã—ã¦ã„ã¾ã™\n",
        "labels = ['PIGNN (Î±=1.0)', 'MLP Baseline']\n",
        "\n",
        "# F1ã‚¹ã‚³ã‚¢ã®å¤‰åŒ–\n",
        "f1_cheating_on = [0.3654, 0.4200]  # ã‚«ãƒ³ãƒ‹ãƒ³ã‚°ã‚ã‚Š\n",
        "f1_cheating_off = [0.1646, 0.1475] # ã‚«ãƒ³ãƒ‹ãƒ³ã‚°ãªã— (ä»Šå›ã®çµæœ)\n",
        "\n",
        "# å†ç¾ç‡ (Recall) ã®å¤‰åŒ–\n",
        "recall_cheating_on = [0.65, 0.64]   # æ¨å®šå€¤ï¼ˆPIGNNï¼‰, MLPï¼ˆãƒ¬ãƒãƒ¼ãƒˆå€¤ï¼‰\n",
        "recall_cheating_off = [0.89, 0.23]  # ä»Šå›ã®çµæœ\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "# ==========================================\n",
        "# 2. ã‚°ãƒ©ãƒ•æç”»ï¼šF1ã‚¹ã‚³ã‚¢ã®ä½ä¸‹æ¯”è¼ƒ\n",
        "# ==========================================\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# --- å·¦å›³ï¼šF1ã‚¹ã‚³ã‚¢ã®å¤‰åŒ– ---\n",
        "ax1.bar(x - width/2, f1_cheating_on, width, label='With Goal Distance', color='skyblue', alpha=0.6)\n",
        "ax1.bar(x + width/2, f1_cheating_off, width, label='Without Goal Distance', color='blue')\n",
        "ax1.set_ylabel('F1 Score')\n",
        "ax1.set_title('Robustness Comparison: F1 Score Drop')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(labels)\n",
        "ax1.legend()\n",
        "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# --- å³å›³ï¼šå†ç¾ç‡(Recall)ã®ç¶­æŒèƒ½åŠ› ---\n",
        "ax2.bar(x - width/2, recall_cheating_on, width, label='With Goal Distance', color='salmon', alpha=0.6)\n",
        "ax2.bar(x + width/2, recall_cheating_off, width, label='Without Goal Distance', color='red')\n",
        "ax2.set_ylabel('Recall (Success detection)')\n",
        "ax2.set_title('Robustness Comparison: Recall (Tactical Awareness)')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(labels)\n",
        "ax2.legend()\n",
        "ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 3. çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒãƒªãƒ¼å‡ºåŠ›\n",
        "# ==========================================\n",
        "print(\"--- è€ƒå¯Ÿç”¨ã‚µãƒãƒªãƒ¼ ---\")\n",
        "for i in range(len(labels)):\n",
        "    drop = (f1_cheating_on[i] - f1_cheating_off[i]) / f1_cheating_on[i] * 100\n",
        "    print(f\"{labels[i]}: F1ã‚¹ã‚³ã‚¢ä½ä¸‹ç‡ {drop:.1f}%\")"
      ],
      "metadata": {
        "id": "xC3No2xX1s2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. PFIï¼ˆç‰¹å¾´é‡é‡è¦åº¦ï¼‰ã®è§£é‡ˆï¼šåˆ¤æ–­åŸºæº–ã®è³ªã®å·®\n",
        "å·¦å´ã®ã‚°ãƒ©ãƒ•ï¼ˆPermutation Feature Importanceï¼‰ã‚’è¦‹ã‚‹ã¨ã€ä¸¡ãƒ¢ãƒ‡ãƒ«ã®ã€Œé ­ã®ä¸­ã€ã®é•ã„ãŒæ˜ç¢ºã§ã™ã€‚\n",
        "\n",
        "MLPã®ã€Œã‚«ãƒ³ãƒ‹ãƒ³ã‚°ã€ä¾å­˜: ã‚ªãƒ¬ãƒ³ã‚¸è‰²ã®ãƒãƒ¼ã‚’è¦‹ã‚‹ã¨ã€Distance_to_Goal ãŒæœ€å¤§ã®é‡è¦åº¦ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€MLPãŒãƒ—ãƒ¬ãƒ¼ã®è³ªã§ã¯ãªãã€Œå˜ã«ã‚´ãƒ¼ãƒ«ã«è¿‘ã„ã‹ã©ã†ã‹ã€ã¨ã„ã†çµæœè«–çš„ãªæŒ‡æ¨™ã«é ¼ã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "PIGNNã®ã€Œç‰©ç†çš„ãƒ—ãƒ­ã‚»ã‚¹ã€é‡è¦–: é’è‰²ã®ãƒãƒ¼ã¯ã€Position_X ã‚„ Velocity_X ã¨ã„ã£ãŸç§»å‹•ã«é–¢ã™ã‚‹ç‰©ç†é‡ã«é«˜ã„é‡è¦åº¦ã‚’ç½®ã„ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãŒã€Œã©ã†å‰é€²ã—ã¦ã„ã‚‹ã‹ã€ã¨ã„ã†æ¨é€²ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã—ã¦ã„ã‚‹è¨¼æ‹ ã§ã™ã€‚\n",
        "\n",
        "è² ã®é‡è¦åº¦ï¼ˆDistance_to_Ballï¼‰: PIGNNã«ãŠã„ã¦ã“ã®é …ç›®ãŒãƒã‚¤ãƒŠã‚¹ãªã®ã¯ã€ç‰¹å®šã®é¸æ‰‹ãŒãƒœãƒ¼ãƒ«ã«è¿‘ã„ã‹ã©ã†ã‹ã¨ã„ã†å€‹åˆ¥ã®æ•°å€¤ã«æƒ‘ã‚ã•ã‚Œãšã€ã‚°ãƒ©ãƒ•æ§‹é€ å…¨ä½“ã§çŠ¶æ³ã‚’åˆ¤æ–­ã—ã¦ã„ã‚‹ã€Œå …ç‰¢ã•ã€ã®è¡¨ã‚Œã¨è§£é‡ˆã§ãã¾ã™ã€‚\n",
        "\n",
        "2. å …ç‰¢æ€§æ¯”è¼ƒï¼ˆRobustness Comparisonï¼‰ï¼šçœŸã®å®ŸåŠ›\n",
        "å³å´ã®2ã¤ã®ã‚°ãƒ©ãƒ•ã¯ã€æƒ…å ±ã®æ¬ æï¼ˆã‚´ãƒ¼ãƒ«è·é›¢ãªã—ï¼‰ã«å¯¾ã™ã‚‹è€æ€§ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã“ã“ãŒæœ€å¤§ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆã§ã™ã€‚\n",
        "\n",
        "F1ã‚¹ã‚³ã‚¢ã®é€†è»¢ï¼ˆå·¦å›³ï¼‰: ã‚«ãƒ³ãƒ‹ãƒ³ã‚°æŒ‡æ¨™ï¼ˆGoal Distanceï¼‰ã‚’å¥ªã†ã¨ã€MLPã®ã‚¹ã‚³ã‚¢ã¯PIGNNã‚’ä¸‹å›ã‚‹ã¾ã§å¢œè½ã—ã¾ã—ãŸã€‚MLPã®ã€Œè¦‹ã‹ã‘ä¸Šã®é«˜ç²¾åº¦ã€ãŒã„ã‹ã«ç‰¹å®šã®ãƒ’ãƒ³ãƒˆã«ä¾å­˜ã—ã¦ã„ãŸã‹ãŒéœ²å‘ˆã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "å†ç¾ç‡ï¼ˆRecallï¼‰ã®é©šç•°çš„ãªç¶­æŒï¼ˆå³å›³ï¼‰: æœ€ã‚‚ç‰¹ç­†ã™ã¹ãã¯ã€ã‚«ãƒ³ãƒ‹ãƒ³ã‚°ãªã—ã®çŠ¶æ…‹ã§ PIGNNã®å†ç¾ç‡ãŒç´„0.9ï¼ˆ90%ï¼‰è¿‘ãã¾ã§ä¸Šæ˜‡ ã—ã¦ã„ã‚‹ç‚¹ã§ã™ã€‚ä¸€æ–¹ã€MLPã¯0.2ç¨‹åº¦ã¾ã§å´©å£Šã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "è§£é‡ˆ: PIGNNã¯ã€ã‚´ãƒ¼ãƒ«ä½ç½®ãŒã‚ã‹ã‚‰ãªãã¦ã‚‚ã€Œé¸æ‰‹ãŸã¡ãŒä¸€æ–‰ã«å‰æ–¹ã«åŠ é€Ÿã—ã¦ã„ã‚‹ï¼ˆã‚°ãƒ©ãƒ•æ§‹é€ ã®é€£å‹•ï¼‰ã€ã¨ã„ã†æƒ…å ±ã‹ã‚‰ã€ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®æˆåŠŸã‚’é«˜ã„ç¢ºç‡ã§è¦‹æŠœã‘ã¦ã„ã¾ã™ã€‚ã“ã‚Œã“ããŒ**ã€Œæˆ¦è¡“çš„æ–‡è„ˆï¼ˆTactical Awarenessï¼‰ã€ã‚’ç†è§£ã—ã¦ã„ã‚‹**ã¨ã„ã†è¨¼æ‹ ã§ã™ã€‚\n",
        "\n",
        "3. å’è«–ã§ã®ã€Œã‚­ãƒ©ãƒ¼è€ƒå¯Ÿã€æ¡ˆ\n",
        "ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ„ã¿åˆã‚ã›ã¦ã€ä»¥ä¸‹ã®ã‚ˆã†ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã§çµè«–ä»˜ã‘ã¾ã—ã‚‡ã†ã€‚\n",
        "\n",
        "ã€Œæœ¬å®Ÿé¨“ã®å …ç‰¢æ€§è©•ä¾¡ï¼ˆRobustness Comparisonï¼‰ã«ã‚ˆã‚Šã€MLPã®å„ªä½æ€§ã¯ç‰¹å®šã®æ”¯é…çš„ç‰¹å¾´é‡ï¼ˆDistance to Goalï¼‰ã¸ã®éå­¦ç¿’ã«èµ·å› ã™ã‚‹ã‚‚ã®ã§ã‚ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ãŸã€‚ å¯¾ç…§çš„ã«PIGNNã¯ã€æ”¯é…çš„ç‰¹å¾´é‡ã‚’é®æ–­ã—ãŸæ¡ä»¶ä¸‹ã§MLPã‚’å‡Œé§•ã™ã‚‹F1ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²ã—ã€ã•ã‚‰ã«ç´„90%ã¨ã„ã†æ¥µã‚ã¦é«˜ã„å†ç¾ç‡ã‚’ç¶­æŒã—ãŸã€‚ ã“ã‚Œã¯ã€ç‰©ç†åˆ¶ç´„ã‚’ä¼´ã†ã‚°ãƒ©ãƒ•ç•³ã¿è¾¼ã¿ãŒã€å˜ä¸€ã®æŒ‡æ¨™ã«ä¾å­˜ã›ãšã€é¸æ‰‹é–“ã®å‹•çš„ãªç›¸äº’ä½œç”¨ã‹ã‚‰ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®æˆ¦è¡“çš„æœ¬è³ªã‚’æŠ½å‡ºã§ãã¦ã„ã‚‹ã“ã¨ã‚’æ•°å­¦çš„ã«è¨¼æ˜ã™ã‚‹ã‚‚ã®ã§ã‚ã‚‹ã€‚ã€"
      ],
      "metadata": {
        "id": "InIBbplz2g3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã®å¯è¦–åŒ–"
      ],
      "metadata": {
        "id": "nNhUKPcP6W4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã®å¯è¦–åŒ–ã®ãŸã‚ã«ã¯ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’æˆ»ã‚Šå€¤ã¨ã—ã¦è¿”ã™ã‚ˆã†ã«ä¿®æ­£ãŒå¿…è¦ã€‚ãã®ãŸã‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’å†å®šç¾©ã™ã‚‹ã€‚"
      ],
      "metadata": {
        "id": "vysk9sK24FaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PIGNNLayer(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, tau=1.5):\n",
        "        super(PIGNNLayer, self).__init__(aggr='add')\n",
        "        self.tau = tau\n",
        "        self.lin = nn.Linear(in_channels, out_channels)\n",
        "        self.att = nn.Parameter(torch.Tensor(1, out_channels * 2))\n",
        "        nn.init.xavier_uniform_(self.att)\n",
        "\n",
        "    # --- å¼•æ•° return_attention ã‚’è¿½åŠ  ---\n",
        "    def forward(self, x, edge_index, pos, vel, return_attention=False):\n",
        "        h = self.lin(x)\n",
        "        # ãƒãƒ¼ãƒ ID(index 6)ã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‘ãƒƒã‚·ãƒ³ã‚°ã«æ¸¡ã™\n",
        "        out = self.propagate(edge_index, x=h, pos=pos, vel=vel, team=x[:, 6:7])\n",
        "\n",
        "        # å¯è¦–åŒ–ãƒ¢ãƒ¼ãƒ‰ã®æ™‚ã¯ã€å†…éƒ¨ã§è¨ˆç®—ã•ã‚ŒãŸ alpha ã‚’å–å¾—ã™ã‚‹\n",
        "        if return_attention:\n",
        "            # å†…éƒ¨å¤‰æ•°ã‚’ä¿æŒã™ã‚‹ãŸã‚ã«ä¸€æ™‚çš„ãªä¿å­˜ãŒå¿…è¦ã§ã™ãŒã€\n",
        "            # ã‚·ãƒ³ãƒ—ãƒ«ã«ã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯å‡ºåŠ›ã¨å…±ã«ã‚¨ãƒƒã‚¸ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ç›´è¿‘ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¿”ã›ã‚‹ã‚ˆã†ã«è¨­è¨ˆã—ã¾ã™\n",
        "            return out, (edge_index, self._last_att if hasattr(self, '_last_att') else None)\n",
        "        return out\n",
        "\n",
        "    def message(self, x_i, x_j, pos_i, pos_j, vel_i, vel_j, edge_index_i, team_i, team_j):\n",
        "        pos_i_pred = pos_i + vel_i * self.tau\n",
        "        pos_j_pred = pos_j + vel_j * self.tau\n",
        "        dist_future = torch.norm(pos_i_pred - pos_j_pred, dim=-1, keepdim=True)\n",
        "        physics_bias = torch.exp(-dist_future / 2.0)\n",
        "\n",
        "        is_teammate = (team_i == team_j).float()\n",
        "        team_bias = torch.where(is_teammate > 0.5, 0.5, -0.5)\n",
        "\n",
        "        alpha = torch.cat([x_i, x_j], dim=-1)\n",
        "        alpha = (alpha * self.att).sum(dim=-1, keepdim=True)\n",
        "        alpha = softmax(F.leaky_relu(alpha) + physics_bias + team_bias, edge_index_i)\n",
        "\n",
        "        # ã€å¯è¦–åŒ–ç”¨ã€‘è¨ˆç®—ã•ã‚ŒãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ä¸€æ™‚ä¿å­˜\n",
        "        self._last_att = alpha\n",
        "        return alpha * x_j\n",
        "\n",
        "class PIGNNClassifier(nn.Module):\n",
        "    def __init__(self, hidden_channels=64):\n",
        "        super(PIGNNClassifier, self).__init__()\n",
        "        self.conv1 = PIGNNLayer(7, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    # --- å¼•æ•° return_attention ã‚’è¿½åŠ  ---\n",
        "    def forward(self, data, return_attention=False):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        if return_attention:\n",
        "            # conv1 ã‹ã‚‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º\n",
        "            x, (edge_idx_out, att_weights) = self.conv1(x, edge_index, pos, vel, return_attention=True)\n",
        "            x = F.elu(x)\n",
        "            x = self.conv2(x, edge_index, pos, vel)\n",
        "        else:\n",
        "            x = F.elu(self.conv1(x, edge_index, pos, vel))\n",
        "            x = self.conv2(x, edge_index, pos, vel)\n",
        "\n",
        "        x_pool = global_mean_pool(x, batch)\n",
        "        logits = F.log_softmax(self.lin(x_pool), dim=1)\n",
        "\n",
        "        if return_attention:\n",
        "            return logits, (edge_idx_out, att_weights)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ExHECXGQ4Mkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "def visualize_pignn_tactical_analysis(model, data_item, device, title=\"PIGNN Tactical Analysis\"):\n",
        "    \"\"\"\n",
        "    PIGNNã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã¨ç‰©ç†çŠ¶æ…‹ï¼ˆåº§æ¨™ãƒ»é€Ÿåº¦ï¼‰ã‚’ãƒ”ãƒƒãƒä¸Šã«å¯è¦–åŒ–ã™ã‚‹\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # ãƒ‡ãƒ¼ã‚¿ã‚’1ã¤ã ã‘ã®ãƒãƒƒãƒã¨ã—ã¦æ‰±ã†\n",
        "    data_item = data_item.to(device)\n",
        "\n",
        "    # ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰äºˆæ¸¬çµæœã¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã‚’æŠ½å‡º\n",
        "    # â€» forwardãƒ¡ã‚½ãƒƒãƒ‰ãŒ return_attention=True ã§ (out, (edge_index, att_weights)) ã‚’è¿”ã™å‰æ\n",
        "    with torch.no_grad():\n",
        "        out, (edge_index, att_weights) = model(data_item, return_attention=True)\n",
        "        prob = torch.softmax(out, dim=1)[0, 1].item()\n",
        "        pred = out.argmax(dim=1).item()\n",
        "        label = data_item.y.item()\n",
        "\n",
        "    # --- 1. åº§æ¨™ã¨é€Ÿåº¦ã®å¾©å…ƒ ---\n",
        "    # æ­£è¦åŒ–ã•ã‚ŒãŸå€¤ (-1~1) ã‚’å®Ÿéš›ã®ãƒ”ãƒƒãƒã‚µã‚¤ã‚º (105m x 68m) ã«æˆ»ã™\n",
        "    pos = data_item.pos.cpu().numpy()\n",
        "    vel = data_item.vel.cpu().numpy()\n",
        "\n",
        "    pos_plot = np.zeros_like(pos)\n",
        "    pos_plot[:, 0] = pos[:, 0] * 52.5  # Xåº§æ¨™: -52.5 to 52.5\n",
        "    pos_plot[:, 1] = pos[:, 1] * 34.0  # Yåº§æ¨™: -34.0 to 34.0\n",
        "\n",
        "    # é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã®æç”»ç”¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆ1ç§’é–“ã®ç§»å‹•è·é›¢ã‚’å¼·èª¿ï¼‰\n",
        "    vel_plot = vel * 3.0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "    # --- 2. ã‚µãƒƒã‚«ãƒ¼å ´ã®æç”» (èŠç”Ÿã®è‰² #2e7d32) ---\n",
        "    ax.set_facecolor('#2e7d32')\n",
        "    # å¤–æ \n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=True, color='#388e3c', zorder=0))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=False, color='white', lw=3, zorder=1))\n",
        "\n",
        "    # ã‚»ãƒ³ã‚¿ãƒ¼ãƒ©ã‚¤ãƒ³ & ã‚»ãƒ³ã‚¿ãƒ¼ã‚µãƒ¼ã‚¯ãƒ«\n",
        "    ax.plot([0, 0], [-34, 34], color='white', lw=3, zorder=1)\n",
        "    ax.add_patch(patches.Circle((0, 0), 9.15, edgecolor=\"white\", facecolor=\"none\", lw=3, zorder=1))\n",
        "\n",
        "    # ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚¨ãƒªã‚¢\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((52.5-16.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "\n",
        "    # --- 3. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ï¼ˆé»„è‰²ã„å…‰ #FFFF00ï¼‰ã®æç”» ---\n",
        "    # å…¨ã‚¨ãƒƒã‚¸ã®ã†ã¡ã€å½±éŸ¿åŠ›ã®å¼·ã„ä¸Šä½ã®ã‚¨ãƒƒã‚¸ã‚’å…‰ã®ç·šã§è¡¨ç¾\n",
        "    att_weights = att_weights.cpu().numpy().flatten()\n",
        "    edge_index = edge_index.cpu().numpy()\n",
        "\n",
        "    if len(att_weights) > 0:\n",
        "        threshold = np.percentile(att_weights, 95) # ä¸Šä½â—‹%ã®ã‚¨ãƒƒã‚¸ã®ã¿è¡¨ç¤º\n",
        "        max_att = att_weights.max()\n",
        "        for i in range(edge_index.shape[1]):\n",
        "            if att_weights[i] > threshold:\n",
        "                src, dst = edge_index[0, i], edge_index[1, i]\n",
        "                # å¼·ã•ã«å¿œã˜ã¦é€æ˜åº¦(alpha)ã‚’å¤‰åŒ–ã•ã›ã‚‹\n",
        "                alpha_val = (att_weights[i] - threshold) / (max_att - threshold + 1e-9)\n",
        "                ax.plot([pos_plot[src, 0], pos_plot[dst, 0]],\n",
        "                        [pos_plot[src, 1], pos_plot[dst, 1]],\n",
        "                        color='#FFFF00', alpha=alpha_val * 0.7, lw=2.0 + alpha_val*3, zorder=2)\n",
        "\n",
        "    # --- 4. é¸æ‰‹ã¨ãƒœãƒ¼ãƒ«ã®æç”» ---\n",
        "    team_ids = data_item.x[:, 6].cpu().numpy() # 7æ¬¡å…ƒç›®ã®Team_Flagã‚’å–å¾—\n",
        "    num_nodes = pos.shape[0]\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        # ãƒãƒ¼ãƒ ã«å¿œã˜ãŸè‰²åˆ†ã‘\n",
        "        if team_ids[i] == 2.0: # ãƒœãƒ¼ãƒ« (Gold)\n",
        "            color, marker, size, z = 'gold', '*', 600, 15\n",
        "        elif team_ids[i] == 0.0: # æ”»æ’ƒãƒãƒ¼ãƒ  (Blue #0288d1)\n",
        "            color, marker, size, z = '#0288d1', 'o', 300, 10\n",
        "        else: # å®ˆå‚™ãƒãƒ¼ãƒ  (Red #d32f2f)\n",
        "            color, marker, size, z = '#d32f2f', 'o', 300, 10\n",
        "\n",
        "        # æœ¬ä½“æç”»\n",
        "        ax.scatter(pos_plot[i, 0], pos_plot[i, 1], c=color, marker=marker, s=size,\n",
        "                   edgecolors='white', linewidth=1.5, zorder=z)\n",
        "\n",
        "        # é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«çŸ¢å° (ç‰©ç†çš„æ¨é€²åŠ›ã®å¯è¦–åŒ–)\n",
        "        if team_ids[i] != 2.0: # é¸æ‰‹ã®ã¿çŸ¢å°ã‚’è¡¨ç¤º\n",
        "            ax.arrow(pos_plot[i, 0], pos_plot[i, 1], vel_plot[i, 0], vel_plot[i, 1],\n",
        "                     head_width=1.0, head_length=1.2, fc='white', ec='white', alpha=0.5, zorder=z-1)\n",
        "\n",
        "    # æƒ…å ±ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¤º\n",
        "    label_str = \"SUCCESS\" if label == 1 else \"FAILURE\"\n",
        "    pred_str = \"SUCCESS\" if pred == 1 else \"FAILURE\"\n",
        "    match_result = \"CORRECT\" if label == pred else \"INCORRECT\"\n",
        "\n",
        "    ax.set_title(f\"{title}\\nActual: {label_str} | Predicted: {pred_str} ({prob:.1%})\\nResult: {match_result}\",\n",
        "                 fontsize=18, fontweight='bold', pad=20)\n",
        "\n",
        "    ax.set_xlim(-60, 60)\n",
        "    ax.set_ylim(-40, 40)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 5. å®Ÿè¡Œï¼šæˆåŠŸã‚·ãƒ¼ãƒ³ã¨å¤±æ•—ã‚·ãƒ¼ãƒ³ã®è‡ªå‹•æŠ½å‡ºã¨å¯è¦–åŒ–\n",
        "# ==========================================\n",
        "def run_comparison_visualizer(model, data_list, device):\n",
        "    success_case = None\n",
        "    failure_case = None\n",
        "\n",
        "    model.eval()\n",
        "    for data in data_list:\n",
        "        with torch.no_grad():\n",
        "            out, _ = model(data.to(device), return_attention=True)\n",
        "            pred = out.argmax(dim=1).item()\n",
        "            label = data.y.item()\n",
        "\n",
        "            # AIãŒæ­£è§£ã—ãŸã‚±ãƒ¼ã‚¹ã‹ã‚‰1ã¤ãšã¤ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—\n",
        "            if pred == label:\n",
        "                if label == 1 and success_case is None:\n",
        "                    success_case = data\n",
        "                elif label == 0 and failure_case is None:\n",
        "                    failure_case = data\n",
        "\n",
        "        if success_case and failure_case:\n",
        "            break\n",
        "\n",
        "    if success_case:\n",
        "        visualize_pignn_tactical_analysis(model, success_case, device, title=\"Tactical Analysis: Successful Counter\")\n",
        "    if failure_case:\n",
        "        visualize_pignn_tactical_analysis(model, failure_case, device, title=\"Tactical Analysis: Failed Counter\")\n",
        "\n",
        "# å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰\n",
        "# 1. ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–\n",
        "pignn_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "\n",
        "# 2. é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰\n",
        "# alpha_param ã¯ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã«å®šç¾©ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€strict=False ã§ç„¡è¦–ã•ã›ã¾ã™\n",
        "pignn_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/alpha_1_0/pignn_testmatch_1.pth\"\n",
        "state_dict = torch.load(pignn_path, map_location=device)\n",
        "pignn_model.load_state_dict(state_dict, strict=False)\n",
        "print(\"âœ… é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†ã€‚å¯è¦–åŒ–æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚\")\n",
        "\n",
        "# 3. å¯è¦–åŒ–å®Ÿè¡Œ\n",
        "# å…ˆã»ã©ä½œæˆã—ãŸ visualize_pignn_attention_v3 ã‚„ run_comparison_visualizer ã‚’å®Ÿè¡Œ\n",
        "run_comparison_visualizer(pignn_model, all_data_list, device)"
      ],
      "metadata": {
        "id": "GHpvrXqn3ig2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã‚‚è¡¨ç¤º"
      ],
      "metadata": {
        "id": "ZL1PSDQx6nHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# ==========================================\n",
        "# 1. å¯è¦–åŒ–ãƒ¡ã‚¤ãƒ³é–¢æ•° (é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«æ”¹å–„ç‰ˆ)\n",
        "# ==========================================\n",
        "def visualize_pignn_tactical_analysis(model, data_item, device, title=\"PIGNN Tactical Analysis\"):\n",
        "    model.eval()\n",
        "    data_item = data_item.to(device)\n",
        "\n",
        "    # æ¨è«–ã¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æŠ½å‡º\n",
        "    with torch.no_grad():\n",
        "        out, (edge_index, att_weights) = model(data_item, return_attention=True)\n",
        "        prob = torch.softmax(out, dim=1)[0, 1].item()\n",
        "        pred = out.argmax(dim=1).item()\n",
        "        label = data_item.y.item()\n",
        "\n",
        "    # --- åº§æ¨™ã®å¾©å…ƒ ---\n",
        "    pos = data_item.pos.cpu().numpy()\n",
        "    vel = data_item.vel.cpu().numpy()\n",
        "    pos_plot = np.zeros_like(pos)\n",
        "    pos_plot[:, 0] = pos[:, 0] * 52.5\n",
        "    pos_plot[:, 1] = pos[:, 1] * 34.0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "    # --- ã‚µãƒƒã‚«ãƒ¼å ´ã®æç”» ---\n",
        "    ax.set_facecolor('#2e7d32')\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=True, color='#388e3c', zorder=0))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=False, color='white', lw=3, zorder=1))\n",
        "    ax.plot([0, 0], [-34, 34], color='white', lw=3, zorder=1)\n",
        "    ax.add_patch(patches.Circle((0, 0), 9.15, edgecolor=\"white\", facecolor=\"none\", lw=3, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((52.5-16.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "\n",
        "    # --- ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®æç”» (zorder=2) ---\n",
        "    att_weights = att_weights.cpu().numpy().flatten()\n",
        "    edge_index = edge_index.cpu().numpy()\n",
        "\n",
        "    if len(att_weights) > 0:\n",
        "        threshold = np.percentile(att_weights, 98) # ä¸Šä½5%ã‚’è¡¨ç¤º\n",
        "        max_att = att_weights.max()\n",
        "        for i in range(edge_index.shape[1]):\n",
        "            if att_weights[i] > threshold:\n",
        "                src, dst = edge_index[0, i], edge_index[1, i]\n",
        "                alpha_val = (att_weights[i] - threshold) / (max_att - threshold + 1e-9)\n",
        "                ax.plot([pos_plot[src, 0], pos_plot[dst, 0]],\n",
        "                        [pos_plot[src, 1], pos_plot[dst, 1]],\n",
        "                        color='#FFFF00', alpha=alpha_val * 0.8, lw=2.0 + alpha_val*4, zorder=2)\n",
        "\n",
        "    # --- é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã¨é¸æ‰‹ã®æç”» (zorder=10-20) ---\n",
        "    team_ids = data_item.x[:, 6].cpu().numpy()\n",
        "    num_nodes = pos.shape[0]\n",
        "\n",
        "    # ğŸš€ é€Ÿåº¦æç”»ç”¨ã®è¨­å®š\n",
        "    vel_scale = 20.0 # çŸ¢å°ã‚’ã¯ã£ãã‚Šè¦‹ã›ã‚‹ãŸã‚ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        if team_ids[i] == 2.0: # ãƒœãƒ¼ãƒ«\n",
        "            color, marker, size, z = 'gold', '*', 600, 15\n",
        "        elif team_ids[i] == 0.0: # æ”»æ’ƒ\n",
        "            color, marker, size, z = '#0288d1', 'o', 300, 10\n",
        "        else: # å®ˆå‚™\n",
        "            color, marker, size, z = '#d32f2f', 'o', 300, 10\n",
        "\n",
        "        # ğŸš€ æ”¹å–„ï¼šax.quiver ã§é€Ÿåº¦ã‚’æœ€å‰é¢ã«æç”» (zorder=20)\n",
        "        if team_ids[i] != 2.0:\n",
        "            ax.quiver(pos_plot[i, 0], pos_plot[i, 1],\n",
        "                      vel[i, 0], vel[i, 1],\n",
        "                      color='white', alpha=0.9,\n",
        "                      angles='xy', scale_units='xy', scale=1/vel_scale,\n",
        "                      width=0.005, headwidth=4, headlength=5, zorder=20)\n",
        "\n",
        "        # é¸æ‰‹ãƒãƒ¼ãƒ‰æœ¬ä½“\n",
        "        ax.scatter(pos_plot[i, 0], pos_plot[i, 1], c=color, marker=marker, s=size,\n",
        "                   edgecolors='white', linewidth=1.5, zorder=15)\n",
        "\n",
        "    # ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¤º\n",
        "    res_text = \"SUCCESS\" if pred == 1 else \"FAILURE\"\n",
        "    match_status = \"CORRECT\" if label == pred else \"INCORRECT\"\n",
        "    ax.set_title(f\"{title}\\nActual: {'SUCCESS' if label==1 else 'FAILURE'} | Predicted: {res_text} ({prob:.1%})\\nResult: {match_status}\",\n",
        "                 fontsize=18, fontweight='bold', pad=20)\n",
        "\n",
        "    ax.set_xlim(-60, 60); ax.set_ylim(-40, 40)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 2. è‡ªå‹•æŠ½å‡º & å®Ÿè¡Œãƒ«ãƒ¼ãƒ—\n",
        "# ==========================================\n",
        "def run_comparison_visualizer(model, data_list, device):\n",
        "    success_case, failure_case = None, None\n",
        "    model.eval()\n",
        "\n",
        "    for data in data_list:\n",
        "        with torch.no_grad():\n",
        "            # ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€\n",
        "            d_gpu = data.to(device)\n",
        "            out, _ = model(d_gpu, return_attention=True)\n",
        "            pred = out.argmax(dim=1).item()\n",
        "            label = d_gpu.y.item()\n",
        "\n",
        "            # æ­£è§£ã‚·ãƒ¼ãƒ³ã®ä¸­ã‹ã‚‰æŠ½å‡º\n",
        "            if pred == label:\n",
        "                if label == 1 and success_case is None: success_case = data\n",
        "                elif label == 0 and failure_case is None: failure_case = data\n",
        "\n",
        "        if success_case and failure_case: break\n",
        "\n",
        "    if success_case:\n",
        "        visualize_pignn_tactical_analysis(model, success_case, device, title=\"Tactical Analysis: Successful Counter\")\n",
        "    if failure_case:\n",
        "        visualize_pignn_tactical_analysis(model, failure_case, device, title=\"Tactical Analysis: Failed Counter\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ & å®Ÿè¡Œ\n",
        "# ==========================================\n",
        "# 1. ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–\n",
        "pignn_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "\n",
        "# 2. ãƒ­ãƒ¼ãƒ‰ (strict=False)\n",
        "pignn_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/alpha_1_0/pignn_testmatch_1.pth\"\n",
        "if os.path.exists(pignn_path):\n",
        "    state_dict = torch.load(pignn_path, map_location=device)\n",
        "    pignn_model.load_state_dict(state_dict, strict=False)\n",
        "    print(\"âœ… PIGNNé‡ã¿ã®ãƒ­ãƒ¼ãƒ‰ã«æˆåŠŸã—ã¾ã—ãŸã€‚\")\n",
        "    # 3. å®Ÿè¡Œ\n",
        "    run_comparison_visualizer(pignn_model, all_data_list, device)\n",
        "else:\n",
        "    print(f\"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pignn_path}\")"
      ],
      "metadata": {
        "id": "j-H8O2ef6pWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®æ•°å€¤ã®è¿½åŠ "
      ],
      "metadata": {
        "id": "_3bO8VAW-KLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "import os\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# ==========================================\n",
        "# 1. å¯è¦–åŒ– & æ•°å€¤æŠ½å‡ºãƒ¡ã‚¤ãƒ³é–¢æ•°\n",
        "# ==========================================\n",
        "def visualize_pignn_tactical_analysis(model, data_item, device, title=\"PIGNN Tactical Analysis\"):\n",
        "    model.eval()\n",
        "    data_item = data_item.to(device)\n",
        "\n",
        "    # --- æ¨è«–ã¨ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æŠ½å‡º ---\n",
        "    with torch.no_grad():\n",
        "        out, (edge_index, att_weights) = model(data_item, return_attention=True)\n",
        "        prob = torch.softmax(out, dim=1)[0, 1].item()\n",
        "        pred = out.argmax(dim=1).item()\n",
        "        label = data_item.y.item()\n",
        "\n",
        "    # --- åº§æ¨™ã¨é€Ÿåº¦ã®å¾©å…ƒ ---\n",
        "    pos = data_item.pos.cpu().numpy()\n",
        "    vel = data_item.vel.cpu().numpy()\n",
        "    pos_plot = np.zeros_like(pos)\n",
        "    pos_plot[:, 0] = pos[:, 0] * 52.5\n",
        "    pos_plot[:, 1] = pos[:, 1] * 34.0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "    # --- ã‚µãƒƒã‚«ãƒ¼å ´ã®æç”» (zorder=0-1) ---\n",
        "    ax.set_facecolor('#2e7d32')\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=True, color='#388e3c', zorder=0))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=False, color='white', lw=3, zorder=1))\n",
        "    ax.plot([0, 0], [-34, 34], color='white', lw=3, zorder=1)\n",
        "    ax.add_patch(patches.Circle((0, 0), 9.15, edgecolor=\"white\", facecolor=\"none\", lw=3, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((52.5-16.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "\n",
        "    # --- 2. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã®æç”» & æ•°å€¤å‡ºåŠ› (zorder=2) ---\n",
        "    att_weights = att_weights.cpu().numpy().flatten()\n",
        "    edge_index = edge_index.cpu().numpy()\n",
        "\n",
        "    if len(att_weights) > 0:\n",
        "        threshold = np.percentile(att_weights, 98) # ä¸Šä½2%ã«çµã‚Šè¾¼ã¿\n",
        "        max_att = att_weights.max()\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"ğŸ“Š {title} - TOP 2% ATTENTION DETAILS\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"{'Source':<8} | {'Dest':<8} | {'Weight':<10} | {'Team Relation'}\")\n",
        "        print(f\"{'-'*50}\")\n",
        "\n",
        "        # ãƒãƒ¼ãƒ IDå–å¾— (0:æ”»æ’ƒ, 1:å®ˆå‚™, 2:ãƒœãƒ¼ãƒ«)\n",
        "        team_ids = data_item.x[:, 6].cpu().numpy()\n",
        "\n",
        "        for i in range(edge_index.shape[1]):\n",
        "            if att_weights[i] > threshold:\n",
        "                src, dst = edge_index[0, i], edge_index[1, i]\n",
        "                weight = att_weights[i]\n",
        "\n",
        "                # ãƒãƒ¼ãƒ é–¢ä¿‚ã®è¨€èªåŒ–\n",
        "                rel = \"Teammate\" if team_ids[src] == team_ids[dst] else \"Opponent\"\n",
        "                if team_ids[src] == 2.0: rel = \"Ball -> Player\"\n",
        "\n",
        "                # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«æ•°å€¤ã‚’è¡¨ç¤º\n",
        "                print(f\"Node {src:2d} -> Node {dst:2d} | {weight:.4f}     | {rel}\")\n",
        "\n",
        "                # æç”»\n",
        "                alpha_val = (weight - threshold) / (max_att - threshold + 1e-9)\n",
        "                ax.plot([pos_plot[src, 0], pos_plot[dst, 0]],\n",
        "                        [pos_plot[src, 1], pos_plot[dst, 1]],\n",
        "                        color='#FFFF00', alpha=alpha_val * 0.8, lw=2.0 + alpha_val*4, zorder=2)\n",
        "\n",
        "    # --- 3. é¸æ‰‹ã¨é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã®æç”» (zorder=10-20) ---\n",
        "    num_nodes = pos.shape[0]\n",
        "    vel_scale = 15.0 # é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã®è¦–èªæ€§ã‚’ç¢ºä¿\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        if team_ids[i] == 2.0: # ãƒœãƒ¼ãƒ«\n",
        "            color, marker, size, z = 'gold', '*', 600, 15\n",
        "        elif team_ids[i] == 0.0: # æ”»æ’ƒãƒãƒ¼ãƒ \n",
        "            color, marker, size, z = '#0288d1', 'o', 300, 10\n",
        "        else: # å®ˆå‚™ãƒãƒ¼ãƒ \n",
        "            color, marker, size, z = '#d32f2f', 'o', 300, 10\n",
        "\n",
        "        # é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ« (ax.quiver ã§ç¢ºå®Ÿã«æç”»)\n",
        "        if team_ids[i] != 2.0:\n",
        "            ax.quiver(pos_plot[i, 0], pos_plot[i, 1],\n",
        "                      vel[i, 0], vel[i, 1],\n",
        "                      color='white', alpha=0.9,\n",
        "                      angles='xy', scale_units='xy', scale=1/vel_scale,\n",
        "                      width=0.005, headwidth=4, headlength=5, zorder=20)\n",
        "\n",
        "        # é¸æ‰‹ãƒãƒ¼ãƒ‰\n",
        "        ax.scatter(pos_plot[i, 0], pos_plot[i, 1], c=color, marker=marker, s=size,\n",
        "                   edgecolors='white', linewidth=1.5, zorder=15)\n",
        "\n",
        "    # --- ã‚¿ã‚¤ãƒˆãƒ«ã¨è¡¨ç¤ºè¨­å®š ---\n",
        "    res_text = \"SUCCESS\" if pred == 1 else \"FAILURE\"\n",
        "    match_status = \"CORRECT\" if label == pred else \"INCORRECT\"\n",
        "    ax.set_title(f\"{title}\\nActual: {'SUCCESS' if label==1 else 'FAILURE'} | Predicted: {res_text} ({prob:.1%})\\nResult: {match_status}\",\n",
        "                 fontsize=18, fontweight='bold', pad=20)\n",
        "\n",
        "    ax.set_xlim(-60, 60); ax.set_ylim(-40, 40)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 2. è‡ªå‹•æ¯”è¼ƒå®Ÿè¡Œãƒ«ãƒ¼ãƒ—\n",
        "# ==========================================\n",
        "def run_comparison_visualizer(model, data_list, device):\n",
        "    success_case, failure_case = None, None\n",
        "    model.eval()\n",
        "\n",
        "    for data in data_list:\n",
        "        with torch.no_grad():\n",
        "            d_gpu = data.to(device)\n",
        "            out, _ = model(d_gpu, return_attention=True)\n",
        "            pred = out.argmax(dim=1).item()\n",
        "            label = d_gpu.y.item()\n",
        "\n",
        "            if pred == label:\n",
        "                if label == 1 and success_case is None: success_case = data\n",
        "                elif label == 0 and failure_case is None: failure_case = data\n",
        "\n",
        "        if success_case and failure_case: break\n",
        "\n",
        "    if success_case:\n",
        "        visualize_pignn_tactical_analysis(model, success_case, device, title=\"Tactical Analysis: Successful Counter\")\n",
        "    if failure_case:\n",
        "        visualize_pignn_tactical_analysis(model, failure_case, device, title=\"Tactical Analysis: Failed Counter\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³\n",
        "# ==========================================\n",
        "# PIGNNãƒ¢ãƒ‡ãƒ«ã®æº–å‚™\n",
        "pignn_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "\n",
        "pignn_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/alpha_1_0/pignn_testmatch_1.pth\"\n",
        "if os.path.exists(pignn_path):\n",
        "    state_dict = torch.load(pignn_path, map_location=device)\n",
        "    pignn_model.load_state_dict(state_dict, strict=False)\n",
        "    print(\"âœ… PIGNNé‡ã¿ã®ãƒ­ãƒ¼ãƒ‰ã«æˆåŠŸã—ã¾ã—ãŸã€‚\")\n",
        "\n",
        "    # å®Ÿè¡Œ\n",
        "    run_comparison_visualizer(pignn_model, all_data_list, device)\n",
        "else:\n",
        "    print(f\"âš ï¸ ãƒ‘ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pignn_path}\")"
      ],
      "metadata": {
        "id": "fQrj-UNc-OaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "æˆåŠŸã‚·ãƒ¼ãƒ³ã‚’è¦‹ã‚‹ã¨ã€ä¸­å¤®ã§è£ã¸èµ°ã‚Šã“ã‚‚ã†ã¨ã—ã¦ã„ã‚‹é¸æ‰‹ã«å¼·ã„ã‚¨ãƒƒã‚¸ãŒå¼µã‚‰ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã‚‹â†’PIGNNã§ãã®ã‚ˆã†ãªæˆ¦è¡“çš„ãªå‹•ãã«æ³¨ç›®ã§ããŸã®ã§ã¯ãªã„ã‹\n",
        "é€†ã«å¤±æ•—ã‚·ãƒ¼ãƒ³ã‚’è¦‹ã‚‹ã¨ã€æˆåŠŸã‚·ãƒ¼ãƒ³ã®ã‚ˆã†ã«ç‰¹å®šã®é¸æ‰‹ã¸ã®é›†ä¸­ã—ãŸã‚¨ãƒƒã‚¸ã®å¼µã‚ŠãŒãªã„ã€‚â†’ã“ã‚Œã¯ãƒ¢ãƒ‡ãƒ«ãŒã©ã“ã«æ³¨ç›®ã™ã‚‹ã¹ãã‹åˆ¤åˆ¥ã§ããªã‹ã£ãŸã®ã§ã¯ãªã„ã‹\n",
        "ï¼ˆä¸Šä½ï¼™ï¼˜ï¼…ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ã‚¨ãƒƒã‚¸ã‚’æç”»ï¼‰"
      ],
      "metadata": {
        "id": "ZQo6MIyL8X_E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. æˆåŠŸã‚·ãƒ¼ãƒ³ï¼ˆSuccessful Counterï¼‰ã®æ ¸å¿ƒï¼šæ±ºå®šçš„ãªã€Œå‚ç›´æ€§ã€é–¾å€¤ã‚’ä¸Šã’ãŸã“ã¨ã§ã€ãƒœãƒ¼ãƒ«ä¿æŒè€…ï¼ˆâ˜…ï¼‰ã‹ã‚‰ä¼¸ã³ã‚‹ã‚¨ãƒƒã‚¸ãŒã€å³æ–¹å‘ï¼ˆã‚´ãƒ¼ãƒ«æ–¹å‘ï¼‰ã¸ã®ç‰¹å®šã®å‘³æ–¹é¸æ‰‹ã«å®Œå…¨ã«é›†ç´„ã•ã‚Œã¾ã—ãŸã€‚æˆ¦è¡“çš„ãƒ•ã‚©ãƒ¼ã‚«ã‚¹:æ˜Ÿå°ï¼ˆãƒœãƒ¼ãƒ«ï¼‰ã‹ã‚‰å³ä¸‹æ–¹å‘ã¸ä¼¸ã³ã‚‹æ¥µå¤ªã®1æœ¬ã®ã‚¨ãƒƒã‚¸ã«æ³¨ç›®ã—ã¦ãã ã•ã„ã€‚AIã¯ã“ã®å±€é¢ã«ãŠã„ã¦ã€ä»–ã®ã©ã®é¸æ‰‹ã¨ã®é–¢ä¿‚ã‚ˆã‚Šã‚‚ã€Œã“ã®ä½ç½®ã«èµ°ã‚Šè¾¼ã‚“ã§ã„ã‚‹é¸æ‰‹ã¸ã®ãƒ‘ã‚¹ã€ã‚’æˆåŠŸã®æ±ºå®šæ‰“ã¨ã—ã¦è©•ä¾¡ã—ã¦ã„ã¾ã™ã€‚ç‰©ç†åˆ¶ç´„ã®å…·ç¾åŒ–:ã“ã®æ¥µå¤ªã®ã‚¨ãƒƒã‚¸ã§çµã°ã‚ŒãŸé¸æ‰‹ã¯ã€ç™½ã„çŸ¢å°ï¼ˆé€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã‚‚ã‚´ãƒ¼ãƒ«æ–¹å‘ã«å‘ã„ã¦ã„ã¾ã™ã€‚ã¤ã¾ã‚Šã€PIGNNã¯ã€Œé™æ­¢ã—ã¦ã„ã‚‹è¿‘ã„é¸æ‰‹ã€ã‚’ç„¡è¦–ã—ã€ã€Œã‚¹ãƒšãƒ¼ã‚¹ã¸åŠ é€Ÿã—ã¦ã„ã‚‹é¸æ‰‹ã€ã‚’æ”»æ’ƒã®ã‚­ãƒ¼ãƒãƒ³ã¨ã—ã¦æŠ½å‡ºã™ã‚‹ã“ã¨ã«æˆåŠŸã—ã¦ã„ã¾ã™ã€‚æƒ…å ±ã®ç´”åŒ–:ã€Œãã‚‚ã•ã€ãŒæ¶ˆãˆãŸã®ã¯ã€ãƒã‚¤ã‚ºã¨ãªã£ã¦ã„ãŸå¼±ã„é€£æºãŒæ’é™¤ã•ã‚Œã€PIGNNãŒç¢ºä¿¡ã—ã¦ã„ã‚‹ã€Œæˆ¦è¡“ã®æ­£è§£ã€ã ã‘ãŒæ®‹ã£ãŸãŸã‚ã§ã™ã€‚2. å¤±æ•—ã‚·ãƒ¼ãƒ³ï¼ˆFailed Counterï¼‰ã®ç©ºè™šã•åŒã˜98%ã®è¨­å®šã§ã‚ã‚ŠãªãŒã‚‰ã€å¤±æ•—ã‚·ãƒ¼ãƒ³ã§ã¯ã€Œå¼·ã„ã‚¨ãƒƒã‚¸ã€ã®ç¾ã‚Œæ–¹ãŒå…¨ãç•°ãªã‚Šã¾ã™ã€‚å¼·åº¦ã®æ¬ å¦‚:æˆåŠŸã‚·ãƒ¼ãƒ³ã®ã‚ˆã†ãªã€Œå…‰ã‚Šè¼ãæ¥µå¤ªã®ç·šã€ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ã€‚æ®‹ã£ã¦ã„ã‚‹ç·šã‚‚ç´°ãã€ã‹ã¤ãƒœãƒ¼ãƒ«ã®å‘¨å›²ã§åœæ»ã—ã¦ã„ã¾ã™ã€‚å®¢è¦³çš„åˆ¤å®šåŸºæº–:AIãŒã€Œäºˆæ¸¬ç¢ºç‡ 46.4%ã€ã¨å‡ºã—ãŸã®ã¯ã€**ã€Œä¸Šä½2%ã«çµã‚Šè¾¼ã‚“ã§ã‚‚ã€æˆåŠŸã‚’ç¢ºä¿¡ã•ã›ã‚‹ã»ã©ã®å¼·åŠ›ãªé€£æºãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã€**ã¨ã„ã†æ•°å­¦çš„ãªçµè«–ã‚’ãã®ã¾ã¾åæ˜ ã—ã¦ã„ã¾ã™ã€‚3. å’è«–ã§ä¸»å¼µã™ã¹ãã€Œå®šé‡çš„ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã€ã“ã®é–¾å€¤å¤‰æ›´å¾Œã®ç”»åƒã‚’ä¸¦ã¹ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ã®å¼·åŠ›ãªè€ƒå¯ŸãŒæ›¸ã‘ã¾ã™ã€‚ã€Œã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å°–åº¦ï¼ˆé«˜ã¾ã‚Šï¼‰ã€:ã€ŒæˆåŠŸã‚·ãƒ¼ãƒ³ã§ã¯ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãŒç‰¹å®šã®æˆ¦è¡“çš„ã‚­ãƒ¼ãƒãƒ³ã«**é‹­ãé›†ä¸­ï¼ˆSparse Attentionï¼‰**ã™ã‚‹ã®ã«å¯¾ã—ã€å¤±æ•—ã‚·ãƒ¼ãƒ³ã§ã¯ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãŒåˆ†æ•£ã—ã€å¼·åº¦ãŒä½ã„ã€‚ã“ã‚Œã¯PIGNNãŒæˆ¦è¡“ã®ã€æˆå¦ã®åˆ†å²ç‚¹ã€ã‚’æ˜ç¢ºã«è­˜åˆ¥ã—ã¦ã„ã‚‹è¨¼æ‹ ã§ã‚ã‚‹ã€ã€Œç‰©ç†çš„æ ¹æ‹ ã®é€æ˜åŒ–ã€:ã€Œé–¾å€¤ã‚’å³æ ¼åŒ–ã™ã‚‹ã“ã¨ã§ã€ç‰©ç†æå¤± $L_{phys}$ ãŒã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã‚’é€šã˜ã¦ã€å‰å‘ãã®é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŒã¤ãƒãƒ¼ãƒ‰é–“ã®é–¢ä¿‚ã‚’å„ªå…ˆçš„ã«å¼·åŒ–ã—ã¦ã„ã‚‹ã“ã¨ãŒè¦–è¦šçš„ã«è¨¼æ˜ã•ã‚ŒãŸã€"
      ],
      "metadata": {
        "id": "MM_ULn6d85oE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä»¥ä¸Šã€‚"
      ],
      "metadata": {
        "id": "3zPyguqY9lAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ä»¥ä¸‹ã¯V3ã‚ˆã‚Šå‰ã®ã‚‚ã®ãŸã¡"
      ],
      "metadata": {
        "id": "_fjSoPLQ9nty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def visualize_pignn_attention_v3(model, data_item, title=\"PIGNN Tactical Analysis\"):\n",
        "    model.eval()\n",
        "    # 6æ¬¡å…ƒåŒ– & ãƒ‡ãƒã‚¤ã‚¹è»¢é€\n",
        "    data_item = preprocess_batch(data_item, device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out, (edge_index, att_weights) = model(data_item, return_attention=True)\n",
        "        prob = torch.exp(out)[0, 1].item()\n",
        "        pred = out.argmax(dim=1).item()\n",
        "\n",
        "    # --- 1. åº§æ¨™ã®å¾©å…ƒ (-1~1 -> -52.5~52.5, -34~34) ---\n",
        "    pos = data_item.pos.cpu().numpy()\n",
        "    vel = data_item.vel.cpu().numpy()\n",
        "\n",
        "    pos_plot = np.zeros_like(pos)\n",
        "    pos_plot[:, 0] = pos[:, 0] * 52.5\n",
        "    pos_plot[:, 1] = pos[:, 1] * 34\n",
        "\n",
        "    # é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆè¦‹ã‚„ã™ãã™ã‚‹ãŸã‚ã«èª¿æ•´ï¼‰\n",
        "    vel_plot = vel * 5.0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "    # --- 2. ã‚µãƒƒã‚«ãƒ¼å ´ã®æç”» (ç·‘èƒŒæ™¯ + ç™½ç·š) ---\n",
        "    ax.set_facecolor('#2e7d32') # èŠç”Ÿã®è‰²\n",
        "    # ãƒ”ãƒƒãƒå¤–æ \n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=True, color='#388e3c', zorder=0))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=False, color='white', lw=3, zorder=1))\n",
        "\n",
        "    # ã‚»ãƒ³ã‚¿ãƒ¼ãƒ©ã‚¤ãƒ³ & ã‚»ãƒ³ã‚¿ãƒ¼ã‚µãƒ¼ã‚¯ãƒ«\n",
        "    ax.plot([0, 0], [-34, 34], color='white', lw=3, zorder=1)\n",
        "    ax.add_patch(patches.Circle((0, 0), 9.15, edgecolor=\"white\", facecolor=\"none\", lw=3, zorder=1))\n",
        "\n",
        "    # ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚¨ãƒªã‚¢ (å·¦)\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "    # ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚¨ãƒªã‚¢ (å³)\n",
        "    ax.add_patch(patches.Rectangle((52.5-16.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "\n",
        "    # --- 3. ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ä¿‚æ•°ã®æç”» (é»„è‰²ã„å…‰ã®ç·š) ---\n",
        "    att_weights = att_weights.cpu().numpy().flatten()\n",
        "    edge_index = edge_index.cpu().numpy()\n",
        "\n",
        "    if len(att_weights) > 0:\n",
        "        threshold = np.percentile(att_weights, 95) # ä¸Šä½7%ã«çµã‚‹\n",
        "        max_att = att_weights.max()\n",
        "        for i in range(edge_index.shape[1]):\n",
        "            if att_weights[i] > threshold:\n",
        "                src, dst = edge_index[0, i], edge_index[1, i]\n",
        "                alpha_val = (att_weights[i] - threshold) / (max_att - threshold + 1e-9)\n",
        "                ax.plot([pos_plot[src, 0], pos_plot[dst, 0]], [pos_plot[src, 1], pos_plot[dst, 1]],\n",
        "                        color='#FFFF00', alpha=alpha_val * 0.8, lw=2.5, zorder=2)\n",
        "\n",
        "    # --- 4. ãƒãƒ¼ãƒ‰ (é¸æ‰‹ & ãƒœãƒ¼ãƒ«) ã®æç”» ---\n",
        "    num_nodes = pos.shape[0]\n",
        "    for i in range(num_nodes):\n",
        "        if i == num_nodes - 1: # ãƒœãƒ¼ãƒ«\n",
        "            color, marker, size, z = 'gold', '*', 500, 15\n",
        "        elif i < 11: # å‘³æ–¹ (Blue)\n",
        "            color, marker, size, z = '#0288d1', 'o', 250, 10\n",
        "        elif i < 22: # æ•µ (Red)\n",
        "            color, marker, size, z = '#d32f2f', 'o', 250, 10\n",
        "        else:\n",
        "            color, marker, size, z = 'gray', 'o', 100, 5\n",
        "\n",
        "        # é¸æ‰‹ãƒ»ãƒœãƒ¼ãƒ«æœ¬ä½“ (å½±ã‚’ã¤ã‘ã¦ç«‹ä½“çš„ã«)\n",
        "        ax.scatter(pos_plot[i, 0], pos_plot[i, 1], c=color, marker=marker, s=size,\n",
        "                   edgecolors='white', linewidth=1.5, zorder=z)\n",
        "\n",
        "        # é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ« (é»’ã„çŸ¢å°ã§å‹•ãã®æ–¹å‘ã‚’æ˜ç¢ºåŒ–)\n",
        "        ax.arrow(pos_plot[i, 0], pos_plot[i, 1], vel_plot[i, 0], vel_plot[i, 1],\n",
        "                 head_width=1.2, head_length=1.5, fc='white', ec='white', alpha=0.6, zorder=z-1)\n",
        "\n",
        "    # ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±\n",
        "    res_text = \"SUCCESS\" if pred == 1 else \"FAILURE\"\n",
        "    ax.set_title(f\"{title}\\nPredicted: {res_text} (Prob: {prob:.2%})\",\n",
        "                 fontsize=20, color='black', fontweight='bold', pad=20)\n",
        "\n",
        "    ax.set_xlim(-60, 60)\n",
        "    ax.set_ylim(-40, 40)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "zNW6rcuqszB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- å®Ÿè¡Œ ---\n",
        "# loaderã‹ã‚‰1ãƒãƒƒãƒå–å¾—ã—ã€ãã®ä¸­ã®æœ€åˆã®ãƒ‡ãƒ¼ã‚¿(sample_idx=0)ã‚’å¯è¦–åŒ–\n",
        "batch = next(iter(test_loader))\n",
        "data_list = batch.to_data_list()\n",
        "sample_data = data_list[0] # ã“ã“ã§è¦‹ãŸã„ã‚·ãƒ¼ãƒ³ã®ç•ªå·ã‚’é¸ã¶\n",
        "\n",
        "visualize_pignn_attention_v3(model, sample_data)"
      ],
      "metadata": {
        "id": "4ik_y0WRqdum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- å®Ÿè¡Œ ---\n",
        "# loaderã‹ã‚‰1ãƒãƒƒãƒå–å¾—ã—ã€ãã®ä¸­ã®æœ€åˆã®ãƒ‡ãƒ¼ã‚¿(sample_idx=0)ã‚’å¯è¦–åŒ–\n",
        "batch = next(iter(test_loader))\n",
        "data_list = batch.to_data_list()\n",
        "sample_data = data_list[7] # ã“ã“ã§è¦‹ãŸã„ã‚·ãƒ¼ãƒ³ã®ç•ªå·ã‚’é¸ã¶\n",
        "\n",
        "visualize_pignn_attention_v3(model, sample_data)"
      ],
      "metadata": {
        "id": "yoGFM6kStsdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ä¾‹ï¼š3æšç›®ã®ãƒãƒƒãƒï¼ˆã‚‚ã—ãƒãƒƒãƒã‚µã‚¤ã‚º64ãªã‚‰ 129ç•ªç›®ã€œ192ç•ªç›®ã®ã‚·ãƒ¼ãƒ³ï¼‰ã‚’æŠ½å‡º\n",
        "batch_idx_to_view = 2  # 0ã‹ã‚‰æ•°ãˆã‚‹ã®ã§ã€2ã¯3æšç›®\n",
        "\n",
        "for i, b in enumerate(test_loader):\n",
        "    if i == batch_idx_to_view:\n",
        "        batch = b\n",
        "        break\n",
        "\n",
        "data_list = batch.to_data_list()\n",
        "# ãã®ãƒãƒƒãƒå†…ã® 0ç•ªç›® ã‚’è¡¨ç¤º\n",
        "visualize_pignn_attention_v3(model, data_list[5])"
      ],
      "metadata": {
        "id": "y9L4T4fAvCcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_pignn_attention_v3(model, data_list[19])"
      ],
      "metadata": {
        "id": "8ltESb3_xCJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ä¾‹ï¼š3æšç›®ã®ãƒãƒƒãƒï¼ˆã‚‚ã—ãƒãƒƒãƒã‚µã‚¤ã‚º64ãªã‚‰ 129ç•ªç›®ã€œ192ç•ªç›®ã®ã‚·ãƒ¼ãƒ³ï¼‰ã‚’æŠ½å‡º\n",
        "batch_idx_to_view = 4  # 0ã‹ã‚‰æ•°ãˆã‚‹ã®ã§ã€2ã¯3æšç›®\n",
        "\n",
        "for i, b in enumerate(test_loader):\n",
        "    if i == batch_idx_to_view:\n",
        "        batch = b\n",
        "        break\n",
        "\n",
        "data_list = batch.to_data_list()\n",
        "# ãã®ãƒãƒƒãƒå†…ã® 0ç•ªç›® ã‚’è¡¨ç¤º\n",
        "print(f\"SequenceID: {data_list[19].sequence_id.item()}\")\n",
        "visualize_pignn_attention_v3(model, data_list[6])"
      ],
      "metadata": {
        "id": "PO94cz3Q2bnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã€Œæœ¬ãƒ¢ãƒ‡ãƒ«ã«å°å…¥ã—ãŸ**é›†å›£æ¨é€²åŠ›æå¤±ï¼ˆCollective Velocity Lossï¼‰**ã®åŠ¹æœã«ã‚ˆã‚Šã€å€‹ã€…ã®é¸æ‰‹ã®ç¬é–“çš„ãªä½ç½®é–¢ä¿‚ã ã‘ã§ãªãã€ãƒãƒ¼ãƒ å…¨ä½“ã®æ”»æ’ƒæ–¹å‘ã¸ã®ãƒ™ã‚¯ãƒˆãƒ«ãŒäºˆæ¸¬ç¢ºç‡ã«çµ±åˆã•ã‚Œã¦ã„ã‚‹ã€‚å›³ï¼ˆimage_68ee7d.pngï¼‰ã«ãŠã„ã¦ã€ãƒœãƒ¼ãƒ«ä¿æŒè€…å‘¨è¾ºã®é¸æ‰‹ãŸã¡ãŒä¸€æ–‰ã«æ”»æ’ƒæ–¹å‘ï¼ˆå·¦æ–¹å‘ï¼‰ã¸ç§»å‹•ã‚’é–‹å§‹ã—ã¦ã„ã‚‹å±€é¢ã§é«˜ã„ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãŒè¦³æ¸¬ã•ã‚ŒãŸã“ã¨ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒé›†å›£çš„ãªé‹å‹•ã‚¨ãƒãƒ«ã‚®ãƒ¼ã®é›†ç´„åœ°ç‚¹ã‚’æ­£ã—ãè­˜åˆ¥ã—ã¦ã„ã‚‹è¨¼å·¦ã§ã‚ã‚‹ã€‚ã€"
      ],
      "metadata": {
        "id": "HZdAxPn4w_ox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "å­¦ç¿’æ›²ç·š"
      ],
      "metadata": {
        "id": "Y7mObh2d6Z73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# å­¦ç¿’æ™‚ã®ãƒ«ãƒ¼ãƒ—å†…ã§ train_acc ã‚‚è¨˜éŒ²ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ã—ã¦å®Ÿè¡Œã—ãŸã¨ä»®å®š\n",
        "# ã‚‚ã—è¨˜éŒ²ã—ã¦ã„ãªã‘ã‚Œã°ã€ã“ã®ã‚³ãƒ¼ãƒ‰ã§ç¾åœ¨ã® history ã‹ã‚‰ã‚°ãƒ©ãƒ•ã‚’å‡ºã—ã¾ã™\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history['total_loss'], label='Train Loss (Total)')\n",
        "# ã‚‚ã— train_acc ã‚’å–ã£ã¦ã„ã‚Œã°ã“ã“ã«è¿½åŠ \n",
        "plt.plot(history['test_acc'], label='Test Accuracy', marker='o')\n",
        "\n",
        "plt.title('Check for Overfitting: Loss vs Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2lj45c8TDvdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "è»½åº¦ã®éå­¦ç¿’ã ãŒã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºçš„ã«ä»•æ–¹ãŒãªã„ã‚‰ã—ã„ã€‚\n",
        "è¨“ç·´æ›²ç·šãŒæ¸›å°‘ã‹ã‚‰ä¸€å®šã«è½ã¡ç€ã„ã¦ã„ã‚‹ã®ã«å¯¾ã—ã¦ã€ãƒ†ã‚¹ãƒˆæ›²ç·šãŒä¸ŠãŒã£ã¦ã„ãŸã‚‰éå­¦ç¿’ã®å‚¾å‘ã€‚ä»Šå›ã®å ´åˆã¯ãƒ†ã‚¹ãƒˆæ›²ç·šãŒä¹±é™ä¸‹ã—ã¦ã„ã‚‹ã®ã§è»½åº¦ï¼Ÿ"
      ],
      "metadata": {
        "id": "dfBSDtiLYKGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ¤œè¨¼"
      ],
      "metadata": {
        "id": "Fq9_erp86ynd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãƒ©ãƒ™ãƒ«ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ãŸæ™‚ã®ç²¾åº¦"
      ],
      "metadata": {
        "id": "SshWo4xj62Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_label_test(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = preprocess_batch(data, device)\n",
        "            # ãƒ©ãƒ™ãƒ«ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹\n",
        "            random_y = data.y[torch.randperm(data.y.size(0))]\n",
        "\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == random_y.view(-1)).sum().item()\n",
        "            total += data.num_graphs\n",
        "\n",
        "    print(f\"ãƒ©ãƒ™ãƒ«ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ™‚ã®ç²¾åº¦: {correct / total:.4f}\")\n",
        "    print(\">> 0.5 (50%) å‰å¾Œã«ãªã‚Œã°ã€ãƒ¢ãƒ‡ãƒ«ã¯æ­£ã—ããƒ©ãƒ™ãƒ«ã¨ç‰¹å¾´ã®é–¢ä¿‚ã‚’å­¦ã‚“ã§ã„ã¾ã™ã€‚\")\n",
        "\n",
        "shuffle_label_test(model, test_loader, device)"
      ],
      "metadata": {
        "id": "xuFcHaVED2B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç‰©ç†æå¤±ï¼ˆæå¤±é–¢æ•°ã«ä»˜ã‘åŠ ãˆãŸç‰©ç†é …ï¼‰"
      ],
      "metadata": {
        "id": "Hnh1Z9O767qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ç‰©ç†æå¤±ã®å¹³å‡å€¤ã‚’ç®—å‡º\n",
        "avg_phys = sum(history['physics_loss']) / len(history['physics_loss'])\n",
        "print(f\"å¹³å‡ç‰©ç†æå¤±: {avg_phys:.4f}\")\n",
        "\n",
        "if avg_phys < 25: # 18å‰å¾Œãªã‚‰éå¸¸ã«å„ªç§€\n",
        "    print(\">> ç‰©ç†çš„æ•´åˆæ€§ã¯ä¿ãŸã‚Œã¦ã„ã¾ã™ã€‚AIã¯ç¾å®Ÿçš„ãªå‹•ãã®ç¯„å›²å†…ã§äºˆæ¸¬ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\">> ç‰©ç†æå¤±ãŒé«˜ã„ã§ã™ã€‚AIãŒç•°å¸¸ãªé€Ÿåº¦ã‚’æƒ³å®šã—ã¦äºˆæ¸¬ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")"
      ],
      "metadata": {
        "id": "PP6lAzBVD6GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "éå­¦ç¿’ã‚’é˜²ããŸã‚ã«ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤ã‚’è¿½åŠ ã—ãŸä¿®æ­£ç‰ˆPIGNNãƒ¢ãƒ‡ãƒ«"
      ],
      "metadata": {
        "id": "8CW6TCnB7Bff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "class PIGNNClassifier_drop(nn.Module):\n",
        "    def __init__(self, hidden_channels=64, dropout_rate=0.3):\n",
        "        super(PIGNNClassifier_drop, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # ä¿®æ­£: 7æ¬¡å…ƒå…¥åŠ› [x, y, vx, vy, d_goal, d_ball, team_id]\n",
        "        self.conv1 = PIGNNLayer(7, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        # 1å±¤ç›® + Dropout\n",
        "        x = self.conv1(x, edge_index, pos, vel)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        # 2å±¤ç›® + Dropout\n",
        "        x = self.conv2(x, edge_index, pos, vel)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # æœ€çµ‚å‡ºåŠ›\n",
        "        return F.log_softmax(self.lin(x), dim=1)\n",
        "\n",
        "class PIGNNClassifier_v7_Final(nn.Module):\n",
        "    def __init__(self, hidden_channels=64, dropout_rate=0.3):\n",
        "        super(PIGNNClassifier_v7_Final, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # ä¿®æ­£: 7æ¬¡å…ƒ [x, y, vx, vy, d_goal, d_ball, team_id]\n",
        "        self.conv1 = PIGNNLayer(7, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        # 1å±¤ç›® + Dropout\n",
        "        x = F.elu(self.conv1(x, edge_index, pos, vel))\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        # 2å±¤ç›® + Dropout\n",
        "        x = F.elu(self.conv2(x, edge_index, pos, vel))\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return F.log_softmax(self.lin(x), dim=1)"
      ],
      "metadata": {
        "id": "BCUmvO4FYues"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–\n",
        "model_dropout = PIGNNClassifier_drop(hidden_channels=64, dropout_rate=0.3).to(device)\n",
        "optimizer_dropout = torch.optim.Adam(model_dropout.parameters(), lr=LR)\n",
        "\n",
        "# 2. å±¥æ­´ä¿å­˜ç”¨ï¼ˆåå‰ã‚’åˆ†ã‘ã‚‹ï¼šhistory_dropoutï¼‰\n",
        "history_dropout = {\n",
        "    'total_loss': [],\n",
        "    'physics_loss': [],\n",
        "    'test_acc': []\n",
        "}\n",
        "\n",
        "print(f\"PIGNNå­¦ç¿’é–‹å§‹ï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‰ˆï¼‰\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # model_dropout ã‚’ä½¿ã†\n",
        "    avg_loss, avg_phys = train_pignn_epoch(model_dropout, train_loader, optimizer_dropout, ALPHA_P, device)\n",
        "    acc = test_pignn(model_dropout, test_loader, device)\n",
        "\n",
        "    history_dropout['total_loss'].append(avg_loss)\n",
        "    history_dropout['physics_loss'].append(avg_phys)\n",
        "    history_dropout['test_acc'].append(acc)\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | Loss: {avg_loss:.4f} | Phys_L: {avg_phys:.4f} | Acc: {acc:.4f}\")\n",
        "\n",
        "    # ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å¤‰ãˆã‚‹ï¼ˆé‡è¦ï¼ï¼‰\n",
        "    if epoch == 1 or acc > max(history_dropout['test_acc'][:-1]):\n",
        "        torch.save(model_dropout.state_dict(), 'best_pignn_model_v6_dropout.pth')\n",
        "        print(f\" >> Model Saved (Best Dropout Acc: {acc:.4f})\")"
      ],
      "metadata": {
        "id": "cjks8bFpY2Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç²¾åº¦æ›²ç·š"
      ],
      "metadata": {
        "id": "2Xe4VMee7JJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤ãªã—ã¨ã‚ã‚Šã‚’é‡ã­ã¦æç”»\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history['test_acc'], label='Base Model (No Dropout)', alpha=0.6)\n",
        "plt.plot(history_dropout['test_acc'], label='Improved Model (With Dropout)', linewidth=2)\n",
        "plt.title('Effect of Dropout on Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MQvfkBq5ZbCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šä¹±é«˜ä¸‹ã®å¹…ãŒå°ã•ããªã£ã¦ã„ã‚‹â†’æ”¹å–„ãƒã‚¤ãƒ³ãƒˆ"
      ],
      "metadata": {
        "id": "OT_gMd-h7QKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# ==========================================\n",
        "# 5. æœ€çµ‚è©•ä¾¡ã¨ãƒ¬ãƒãƒ¼ãƒˆ (Dropoutç‰ˆãƒ»å°‚ç”¨)\n",
        "# ==========================================\n",
        "\n",
        "# 1. ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
        "# å­¦ç¿’æ™‚ã«ä¿å­˜ã—ãŸã€Œdropoutã€ã¨ã„ã†åå‰ã®æ–¹ã‚’èª­ã¿è¾¼ã¿ã¾ã™\n",
        "model_drop_eval = PIGNNClassifier_drop(hidden_channels=64, dropout_rate=0.3).to(device)\n",
        "model_drop_eval.load_state_dict(torch.load('best_pignn_model_v6_dropout.pth'))\n",
        "model_drop_eval.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# 2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§æœ€çµ‚äºˆæ¸¬\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        data = preprocess_batch(data, device)\n",
        "        out = model_drop_eval(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# 3. ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"       PIGNN æœ€çµ‚è©•ä¾¡çµæœ (Dropoutæ”¹è‰¯ç‰ˆ)\")\n",
        "print(\"=\"*50)\n",
        "print(classification_report(all_labels, all_preds, target_names=['Failure (0)', 'Success (1)']))\n",
        "\n",
        "# 4. æ··åŒè¡Œåˆ—ã®æç”»\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', # åŒºåˆ¥ã™ã‚‹ãŸã‚ã«è‰²ã‚’ã‚ªãƒ¬ãƒ³ã‚¸ç³»ã«\n",
        "            xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title('Confusion Matrix (PIGNN with Dropout)')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EvaPTIG8b-m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç²¾åº¦ã¯ä¸‹ãŒã£ãŸãŒã€ãƒ¢ãƒ‡ãƒ«ã¯æ”¹å–„ã—ã¦ã„ã‚‹ã¨è¨€ãˆã‚‹ã€‚"
      ],
      "metadata": {
        "id": "D1k1IKXCcmXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "æ¤œè¨¼"
      ],
      "metadata": {
        "id": "7biH1yrO7eMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_label_test_dropout_ver(model_to_test, loader, device):\n",
        "    model_to_test.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # åˆ¤å®šç”¨ã®é–¾å€¤ï¼ˆä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€å¤šæ•°æ´¾ã®å‰²åˆã«å¼•ã£å¼µã‚‰ã‚Œã‚‹ãŸã‚ï¼‰\n",
        "    # ä»Šå›ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ Failure:239, Success:90 ãªã®ã§ã€ãƒ©ãƒ³ãƒ€ãƒ ãªã‚‰\n",
        "    # (239/329)^2 + (90/329)^2 â‰’ 0.6 ãã‚‰ã„ã«ãªã‚‹ã®ãŒçµ±è¨ˆå­¦çš„ãªã€Œå‹˜ã€ã®é™ç•Œã§ã™ã€‚\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = preprocess_batch(data, device)\n",
        "\n",
        "            # ãƒ©ãƒ™ãƒ«ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
        "            random_y = data.y[torch.randperm(data.y.size(0))]\n",
        "\n",
        "            out = model_to_test(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            correct += (pred == random_y.view(-1)).sum().item()\n",
        "            total += data.num_graphs\n",
        "\n",
        "    shuffle_acc = correct / total\n",
        "    print(f\"ãƒ©ãƒ™ãƒ«ã‚·ãƒ£ãƒƒãƒ•ãƒ«æ™‚ã®ç²¾åº¦: {shuffle_acc:.4f}\")\n",
        "\n",
        "    if shuffle_acc < 0.58: # 0.62ã‹ã‚‰ä¸‹ãŒã£ã¦ã„ã‚Œã°æ”¹å–„\n",
        "        print(\">> åˆæ ¼ï¼šæš—è¨˜ï¼ˆéå­¦ç¿’ï¼‰ãŒæŠ‘åˆ¶ã•ã‚Œã€ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã®çœŸã®ç›¸é–¢ã‚’å­¦ã‚“ã§ã„ã¾ã™ã€‚\")\n",
        "    else:\n",
        "        print(\">> è­¦å‘Šï¼šä¾ç„¶ã¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã®åã‚Šï¼ˆåˆæœŸé…ç½®ãªã©ï¼‰ã‚’å¼·ãè¦šãˆã™ãã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "\n",
        "# å®Ÿè¡Œï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‰ˆã®ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šï¼‰\n",
        "shuffle_label_test_dropout_ver(model_drop_eval, test_loader, device)"
      ],
      "metadata": {
        "id": "hm1YcFwvc21q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãªãœã€Œ0.59ã€ã§æ­£è§£ãªã®ã‹ï¼Ÿ\n",
        "ä»Šå›ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ï¼‰ã«ãŠã„ã¦ã€**0.59 ã¨ã„ã†æ•°å€¤ã¯å®Ÿè³ªçš„ãªã€Œ0 (ã‚¼ãƒ­) ç‚¹ã€**ã‚’æ„å‘³ã—ã¾ã™ã€‚\n",
        "\n",
        "ãƒ‡ãƒ¼ã‚¿ã®æ¯”ç‡: å¤±æ•—ï¼ˆFailï¼‰ãŒç´„ 73%ã€æˆåŠŸï¼ˆSuccessï¼‰ãŒç´„ 27% ã§ã™ã€‚\n",
        "\n",
        "å¶ç„¶ã®æœŸå¾…å€¤: çµ±è¨ˆå­¦çš„ã«ã€ã“ã®æ¯”ç‡ã®ãƒ‡ãƒ¼ã‚¿ã§é©å½“ã«ãƒ©ãƒ™ãƒ«ã‚’æŒ¯ã£ã¦å½“ã¦ãšã£ã½ã†ã§äºˆæ¸¬ã™ã‚‹ã¨ã€ç´„ 0.6 (60%) å‰å¾Œã®ç²¾åº¦ã«ãªã‚‹ã®ãŒæ­£å¸¸ãªæŒ™å‹•ã§ã™ã€‚\n",
        "\n",
        "åˆ¤å®š: ã¤ã¾ã‚Šã€0.5927 ã¨ã„ã†ã®ã¯ã€Œãƒ‡ã‚¿ãƒ©ãƒ¡ãªç­”ãˆã‚’ä¸ãˆã‚‰ã‚ŒãŸã‚‰ã€AIã¯ä½•ã‚‚å½“ã¦ã‚‹è¡“ã‚’æŒã£ã¦ã„ãªã„ã€ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã‚‚ã—ã“ã‚ŒãŒ 0.8 ã¨ã‹ã§ã‚ã‚Œã°ã€Œç­”ãˆã‚’è¦‹ãªãã¦ã‚‚ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å½“ã¦ã¦ã„ã‚‹ï¼ˆæš—è¨˜ï¼‰ã€ã“ã¨ã«ãªã‚Šã¾ã™ãŒã€0.59 ãªã‚‰**ã€Œæš—è¨˜ã—ã¦ã„ãªã„ã€**ã¨è¨€ã„åˆ‡ã‚Œã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "yUtN7OxNeKfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç‰©ç†æå¤±"
      ],
      "metadata": {
        "id": "DW3LormL7h9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ç‰©ç†çš„æ•´åˆæ€§ã®æœ€çµ‚ç¢ºèª (Dropoutç‰ˆ)\n",
        "# ==========================================\n",
        "\n",
        "# history_dropout ã®ä¸­èº«ã‚’ä½¿ã£ã¦è¨ˆç®—ã—ã¾ã™\n",
        "if 'physics_loss' in history_dropout and len(history_dropout['physics_loss']) > 0:\n",
        "    avg_phys = sum(history_dropout['physics_loss']) / len(history_dropout['physics_loss'])\n",
        "    print(f\"ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‰ˆ å¹³å‡ç‰©ç†æå¤±: {avg_phys:.4f}\")\n",
        "\n",
        "    # åˆ¤å®š\n",
        "    if avg_phys < 25:\n",
        "        print(\">> åˆæ ¼ï¼šç‰©ç†çš„æ•´åˆæ€§ã¯ä¿ãŸã‚Œã¦ã„ã¾ã™ã€‚\")\n",
        "        print(\">> ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’å°å…¥ã—ã¦ã‚‚ã€AIã¯ç¾å®Ÿçš„ãªç‰©ç†æ³•å‰‡ï¼ˆé€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã‚’ç„¡è¦–ã—ã¦ã„ã¾ã›ã‚“ã€‚\")\n",
        "    else:\n",
        "        print(\">> è­¦å‘Šï¼šç‰©ç†æå¤±ãŒå¢—å¤§ã—ã¦ã„ã¾ã™ã€‚\")\n",
        "        print(\">> æ±åŒ–æ€§èƒ½ã‚’å„ªå…ˆã™ã‚‹ã‚ã¾ã‚Šã€ç‰©ç†ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®åˆ¶ç´„ãŒå¼±ã¾ã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "else:\n",
        "    print(\">> ã‚¨ãƒ©ãƒ¼ï¼šhistory_dropout ã« physics_loss ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\")"
      ],
      "metadata": {
        "id": "LGzGGSqSdA8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ä»Šå›ã®çµæœï¼ˆ18.05ï¼‰ã¯ã©ã†è¦‹ã‚‹ã¹ãï¼Ÿã‚ãªãŸãŒç®—å‡ºã—ãŸ 18.05 ã¨ã„ã†æ•°å€¤ã¯ã€å’è«–ã«ãŠã„ã¦éå¸¸ã«å¼·åŠ›ãªæ­¦å™¨ã«ãªã‚Šã¾ã™ã€‚æ•°å€¤ã®æ„å‘³: ä»¥å‰ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆ18.01ï¼‰ã¨ã»ã¼åŒã˜ã§ã™ã€‚è©•ä¾¡: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’å…¥ã‚Œã¦å­¦ç¿’ã‚’å³ã—ãã—ãŸã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€AIã¯ã€Œç‰©ç†ã‚’ç„¡è¦–ã—ã¦é©å½“ã«å½“ã¦ã‚‹ã€ã¨ã„ã†é€ƒã’é“ã‚’ä½¿ã„ã¾ã›ã‚“ã§ã—ãŸã€‚çµè«–: **ã€Œã“ã®AIã®äºˆæ¸¬ã¯ã€ã‚µãƒƒã‚«ãƒ¼ã®ç‰©ç†çš„ãƒªã‚¢ãƒªãƒ†ã‚£ã«åŸºã¥ã„ã¦ã„ã‚‹ã€**ã¨èƒ¸ã‚’å¼µã£ã¦è¨€ãˆã‚‹æ ¹æ‹ ã«ãªã‚Šã¾ã™ã€‚å’è«–ã§ã€Œç‰©ç†æå¤±ã€ã‚’èª¬æ˜ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ã‚ºã€Œç‰©ç†æå¤±ã¨ã¯ä½•ã§ã™ã‹ï¼Ÿã€ã¨èã‹ã‚ŒãŸã‚‰ã€ã“ã†ç­”ãˆã¦ãã ã•ã„ã€‚ã€Œæœ¬ç ”ç©¶ã«ãŠã‘ã‚‹ç‰©ç†æå¤±ã¯ã€äºˆæ¸¬ã•ã‚ŒãŸé¸æ‰‹ã®æ¬¡çŠ¶æ…‹ãŒé‹å‹•å­¦çš„æ•´åˆæ€§ã‚’æ¬ ã„ã¦ã„ãªã„ã‹ã‚’è©•ä¾¡ã™ã‚‹æŒ‡æ¨™ã§ã™ã€‚å…·ä½“çš„ã«ã¯ã€å…¥åŠ›ã•ã‚ŒãŸé€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã¨äºˆæ¸¬ä½ç½®ã¨ã®ä¹–é›¢ã‚’æå¤±é–¢æ•° $L_{motion}$ ã¨ã—ã¦å®šç¾©ã—ã€ãƒ¢ãƒ‡ãƒ«ã«èª²ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ãŒé™¥ã‚ŠãŒã¡ãªã€ç‰©ç†çš„ã«ä¸å¯èƒ½ãªç‰¹å¾´é‡ã¸ã®éå­¦ç¿’ã€ã‚’æŠ‘åˆ¶ã—ã€å®Ÿæˆ¦ã«å³ã—ãŸåˆ¤æ–­åŸºæº–ã‚’å­¦ç¿’ã•ã›ã¦ã„ã¾ã™ã€‚ã€"
      ],
      "metadata": {
        "id": "GFLyB4t9egmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GNNExplainer\n",
        "ï¼ˆè§£èª¬è¨˜äº‹ï¼šhttps://engineers.ntt.com/entry/2023/12/11/085715ï¼‰\n",
        "ï¼ˆè«–æ–‡ï¼šhttps://arxiv.org/pdf/1903.03894ï¼‰\n",
        "ç°¡å˜ã«èª¬æ˜ã™ã‚‹ã¨ã€å…¨ä½“ã®ã‚°ãƒ©ãƒ•ã‹ã‚‰å¯¾è±¡ãƒãƒ¼ãƒ‰ vi\n",
        " ã¨ã¯åˆ¥ã®ãƒãƒ¼ãƒ‰ã§ã‚ã‚‹ vj\n",
        " ã‚’é™¤å¤–ã—ãŸéš›ã®äºˆæ¸¬ç¢ºç‡ y^i\n",
        " ã®å¢—æ¸›ã‚’è¦‹ã¦ã€äºˆæ¸¬ç¢ºç‡ãŒå¤§ããæ¸›å°‘ã™ã‚‹å ´åˆã¯ãƒãƒ¼ãƒ‰ vj\n",
        " ã¯äºˆæ¸¬ã«è‰¯ã„å½±éŸ¿ã‚’ä¸ãˆã‚‹ã¨åˆ¤æ–­ã—ã¦ã€äºˆæ¸¬ã«å¤§ããå¯„ä¸ã™ã‚‹ã‚¨ãƒƒã‚¸ã®ã¿ã‚’é¸æŠã—ã¦ã„ãã“ã¨ã§æœ‰åŠ¹ãªã‚µãƒ–ã‚°ãƒ©ãƒ•ã®ç²å¾—ã‚’ç›®æŒ‡ã—ã¦ã„ãã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "Tk6OziB27_Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.explain import Explainer, GNNExplainer\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# --- ä¿®æ­£ã®æ ¸å¿ƒï¼šé‡è¦åº¦ã‚’æŠ½å‡ºã™ã‚‹1è¡Œ ---\n",
        "# node_mask ã¯ (ãƒãƒ¼ãƒ‰æ•°, ç‰¹å¾´é‡æ•°) ã®å½¢çŠ¶ãªã®ã§ã€å…¨ãƒãƒ¼ãƒ‰ã§å¹³å‡ã—ã¦ç‰¹å¾´é‡ã”ã¨ã®é‡è¦åº¦ã«ã™ã‚‹\n",
        "importances = explanation.node_mask.mean(dim=0)\n",
        "\n",
        "# --- ä»¥é™ã€ã‚ãªãŸã®å¯è¦–åŒ–ã‚³ãƒ¼ãƒ‰ã¸ç¶šã ---\n",
        "if torch.is_tensor(importances):\n",
        "    importances = importances.cpu().numpy()\n",
        "\n",
        "# 1. ãƒãƒ©ãƒãƒ©ã®å¼•æ•°ã‚’ã€Œdataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€ã«æ¢±åŒ…ã—ã¦ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™ãƒ©ãƒƒãƒ‘ãƒ¼\n",
        "class ExplainerWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None, **kwargs):\n",
        "        # GNNExplainerã‹ã‚‰å±Šãå„ãƒ†ãƒ³ã‚½ãƒ«ã‚’ã€ãƒ¢ãƒ‡ãƒ«ãŒæœŸå¾…ã™ã‚‹ Data ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«æ“¬ä¼¼å†ç¾\n",
        "        # pos ã¨ vel ã¯ x ã®ä¸­ã«å…¥ã£ã¦ã„ã‚‹ã€ã‚ã‚‹ã„ã¯åˆ¥é€”æ¸¡ã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®š\n",
        "        # ã‚ãªãŸã®ãƒ¢ãƒ‡ãƒ«å®šç¾©ã«åˆã‚ã›ã¦ x ã‹ã‚‰ pos, vel ã‚’åˆ‡ã‚Šå‡ºã™ã€\n",
        "        # ã¾ãŸã¯ data.pos, data.vel ã¨ã—ã¦ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚\n",
        "\n",
        "        # ä»®ã« x ã® 0-1åˆ—ç›®ãŒ pos, 2-3åˆ—ç›®ãŒ vel ã ã¨æƒ³å®šã•ã‚Œã‚‹å ´åˆï¼š\n",
        "        # (ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã«åˆã‚ã›ã¦èª¿æ•´ã—ã¦ãã ã•ã„ã€‚xè‡ªä½“ã«å…¨ã¦å«ã¾ã‚Œã¦ã„ã‚‹ãªã‚‰ãã®ã¾ã¾ã§ã‚‚å¯)\n",
        "        tmp_data = Data(x=x, edge_index=edge_index, batch=batch)\n",
        "\n",
        "        # ã‚‚ã—ãƒ¢ãƒ‡ãƒ«ãŒ data.pos ã‚„ data.vel ã‚’ç›´æ¥å‚ç…§ã—ã¦ã„ã‚‹ãªã‚‰ã€ã“ã“ã§ä»£å…¥\n",
        "        # x ã®æ§‹æˆãŒ [ç‰¹å¾´é‡...] ã§ã€pos/velãŒåˆ¥ç®¡ç†ãªã‚‰ä»¥ä¸‹ã®ã‚ˆã†ã«å¾©å…ƒ\n",
        "        tmp_data.pos = x[:, :2]  # ä¾‹: æœ€åˆã®2åˆ—ãŒåº§æ¨™\n",
        "        tmp_data.vel = x[:, 2:4] # ä¾‹: æ¬¡ã®2åˆ—ãŒé€Ÿåº¦\n",
        "\n",
        "        return self.model(tmp_data)\n",
        "\n",
        "# ãƒ©ãƒƒãƒ—ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ\n",
        "wrapped_model = ExplainerWrapper(model_drop_eval)\n",
        "\n",
        "# 2. Explainerã®è¨­å®š\n",
        "model_config = {\n",
        "    'mode': 'multiclass_classification',\n",
        "    'task_level': 'graph',\n",
        "    'return_type': 'log_probs',\n",
        "}\n",
        "\n",
        "explainer = Explainer(\n",
        "    model=wrapped_model,\n",
        "    algorithm=GNNExplainer(epochs=200),\n",
        "    explanation_type='model',\n",
        "    node_mask_type='attributes',\n",
        "    edge_mask_type='object',\n",
        "    model_config=model_config,\n",
        ")\n",
        "\n",
        "# --- ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ ---\n",
        "test_batch = next(iter(test_loader))\n",
        "test_batch = preprocess_batch(test_batch, device)\n",
        "data_list = test_batch.to_data_list()\n",
        "data_single = data_list[0]\n",
        "\n",
        "# 3. é‡è¦åº¦ã®ç®—å‡º\n",
        "explanation = explainer(\n",
        "    x=data_single.x,\n",
        "    edge_index=data_single.edge_index,\n",
        "    batch=torch.zeros(data_single.x.size(0), dtype=torch.long).to(device)\n",
        ")\n",
        "\n",
        "print(\"é‡è¦åº¦ã®ç®—å‡ºã«æˆåŠŸã—ã¾ã—ãŸï¼\")\n",
        "\n",
        "# --- ä¿®æ­£ç‰ˆï¼šé‡è¦åº¦ã®å¯è¦–åŒ–ã‚³ãƒ¼ãƒ‰ ---\n",
        "\n",
        "# ãƒ©ãƒ™ãƒ«ã‚’7æ¬¡å…ƒç”¨ã«æ›´æ–°\n",
        "labels = ['x', 'y', 'vx', 'vy', 'dist_goal', 'dist_ball', 'team_id']\n",
        "\n",
        "# importances ãŒ numpy å½¢å¼ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\n",
        "if torch.is_tensor(importances):\n",
        "    importances = importances.cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "# ã“ã“ã§æ¬¡å…ƒæ•°ã‚’è‡ªå‹•ã§åˆã‚ã›ã¾ã™\n",
        "plt.bar(labels, importances, color='teal')\n",
        "\n",
        "plt.title('GNNExplainer: Feature Importance (PIGNN v7)')\n",
        "plt.ylabel('Importance Score')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# å€¤ã‚’æ£’ã®ä¸Šã«è¡¨ç¤º\n",
        "for i, v in enumerate(importances):\n",
        "    plt.text(i, v + 0.01, f\"{v:.3f}\", ha='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9i9AUGXx97sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GNNExplainerãŒç®—å‡ºã—ãŸã‚¹ã‚³ã‚¢ã«ã¯ã€ã‚µãƒƒã‚«ãƒ¼ã®æˆ¦è¡“ã‚’è§£æ˜ã™ã‚‹ä¸Šã§éå¸¸ã«é¢ç™½ã„å‚¾å‘ãŒå‡ºã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "x, y (ä½ç½®) ãŒåœ§å€’çš„ã«é‡è¦ï¼ˆ0.362, 0.308ï¼‰: ãƒ¢ãƒ‡ãƒ«ã¯ã€Œé¸æ‰‹ãŒãƒ”ãƒƒãƒã®ã©ã“ã«ç«‹ã£ã¦ã„ã‚‹ã‹ã€ã‚’æœ€ã‚‚é‡è¦–ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®æˆåŠŸãƒ»å¤±æ•—ãŒã€å€‹ã€…ã®é¸æ‰‹ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ï¼ˆvx, vyï¼‰ã‚ˆã‚Šã‚‚ã€**ã€Œé©åˆ‡ãªãƒã‚¸ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã€**ã«ã‚ˆã£ã¦æ±ºã¾ã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "ç‰©ç†æŒ‡æ¨™ (dist_goal, dist_ball) ã®è²¢çŒ®: ãƒœãƒ¼ãƒ«ã¨ã®è·é›¢ã‚„ã‚´ãƒ¼ãƒ«ã¸ã®è·é›¢ã‚‚ã—ã£ã‹ã‚ŠåŠ å‘³ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "team_id (0.149) ã®é‡ã¿: å¾Œã‹ã‚‰ç„¡ç†ã‚„ã‚Šä»˜ã‘è¶³ã—ãŸãƒãƒ¼ãƒ ãƒ•ãƒ©ã‚°ã§ã™ãŒã€ã—ã£ã‹ã‚Š vx (0.129) ã‚„ vy (0.114) ã‚ˆã‚Šã‚‚é«˜ã„é‡è¦åº¦ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€Œèª°ãŒå‘³æ–¹ã§èª°ãŒæ•µã‹ã€ã¨ã„ã†æƒ…å ±ãŒã€å€‹äººã®è¶³ã®é€Ÿã•ã‚ˆã‚Šã‚‚äºˆæ¸¬ã«å½¹ç«‹ã£ã¦ã„ã‚‹ã“ã¨ã‚’è¨¼æ˜ã—ã¦ã„ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "PixsbxuwBX7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PFIã¨GNï¼®Explainerã®é•ã„ï¼š\n",
        "PFIï¼ˆå…¨ä½“çš„ï¼‰: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦ã€ã€Œã“ã®é …ç›®ã‚’ãƒ¡ãƒãƒ£ã‚¯ãƒãƒ£ã«ã—ãŸã‚‰ã€å…¨ä½“ã®æ­£è§£ç‡ãŒã©ã‚Œãã‚‰ã„ä¸‹ãŒã‚‹ã‹ã€ã‚’è¦‹ã¾ã™ã€‚ã„ã‚ã°**ã€Œå¹³å‡çš„ãªå½±éŸ¿åŠ›ã€**ã§ã™ã€‚\n",
        "\n",
        "GNNExplainerï¼ˆå±€æ‰€çš„ï¼‰: ã€Œã“ã®ã‚·ãƒ¼ãƒ³ã§æˆåŠŸã¨åˆ¤å®šã—ãŸã®ã¯ã€ã“ã®é¸æ‰‹ã®ã“ã®é€Ÿåº¦ãŒæ±ºã‚æ‰‹ã ã£ãŸã€ã¨ã„ã†**ã€Œç‰¹å®šã®æ„æ€æ±ºå®šã®æ ¹æ‹ ã€**ã‚’æ·±æ˜ã‚Šã—ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "8vPCKFDQB6hO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ•°ç™¾ã‚·ãƒ¼ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ãã‚Œãã‚Œã®é‡è¦åº¦ã‚’è¨ˆç®—ã—ã¦å¹³å‡åŒ–ã™ã‚‹ã¨"
      ],
      "metadata": {
        "id": "mHxyxFOYBnOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "â€»ãªãœã€Œå…¨ä½“å¹³å‡ã€ãŒå¿…è¦ãªã®ã‹ï¼ˆã•ã£ãã®ææ¡ˆã®æ„å›³ï¼‰\n",
        "1æšç›®ã®ã‚°ãƒ©ãƒ•ï¼ˆ1ã‚·ãƒ¼ãƒ³ã®ã¿ï¼‰ã‚’å’è«–ã«è¼‰ã›ã‚‹ã¨ã€å¯©æŸ»å“¡ã‹ã‚‰**ã€Œãã‚Œã¯ãã®ã‚·ãƒ¼ãƒ³ã ã‘ã§ã€ä»–ã®ã‚·ãƒ¼ãƒ³ã§ã¯é•ã†ã‚“ã˜ã‚ƒãªã„ã®ï¼Ÿã€**ã¨ãƒ„ãƒƒã‚³ãƒŸãŒå…¥ã‚Šã¾ã™ã€‚\n",
        "\n",
        "ãã“ã§ã€ç§ãŒææ¡ˆã—ãŸ**ã€Œæ•°ç™¾ã‚·ãƒ¼ãƒ³ã‚’å›ã—ã¦å¹³å‡ã‚’ã¨ã‚‹ã€**ã¨ã„ã†ä½œæ¥­ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚\n",
        "\n",
        "å„ã‚·ãƒ¼ãƒ³ã®é‡è¦åº¦ã‚’å‡ºã™: 1ã‚·ãƒ¼ãƒ³ã”ã¨ã«GNNExplainerã‚’å®Ÿè¡Œã€‚\n",
        "\n",
        "ãã‚Œã‚‰ã‚’å¹³å‡ã™ã‚‹: ã“ã‚Œã«ã‚ˆã‚Šã€**ã€ŒGNNã¨ã„ã†é«˜åº¦ãªæ§‹é€ è§£æãƒ¢ãƒ‡ãƒ«ãŒã€å…¨ä½“ã¨ã—ã¦ã©ã®ç‰¹å¾´é‡ã‚’é‡è¦–ã™ã‚‹å‚¾å‘ã«ã‚ã‚‹ã‹ã€**ã¨ã„ã†ã€PFIã‚ˆã‚Šã‚‚ã•ã‚‰ã«é«˜ç²¾åº¦ãªå…¨ä½“é‡è¦åº¦ãŒæ‰‹ã«å…¥ã‚Šã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "3kci-v68CHVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. è§£æã®è¨­å®š\n",
        "num_samples = 100\n",
        "all_node_importances = []\n",
        "\n",
        "print(f\"{num_samples}ã‚·ãƒ¼ãƒ³ã®è§£æã‚’é–‹å§‹ã—ã¾ã™ã€‚å‹¾é…è¨ˆç®—ã‚’æœ‰åŠ¹ã«ã—ã¦æœ€é©åŒ–ã‚’è¡Œã†ãŸã‚ã€å°‘ã—æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™...\")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«ã—ã¤ã¤ã€GNNExplainerå†…éƒ¨ã®å­¦ç¿’ã¯è¨±å¯ã™ã‚‹\n",
        "model_drop_eval.eval()\n",
        "\n",
        "# é€²æ—ç®¡ç†ç”¨ã®ã‚«ã‚¦ãƒ³ã‚¿\n",
        "count = 0\n",
        "\n",
        "for data in tqdm(test_loader):\n",
        "    if count >= num_samples:\n",
        "        break\n",
        "\n",
        "    data = preprocess_batch(data, device)\n",
        "    data_list = data.to_data_list()\n",
        "\n",
        "    for data_single in data_list:\n",
        "        if count >= num_samples:\n",
        "            break\n",
        "\n",
        "        # --- ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆï¼šwith torch.no_grad() ã‚’å‰Šé™¤ ---\n",
        "        # GNNExplainerã¯å†…éƒ¨ã§ãƒ­ã‚¹ã‚’è¨ˆç®—ã— .backward() ã‚’å‘¼ã¶ãŸã‚å‹¾é…ãŒå¿…è¦\n",
        "        explanation = explainer(\n",
        "            x=data_single.x,\n",
        "            edge_index=data_single.edge_index,\n",
        "            batch=torch.zeros(data_single.x.size(0), dtype=torch.long).to(device)\n",
        "        )\n",
        "\n",
        "        node_importance = explanation.node_mask.abs().mean(dim=0).cpu().numpy()\n",
        "        all_node_importances.append(node_importance)\n",
        "        count += 1\n",
        "\n",
        "# 3. å¹³å‡ã¨æ¨™æº–èª¤å·®ã®è¨ˆç®—\n",
        "avg_importance = np.mean(all_node_importances, axis=0)\n",
        "std_importance = np.std(all_node_importances, axis=0) / np.sqrt(len(all_node_importances))\n",
        "\n",
        "# --- ä¿®æ­£ç‰ˆï¼šãƒ©ãƒ™ãƒ«ã¨ã‚°ãƒ©ãƒ•æç”» ---\n",
        "\n",
        "# 1. ãƒ©ãƒ™ãƒ«ã‚’ç¾åœ¨ã®7æ¬¡å…ƒä»•æ§˜ã«å®Œå…¨ã«åˆã‚ã›ã‚‹\n",
        "# [x, y, vx, vy, dist_goal, dist_ball, team_id]\n",
        "labels = ['PosX', 'PosY', 'VelX', 'VelY', 'DistGoal', 'DistBall', 'TeamID']\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# 2. ãƒ‡ãƒ¼ã‚¿ã®æ•°ã¨ãƒ©ãƒ™ãƒ«ã®æ•°ãŒä¸€è‡´ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦æç”»\n",
        "# avg_importance ã¨ std_importance ãŒ 7è¦ç´ ã§ã‚ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¾ã™\n",
        "bars = plt.bar(labels, avg_importance, yerr=std_importance,\n",
        "               color='teal', edgecolor='navy', capsize=5, alpha=0.8)\n",
        "\n",
        "plt.title('GNNExplainer: Mean Feature Importance over 100 Scenes (v7)', fontsize=14)\n",
        "plt.ylabel('Importance Score', fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# æ£’ã®ä¸Šã«æ•°å€¤ã‚’è¡¨ç¤º\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fztqYk2oBvLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "å·¥ï¼ˆã²ã’ï¼‰ã¯ã‚·ãƒ¼ãƒ³ã«ã‚ˆã‚‹ã°ã‚‰ã¤ãã€‚ã“ã®ã€Œå·¥ã€ãŒçŸ­ã„ã»ã©ã€**ã€Œã©ã®ã‚·ãƒ¼ãƒ³ã§ã‚‚åŒã˜ã‚ˆã†ã«ãã®ç‰¹å¾´é‡ãŒé‡è¦–ã•ã‚Œã¦ã„ã‚‹ï¼ˆçµæœãŒå®‰å®šã—ã¦ã„ã‚‹ï¼‰ã€**ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚ï¼‘å›ã ã‘ã§ãªãï¼‘ï¼ï¼å›ã§ã‚„ã£ã¦ã‚‚åŒã˜å‚¾å‘ãŒè¦‹ã‚‰ã‚Œã‚‹ï¼ˆå†ç¾æ€§ï¼‰ã€‚"
      ],
      "metadata": {
        "id": "qXVO6cfFDCIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ã‚°ãƒ©ãƒ•ã®ã€Œå·¥ï¼ˆã²ã’ï¼‰ã€ãŒè¨¼æ˜ã™ã‚‹ã“ã¨\n",
        "ã“ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ¼ï¼ˆã²ã’ï¼‰ã¯ã€100ã‚·ãƒ¼ãƒ³ã«ãŠã‘ã‚‹é‡è¦åº¦ã®**ã€Œã°ã‚‰ã¤ãã€**ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "PosX / PosYã®ã²ã’: æ¯”è¼ƒçš„é•·ã‚ã§ã™ã€‚ã“ã‚Œã¯ã‚·ãƒ¼ãƒ³ï¼ˆãƒ”ãƒƒãƒã®å ´æ‰€ï¼‰ã«ã‚ˆã£ã¦ã€ä½ç½®ã®é‡è¦æ€§ãŒãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ã«å¤‰åŒ–ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ãŠã‚Šã€ã‚µãƒƒã‚«ãƒ¼ã®æµå‹•æ€§ã‚’AIãŒæ‰ãˆã¦ã„ã‚‹è¨¼æ‹ ã§ã™ã€‚\n",
        "\n",
        "VelX / VelYã®ã²ã’: æ¥µã‚ã¦çŸ­ã„ã§ã™ã€‚ ã“ã‚ŒãŒé‡è¦ã§ã™ã€‚ã€Œã©ã‚“ãªã‚·ãƒ¼ãƒ³ã§ã‚ã£ã¦ã‚‚ã€AIã¯ä¸€è²«ã—ã¦ä¸€å®šã®å‰²åˆã§ã€é€Ÿåº¦ã€ã‚’åˆ¤æ–­æ ¹æ‹ ã«åŠ ãˆã¦ã„ã‚‹ã€ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€ã‚ãªãŸãŒå°å…¥ã—ãŸ**Physics-Informedï¼ˆç‰©ç†æƒ…å ±å‹ï¼‰**ã®è¨­è¨ˆãŒã€å…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å®‰å®šã—ã¦æ©Ÿèƒ½ã—ã¦ã„ã‚‹ã“ã¨ã‚’çµ±è¨ˆçš„ã«è¨¼æ˜ã—ã¦ã„ã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "c4UOdCmGEtdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PFIï¼ˆç²¾åº¦ä½ä¸‹ï¼‰ã¨GNNExplainerã®ã€Œã‚ºãƒ¬ã€ã‚’ã©ã†æ›¸ãã‹\n",
        "ã“ã“ãŒå’è«–ã®**ä¸€ç•ªã®ã€Œè¦‹ã›å ´ã€**ã«ãªã‚Šã¾ã™ã€‚\n",
        "\n",
        "PFIã®çµæœ: é€Ÿåº¦ï¼ˆvel_x/yï¼‰ã‚’æ¶ˆã—ã¦ã‚‚ç²¾åº¦ã¯ã»ã¼å¤‰ã‚ã‚‰ãªã„ï¼ˆÂ±0.003ï¼‰ã€‚\n",
        "\n",
        "GNNExplainerã®çµæœ: é€Ÿåº¦ï¼ˆVelX/Yï¼‰ã«ç´„0.13ã®æ˜ç¢ºãªé‡è¦åº¦ãŒã‚ã‚‹ã€‚\n",
        "\n",
        "ã€è€ƒå¯Ÿã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã€‘\n",
        "\n",
        "ã€Œç‰¹å¾´é‡æ¶ˆå»æ™‚ã®ç²¾åº¦ä½ä¸‹ï¼ˆPFIï¼‰ã«ãŠã„ã¦ã¯é€Ÿåº¦æˆåˆ†ã®å¯„ä¸ãŒå¾®å°ã§ã‚ã£ãŸä¸€æ–¹ã§ã€GNNExplainerã«ã‚ˆã‚‹è§£æã§ã¯é€Ÿåº¦æˆåˆ†ã«å¯¾ã—ä¸€è²«ã—ãŸé‡è¦åº¦ãŒèªã‚ã‚‰ã‚ŒãŸã€‚\n",
        "\n",
        "ã“ã‚Œã¯ã€æœ¬ãƒ¢ãƒ‡ãƒ«ãŒåº§æ¨™æƒ…å ±ã®ã¿ã§é«˜ã„äºˆæ¸¬ç²¾åº¦ã‚’ç¶­æŒå¯èƒ½ã§ã‚ã‚ŠãªãŒã‚‰ã‚‚ã€ç‰©ç†æå¤±ï¼ˆPhysics Lossï¼‰ã«ã‚ˆã‚‹æ‹˜æŸã‚’èª²ã—ãŸã“ã¨ã§ã€å†…éƒ¨çš„ãªæ„æ€æ±ºå®šãƒ—ãƒ­ã‚»ã‚¹ã«ãŠã„ã¦é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç©æ¥µçš„ãªåˆ¤æ–­ææ–™ã¨ã—ã¦æ¡ç”¨ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚ã™ãªã‚ã¡ã€PIGNNã¯å˜ãªã‚‹ãƒ©ãƒ™ãƒ«ã®ç¶™ãæ¥ãï¼ˆFittingï¼‰ã§ã¯ãªãã€ç‰©ç†çš„ãªã€å‹¢ã„ã€ã¨ã„ã†æ–‡è„ˆã‚’ç†è§£ã—ãŸä¸Šã§æˆå¦ã‚’äºˆæ¸¬ã—ã¦ã„ã‚‹ã¨è¨€ãˆã‚‹ã€‚ã€"
      ],
      "metadata": {
        "id": "5PjC2mqEEzrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# æˆåŠŸã¨å¤±æ•—ã®æ¯”è¼ƒè§£æ (PIGNN v7å¯¾å¿œç‰ˆ)\n",
        "# ==========================================\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. ãƒ‡ãƒ¼ã‚¿ã®ä»•åˆ†ã‘ç”¨ãƒªã‚¹ãƒˆ\n",
        "success_importances = []\n",
        "failure_importances = []\n",
        "\n",
        "num_samples = 150  # çµ±è¨ˆçš„å®‰å®šæ€§ã®ãŸã‚ã«150ã‚·ãƒ¼ãƒ³ã‚’æ¨å¥¨\n",
        "count = 0\n",
        "\n",
        "print(f\"{num_samples}ã‚·ãƒ¼ãƒ³ã‚’æˆåŠŸ/å¤±æ•—åˆ¥ã«è§£æã—ã¾ã™...\")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«\n",
        "model_drop_eval.eval()\n",
        "\n",
        "# tqdmã§é€²æ—ã‚’è¡¨ç¤ºã—ãªãŒã‚‰ãƒ«ãƒ¼ãƒ—\n",
        "for data in tqdm(test_loader):\n",
        "    if count >= num_samples:\n",
        "        break\n",
        "\n",
        "    # ãƒãƒƒãƒã‚’ãƒ‡ãƒã‚¤ã‚¹ã«é€ã‚Šã€å€‹åˆ¥ãƒ‡ãƒ¼ã‚¿ã«åˆ†è§£\n",
        "    data = preprocess_batch(data, device)\n",
        "    data_list = data.to_data_list()\n",
        "\n",
        "    for data_single in data_list:\n",
        "        if count >= num_samples:\n",
        "            break\n",
        "\n",
        "        # GNNExplainerã§é‡è¦åº¦ç®—å‡º\n",
        "        explanation = explainer(\n",
        "            x=data_single.x,\n",
        "            edge_index=data_single.edge_index,\n",
        "            # batchãƒ†ãƒ³ã‚½ãƒ«ã‚‚ãƒ‡ãƒã‚¤ã‚¹ã«åˆã‚ã›ã‚‹\n",
        "            batch=torch.zeros(data_single.x.size(0), dtype=torch.long).to(device)\n",
        "        )\n",
        "\n",
        "        # ãƒãƒ¼ãƒ‰ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆçµ¶å¯¾å€¤ã®å¹³å‡ï¼‰ã‚’å–å¾—\n",
        "        node_importance = explanation.node_mask.abs().mean(dim=0).cpu().numpy()\n",
        "\n",
        "        # æ­£è§£ãƒ©ãƒ™ãƒ«(y)ã«åŸºã¥ã„ã¦ä»•åˆ†ã‘\n",
        "        if data_single.y.item() == 1:\n",
        "            success_importances.append(node_importance)\n",
        "        else:\n",
        "            failure_importances.append(node_importance)\n",
        "\n",
        "        count += 1\n",
        "\n",
        "# 2. ã€é‡è¦ã€‘ãƒ©ãƒ™ãƒ«ã‚’7æ¬¡å…ƒï¼ˆv7ä»•æ§˜ï¼‰ã«æ›´æ–°\n",
        "labels = ['PosX', 'PosY', 'VelX', 'VelY', 'DistGoal', 'DistBall', 'TeamID']\n",
        "\n",
        "# å„ã‚°ãƒ«ãƒ¼ãƒ—ã®å¹³å‡ã‚’ç®—å‡º\n",
        "# ã“ã“ã§ avg_success ã® shape ã¯ (7,) ã«ãªã‚Šã¾ã™\n",
        "avg_success = np.mean(success_importances, axis=0)\n",
        "avg_failure = np.mean(failure_importances, axis=0)\n",
        "\n",
        "# 3. æ¯”è¼ƒã‚°ãƒ©ãƒ•ã®æç”»\n",
        "x = np.arange(len(labels)) # 0ã‹ã‚‰6ã¾ã§ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# æ£’ã‚°ãƒ©ãƒ•ã®æç”»\n",
        "rects1 = ax.bar(x - width/2, avg_success, width, label='Success (1)', color='forestgreen', alpha=0.8)\n",
        "rects2 = ax.bar(x + width/2, avg_failure, width, label='Failure (0)', color='crimson', alpha=0.8)\n",
        "\n",
        "# ã‚°ãƒ©ãƒ•ã®è£…é£¾\n",
        "ax.set_ylabel('Mean Importance Score', fontsize=12)\n",
        "ax.set_title('Feature Importance Comparison: Success vs Failure (PIGNN v7)', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels, fontsize=11)\n",
        "ax.legend(fontsize=12)\n",
        "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
        "\n",
        "# æ•°å€¤ãƒ©ãƒ™ãƒ«ã‚’è¡¨ç¤ºã™ã‚‹è£œåŠ©é–¢æ•°\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.3f}',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3), # 3ptä¸Šã«è¡¨ç¤º\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. æ•°å€¤ã®è¦ç´„è¡¨ç¤º\n",
        "print(\"\\n--- è§£æçµæœã®è¦ç´„ ---\")\n",
        "for i, label in enumerate(labels):\n",
        "    diff = avg_success[i] - avg_failure[i]\n",
        "    trend = \"â†‘ Successã§é‡è¦–\" if diff > 0 else \"â†“ Failureã§é‡è¦–\"\n",
        "    print(f\"[{label}] Success: {avg_success[i]:.4f} | Failure: {avg_failure[i]:.4f} | {trend}\")"
      ],
      "metadata": {
        "id": "Jqps80gtE7Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. ã€Œå¤±æ•—ã‚·ãƒ¼ãƒ³ã€ã§ã® pos_x ã®çªå‡º\n",
        "ã‚°ãƒ©ãƒ•ã‚’è¦‹ã‚‹ã¨ã€å¤±æ•—ï¼ˆ0ï¼‰ã®ã¨ãã®æ–¹ãŒ pos_x ã®é‡è¦åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0.44ï¼‰ãŒæˆåŠŸæ™‚ï¼ˆ0.32ï¼‰ã‚ˆã‚Šã‚‚æ˜ç¢ºã«é«˜ã„ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚\n",
        "\n",
        "æˆ¦è¡“çš„è§£é‡ˆ: AIã¯ã€Œå¤±æ•—ã™ã‚‹ã€ã¨åˆ¤æ–­ã™ã‚‹ã¨ãã€é¸æ‰‹ã®å‹¢ã„ã‚ˆã‚Šã‚‚**ã€Œä»Šã©ã“ã«ã„ã‚‹ã‹ï¼ˆä½ç½®ï¼‰ã€**ã¨ã„ã†ãƒã‚¬ãƒ†ã‚£ãƒ–ãªçŠ¶æ³ã‚’é‡ãè¦‹ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "è«–æ–‡ã¸ã®è¨˜è¼‰æ¡ˆ: ã€Œå¤±æ•—ã‚·ãƒ¼ãƒ³ã«ãŠã„ã¦ pos_x ã®å¯„ä¸åº¦ãŒæœ€å¤§ã¨ãªã£ãŸã“ã¨ã¯ã€ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®æˆå¦ãŒåˆæœŸé…ç½®ï¼ˆè‡ªé™£æ·±ãã§ã®ãƒœãƒ¼ãƒ«å¥ªå–ãªã©ï¼‰ã«å¼·ãä¾å­˜ã—ã¦ãŠã‚Šã€AIãŒãã®åœ°ç†çš„åˆ©ä¸åˆ©ã‚’æ•æ„Ÿã«å¯ŸçŸ¥ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚ã€\n",
        "\n",
        "2. ã€ŒæˆåŠŸã‚·ãƒ¼ãƒ³ã€ã§ã®ãƒãƒ©ãƒ³ã‚¹ã®è‰¯ã•\n",
        "æˆåŠŸï¼ˆ1ï¼‰ã®ã‚°ãƒ©ãƒ•ã‚’è¦‹ã‚‹ã¨ã€pos_x ã®ã‚¹ã‚³ã‚¢ãŒä¸‹ãŒã‚Šã€ä»£ã‚ã‚Šã«ä»–ã®è¦ç´ ã«é‡è¦åº¦ãŒåˆ†æ•£ã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "ç‰©ç†æƒ…å ±ï¼ˆé€Ÿåº¦ï¼‰ã®è²¢çŒ®: vel_x, vel_y ãŒæˆåŠŸæ™‚ã«ã‚‚ã—ã£ã‹ã‚Šã¨ç¶­æŒã•ã‚Œã¦ã„ã¾ã™ã€‚æˆåŠŸã™ã‚‹ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã«ã¯ã€å˜ãªã‚‹ä½ç½®ã®è‰¯ã•ã ã‘ã§ãªãã€**ã€Œé©åˆ‡ãªé€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã€**ãŒç¶­æŒã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’AIãŒè©•ä¾¡ã®æ ¹æ‹ ã«ã—ã¦ã„ã‚‹è¨¼æ‹ ã§ã™ã€‚\n",
        "\n",
        "3. dist_ballï¼ˆãƒœãƒ¼ãƒ«è·é›¢ï¼‰ã®æ±ºå®šçš„ãªå·®\n",
        "ã“ã“ãŒæœ€ã‚‚é¢ç™½ã„ã€Œç™ºè¦‹ã€ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚å¤±æ•—ã‚·ãƒ¼ãƒ³ã§ã¯ dist_ball ã®é‡è¦åº¦ï¼ˆ0.23ï¼‰ãŒæˆåŠŸã‚·ãƒ¼ãƒ³ï¼ˆ0.16ï¼‰ã‚ˆã‚Šåœ§å€’çš„ã«é«˜ã„ã§ã™ã€‚\n",
        "\n",
        "æˆ¦è¡“çš„è§£é‡ˆ: å¤±æ•—ã™ã‚‹ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã§ã¯ã€AIã¯ã€Œãƒœãƒ¼ãƒ«ã¨é¸æ‰‹ã®è·é›¢ï¼ˆï¼ãƒœãƒ¼ãƒ«ã¸ã®é–¢ä¸åº¦ã‚„ã‚µãƒãƒ¼ãƒˆã®é…ã‚Œï¼‰ã€ã‚’é‡è¦–ã—ã¦ã€Œã“ã‚Œã¯å¤±æ•—ã™ã‚‹ã€ã¨åˆ¤å®šã—ã¦ã„ã¾ã™ã€‚\n",
        "\n",
        "è«–æ–‡ã¸ã®è¨˜è¼‰æ¡ˆ: ã€ŒæˆåŠŸã‚·ãƒ¼ãƒ³ã¨æ¯”è¼ƒã—ã€å¤±æ•—ã‚·ãƒ¼ãƒ³ã§ dist_ball ã®å¯„ä¸ãŒé¡•è‘—ã«é«˜ã¾ã£ãŸã€‚ã“ã‚Œã¯ã€æ”»æ’ƒãŒåœæ»ãƒ»å¤±æ•—ã™ã‚‹è¦å› ã¨ã—ã¦ã€ãƒœãƒ¼ãƒ«ä¿æŒè€…ã¸ã®ã‚µãƒãƒ¼ãƒˆè·é›¢ã®ä¸è¶³ã‚’ãƒ¢ãƒ‡ãƒ«ãŒé‡è¦è¦–ã—ã¦ã„ã‚‹ã“ã¨ã‚’å®šé‡çš„ã«è£ä»˜ã‘ã¦ã„ã‚‹ã€‚ã€\n",
        "\n",
        "å’è«–ã®ã€Œçµè«–ã€ã¨ã—ã¦ã¾ã¨ã‚ã‚‹ãªã‚‰\n",
        "ã“ã®ä¸€é€£ã®ã‚°ãƒ©ãƒ•ï¼ˆ1æšç›®ã®å˜ä¸€è§£æã‹ã‚‰ã€5æšç›®ã®æ¯”è¼ƒè§£æã¾ã§ï¼‰ã‚’ä¸¦ã¹ã‚‹ã“ã¨ã§ã€ã‚ãªãŸã®ç ”ç©¶ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ç· ã‚ããã‚Œã¾ã™ã€‚\n",
        "\n",
        "ã€Œæœ¬ç ”ç©¶ã§æ§‹ç¯‰ã—ãŸPIGNNã¯ã€å˜ã«é«˜ã„äºˆæ¸¬ç²¾åº¦ã‚’é”æˆã™ã‚‹ã®ã¿ãªã‚‰ãšã€GNNExplainerã‚’ç”¨ã„ãŸå¯è¦–åŒ–ã«ã‚ˆã‚Šã€ãã®äºˆæ¸¬æ ¹æ‹ ãŒã‚µãƒƒã‚«ãƒ¼æˆ¦è¡“ãŠã‚ˆã³ç‰©ç†çš„å¦¥å½“æ€§ã¨åˆè‡´ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚\n",
        "\n",
        "ç‰¹ã«æˆåŠŸãƒ»å¤±æ•—åˆ¥ã®æ¯”è¼ƒè§£æã«ã‚ˆã‚Šã€ã€ä½ç½®æƒ…å ±ã«åŸºã¥ãçŠ¶æ³åˆ¤æ–­ã€ã¨ã€é€Ÿåº¦æƒ…å ±ã«åŸºã¥ãå‹¢ã„ã®è©•ä¾¡ã€ã®åŒæ–¹ãŒãƒ¢ãƒ‡ãƒ«å†…ã§æœ‰æ©Ÿçš„ã«çµ±åˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã¨ãªã£ãŸã€‚ã“ã‚Œã¯ã€ç‰©ç†æå¤±ã‚’èª²ã—ãŸå­¦ç¿’ãŒã€AIã«å¯¾ã—ã¦å˜ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã®ç›¸é–¢ã‚’è¶…ãˆãŸã€æˆ¦è¡“çš„ãªã€æ–‡è„ˆã€ã®ç†è§£ã‚’ä¿ƒã—ãŸæˆæœã§ã‚ã‚‹ã¨è¨€ãˆã‚‹ã€‚"
      ],
      "metadata": {
        "id": "Ca3jxdbbGN1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ç‰©ç†ãƒã‚¤ã‚¢ã‚¹ã®æŠ½å‡º:ãƒ¢ãƒ‡ãƒ«å†…ã® physics_biasï¼ˆå¼ï¼š$\\exp(-\\text{dist\\_future}/5)$ï¼‰ã®å€¤ã‚’å…¨ã‚¨ãƒƒã‚¸ï¼ˆé¸æ‰‹ãƒšã‚¢ï¼‰åˆ†å–ã‚Šå‡ºã™ã€‚ãƒˆãƒƒãƒ—ãƒšã‚¢ã®ç‰¹å®š:ãã®å€¤ãŒæœ€ã‚‚é«˜ã„ï¼ˆï¼å°†æ¥æ¿€çªã™ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ï¼‰æ•µå‘³æ–¹ãƒšã‚¢ã‚’ç‰¹å®šã™ã‚‹ã€‚ãƒ”ãƒƒãƒå›³ã¸ã®ãƒ—ãƒ­ãƒƒãƒˆ:ç¾åœ¨ã®é¸æ‰‹ä½ç½®ã‹ã‚‰ã€é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¼¸ã°ã—ã€ãã®å…ˆç«¯ï¼ˆ$\\tau=1.5$ç§’å¾Œï¼‰ã§ã€Œè¡çªã€ãŒèµ·ãã¦ã„ã‚‹æ§˜å­ã‚’å¯è¦–åŒ–ã™ã‚‹ã€‚2. ãªãœã“ã‚ŒãŒã€Œã‚«ã‚¦ãƒ³ã‚¿ãƒ¼å¤±æ•—ã€ã®æ ¹æ‹ ã«ãªã‚‹ã®ã‹ã‚‚ã—ã€å¯è¦–åŒ–ã—ãŸå›³ã§ä»¥ä¸‹ã®ç¾è±¡ãŒèµ·ãã¦ã„ã‚Œã°ã€ã‚ãªãŸã®ãƒ¢ãƒ‡ãƒ«ã®æ­£ã—ã•ãŒå®¢è¦³çš„ã«è¨¼æ˜ã•ã‚Œã¾ã™ã€‚ç¾è±¡: æ”»æ’ƒå´ã®ãƒ‘ã‚¹ã®å—ã‘æ‰‹ã«å‘ã‹ã£ã¦ã€å®ˆå‚™å´ã®DFã®å°†æ¥äºˆæ¸¬ä½ç½®ãŒé‡ãªã£ã¦ã„ã‚‹ã€‚è§£é‡ˆ: ã€ŒAIã¯ä»Šã®é…ç½®ã ã‘ã‚’è¦‹ã¦ã„ã‚‹ã®ã§ã¯ãªã„ã€‚DFã®æˆ»ã‚‹ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚’è¦‹ã¦ã€1.5ç§’å¾Œã«ã¯ãƒ‘ã‚¹ã‚«ãƒƒãƒˆã‚„ãƒãƒ£ãƒ¼ã‚¸ãŒèµ·ãã‚‹ï¼ˆï¼ç‰©ç†ãƒã‚¤ã‚¢ã‚¹ãŒè·³ã­ä¸ŠãŒã‚‹ï¼‰ã“ã¨ã‚’æ¤œçŸ¥ã—ã€ã ã‹ã‚‰ã€ã“ã®ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã¯å¤±æ•—ã™ã‚‹ã€ã¨åˆ¤æ–­ã—ãŸã€ã€‚"
      ],
      "metadata": {
        "id": "MuiaGPAXBv4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ãƒã‚¤ã‚¢ã‚¹ã®å¯è¦–åŒ–"
      ],
      "metadata": {
        "id": "sYGevTJQeSGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# æç”»ã®å‰ã«ã“ã‚Œã‚’å…¥ã‚Œã¦ãã ã•ã„\n",
        "sample = next(iter(test_loader))\n",
        "# æœ€åˆã®1ãƒãƒƒãƒåˆ†ã®ãƒãƒ¼ãƒ IDã®ä¸­èº«ã‚’ã™ã¹ã¦è¡¨ç¤º\n",
        "print(\"--- TeamID Raw Data Check ---\")\n",
        "print(sample.x[:, 6])\n",
        "print(\"-----------------------------\")\n",
        "\n",
        "# ã‚‚ã—ã“ã“ã§ 0.0 ã—ã‹å‡ºã¦ã“ãªã„ãªã‚‰ã€ãƒ‡ãƒ¼ã‚¿ã®ä½œã‚Šç›´ã—ãŒå¿…è¦ã§ã™ã€‚\n",
        "# ã‚‚ã— 1.0 ã‚„ 2.0 ãŒæ··ã–ã£ã¦ã„ã‚‹ãªã‚‰ã€ä»¥ä¸‹ã®ã€Œçµ¶å¯¾è‰²åˆ†ã‘ã‚³ãƒ¼ãƒ‰ã€ã§ç›´ã‚Šã¾ã™ã€‚"
      ],
      "metadata": {
        "id": "acgKVh70hQLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_physics_bias(model, data, tau=1.5):\n",
        "    \"\"\"\n",
        "    ãƒ¢ãƒ‡ãƒ«å†…ã®ç‰©ç†ãƒã‚¤ã‚¢ã‚¹ã‚’è§£æã™ã‚‹é–¢æ•°ï¼ˆåå‰ã‚’ä¿®æ­£ï¼‰\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pos = data.pos\n",
        "        vel = data.vel\n",
        "        edge_index = data.edge_index\n",
        "\n",
        "        # æœªæ¥ä½ç½®ã®äºˆæ¸¬\n",
        "        pos_pred = pos + vel * tau\n",
        "\n",
        "        # ã‚¨ãƒƒã‚¸ã”ã¨ã®æœªæ¥è·é›¢ã¨ç‰©ç†ãƒã‚¤ã‚¢ã‚¹\n",
        "        row, col = edge_index\n",
        "        dist_future = torch.norm(pos_pred[row] - pos_pred[col], dim=-1)\n",
        "        physics_bias = torch.exp(-dist_future / 1.0)#5.0ã‹ã‚‰1.0ã«ä¿®æ­£\n",
        "\n",
        "        # æœ€å¤§ãƒã‚¤ã‚¢ã‚¹ã®å–å¾—\n",
        "        top_idx = torch.argmax(physics_bias)\n",
        "        top_pair = (edge_index[0, top_idx].item(), edge_index[1, top_idx].item())\n",
        "\n",
        "    # æˆ»ã‚Šå€¤ã®ã‚­ãƒ¼ã‚’ 'max_bias' ã«ä¿®æ­£ã—ã¦ã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆ\n",
        "    return {\n",
        "        \"top_pair\": top_pair,\n",
        "        \"max_bias\": physics_bias[top_idx].item(), # ã“ã“ã‚’ bias_value ã‹ã‚‰ max_bias ã¸ä¿®æ­£\n",
        "        \"pos_pred\": pos_pred\n",
        "    }"
      ],
      "metadata": {
        "id": "b6RnU4u7DYFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_pignn_v7_absolute_colors(model, loader, sample_idx=0, tau=1.5):\n",
        "    model.eval()\n",
        "    # ãƒ‡ãƒ¼ã‚¿ã‚’1ã¤å–å¾—\n",
        "    batch = next(iter(loader)).to(device)\n",
        "    input_data = preprocess_batch(batch.clone(), device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(input_data)\n",
        "        probs = torch.exp(out)\n",
        "        preds = out.argmax(dim=1)\n",
        "\n",
        "    # æç”»å¯¾è±¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æŠ½å‡º\n",
        "    mask = (batch.batch == sample_idx)\n",
        "    pos = batch.pos[mask].cpu().numpy()\n",
        "    vel = batch.vel[mask].cpu().numpy()\n",
        "\n",
        "    # ã€æœ€é‡è¦ã€‘team_id (index 6) ã‚’ç›´æ¥å–å¾—ã—ã¦ä¸­èº«ã‚’ç¢ºèª\n",
        "    team_ids = input_data.x[mask, 6].cpu().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    # ãƒ”ãƒƒãƒèƒŒæ™¯\n",
        "    ax.set_facecolor('#f0f0f0')\n",
        "    ax.add_patch(plt.Rectangle((-52.5, -34), 105, 68, fill=False, color='black', lw=2))\n",
        "    ax.plot([0, 0], [-34, 34], color='black', alpha=0.3)\n",
        "\n",
        "    for i in range(len(pos)):\n",
        "        t_val = team_ids[i]\n",
        "\n",
        "        # --- åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã‚’ã€Œç¯„å›²ã€ã«ã—ã¦èª¤å·®ã‚’è¨±å®¹ ---\n",
        "        if t_val > 1.5:          # Ball (2.0)\n",
        "            color, marker, size, z = '#FFD700', '*', 500, 15 # Gold\n",
        "            lbl = \"Ball\"\n",
        "        elif t_val > 0.5:        # Defender (1.0)\n",
        "            color, marker, size, z = '#EE3333', 'o', 250, 10 # Red\n",
        "            lbl = \"Defender (Away)\"\n",
        "        else:                    # Attacker (0.0)\n",
        "            color, marker, size, z = '#3366FF', 'o', 250, 10 # Blue\n",
        "            lbl = \"Attacker (Home)\"\n",
        "\n",
        "        # æç”»\n",
        "        ax.scatter(pos[i, 0], pos[i, 1], c=color, marker=marker, s=size,\n",
        "                   edgecolors='black', linewidths=1.2, zorder=z, label=lbl)\n",
        "\n",
        "        # é€Ÿåº¦ãƒ™ã‚¯ãƒˆãƒ«\n",
        "        ax.arrow(pos[i, 0], pos[i, 1], vel[i, 0]*tau, vel[i, 1]*tau,\n",
        "                 head_width=0.8, head_length=1.0, fc=color, ec=color,\n",
        "                 alpha=0.3, zorder=z-1)\n",
        "\n",
        "    # ç‰©ç†ãƒã‚¤ã‚¢ã‚¹ã®æç”»ï¼ˆç·‘ã®Xå°ã¨ç‚¹ç·šï¼‰\n",
        "    res = analyze_physics_bias(model, batch.to_data_list()[sample_idx], tau=tau)\n",
        "    p1, p2 = res[\"top_pair\"] # ã‚¿ãƒ—ãƒ«ãªã®ã§ãã®ã¾ã¾å—ã‘å–ã‚‹ã ã‘ã§OK\n",
        "    p1, p2 = int(p1), int(p2) # å¿µã®ãŸã‚æ•´æ•°å‹ã«å¤‰æ›\n",
        "    ax.plot([pos[p1,0], pos[p2,0]], [pos[p1,1], pos[p2,1]], 'green', linestyle='--', lw=2, alpha=0.6)\n",
        "    ax.scatter(res[\"pos_pred\"][p1,0].cpu(), res[\"pos_pred\"][p1,1].cpu(),\n",
        "               color='green', marker='X', s=350, edgecolors='white', label='Conflict Point', zorder=20)\n",
        "\n",
        "    # ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±\n",
        "    res_str = \"SUCCESS\" if preds[sample_idx] == 1 else \"FAILURE\"\n",
        "    plt.title(f\"PIGNN v7 Tactical Analysis: {res_str}\\n\"\n",
        "              f\"AI Prediction Prob: {probs[sample_idx, 1]:.2%} | Max Physics Bias: {res['max_bias']:.3f}\",\n",
        "              fontsize=14, fontweight='bold')\n",
        "\n",
        "    # å‡¡ä¾‹ã®é‡è¤‡ã‚’å‰Šé™¤\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    by_label = dict(zip(labels, handles))\n",
        "    ax.legend(by_label.values(), by_label.keys(), loc='upper right', frameon=True, shadow=True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "visualize_pignn_v7_absolute_colors(model, test_loader, sample_idx=0)"
      ],
      "metadata": {
        "id": "qCRYWGTXgvJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# ==========================================\n",
        "# 1. ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆå…¨ä½“ã‹ã‚‰æœ€å°ãƒã‚¤ã‚¢ã‚¹ã‚’æ¢ç´¢\n",
        "# ==========================================\n",
        "model.eval()\n",
        "min_bias = float('inf')\n",
        "low_bias_data = None\n",
        "low_bias_idx = -1\n",
        "\n",
        "print(\"ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‹ã‚‰æœ€ã‚‚ç‰©ç†çš„ã«å®‰å®šã—ãŸã‚·ãƒ¼ãƒ³ã‚’æ¢ç´¢ä¸­...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    # test_set ã¯ Data ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆã§ã‚ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™\n",
        "    for i, data in enumerate(test_set):\n",
        "        # ãƒ‡ãƒ¼ã‚¿ã®ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•\n",
        "        data_to_device = data.to(device)\n",
        "\n",
        "        # è§£æé–¢æ•°ã‚’å‘¼ã³å‡ºã—ï¼ˆtau=1.5ç§’å¾Œã®æœªæ¥ã‚’äºˆæ¸¬ï¼‰\n",
        "        res = analyze_physics_bias(model, data_to_device, tau=1.5)\n",
        "\n",
        "        current_max_bias = res['max_bias']\n",
        "\n",
        "        # æœ€å°å€¤ã‚’æ›´æ–°\n",
        "        if current_max_bias < min_bias:\n",
        "            min_bias = current_max_bias\n",
        "            low_bias_idx = i\n",
        "            low_bias_data = data_to_device\n",
        "\n",
        "print(f\"âœ… æ¢ç´¢å®Œäº†\")\n",
        "print(f\"ç™ºè¦‹ã•ã‚ŒãŸæœ€å°ãƒã‚¤ã‚¢ã‚¹: {min_bias:.4f}\")\n",
        "print(f\"è©²å½“ãƒ‡ãƒ¼ã‚¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹: {low_bias_idx}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. ä¿®æ­£ç‰ˆï¼šIndexErrorå›é¿ç”¨å¯è¦–åŒ–å‘¼ã³å‡ºã—\n",
        "# ==========================================\n",
        "\n",
        "def visualize_specific_scene(model, data_list, target_idx, tau=1.5):\n",
        "    \"\"\"\n",
        "    ç‰¹å®šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã ã‘ã‚’æŠ½å‡ºã—ã€\n",
        "    ãƒãƒƒãƒã¨ã—ã¦å¯è¦–åŒ–é–¢æ•°ã«æ¸¡ã™ã“ã¨ã§ IndexError ã‚’é˜²ã\n",
        "    \"\"\"\n",
        "    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ãƒ‡ãƒ¼ã‚¿1æšã ã‘ã‚’å«ã‚€ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
        "    single_data_list = [data_list[target_idx]]\n",
        "\n",
        "    # ãƒãƒƒãƒã‚µã‚¤ã‚º1ã®å°‚ç”¨ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ\n",
        "    single_loader = DataLoader(single_data_list, batch_size=1)\n",
        "\n",
        "    # æ—¢å­˜ã®å¯è¦–åŒ–é–¢æ•°ã‚’å‘¼ã³å‡ºã—\n",
        "    # ãƒãƒƒãƒå†…ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯å¿…ãš 0 ã«ãªã‚‹\n",
        "    visualize_pignn_v7_absolute_colors(model, single_loader, sample_idx=0, tau=tau)\n",
        "\n",
        "# å®Ÿè¡Œï¼šç‰©ç†çš„ã«æœ€ã‚‚ã€Œç¶ºéº—ã€ãªã‚·ãƒ¼ãƒ³ã‚’æç”»\n",
        "visualize_specific_scene(model, test_set, low_bias_idx, tau=1.5)"
      ],
      "metadata": {
        "id": "Ur3rTejiX-3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_most_intense_duel(model, data_list, tau=1.5):\n",
        "    model.eval()\n",
        "    max_duel_bias = -1.0\n",
        "    best_idx = -1\n",
        "\n",
        "    print(\"æ”»å®ˆãŒæœ€ã‚‚æ¿€ã—ãã€ã¶ã¤ã‹ã‚‹ã€ã‚·ãƒ¼ãƒ³ã‚’æ¢ç´¢ä¸­...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(data_list):\n",
        "            data_to_device = data.to(device)\n",
        "            # å…¨ãƒšã‚¢ã®ãƒã‚¤ã‚¢ã‚¹è©³ç´°ã‚’å–å¾—\n",
        "            res = analyze_physics_bias(model, data_to_device, tau=tau)\n",
        "\n",
        "            p1, p2 = res[\"top_pair\"]\n",
        "            # ãƒãƒ¼ãƒ IDã‚’å–å¾— (index 6)\n",
        "            t1 = data.x[p1, 6].item()\n",
        "            t2 = data.x[p2, 6].item()\n",
        "\n",
        "            # ç•°ãªã‚‹ãƒãƒ¼ãƒ åŒå£«ï¼ˆ0.0:Home vs 1.0:Awayï¼‰ã®è¡çªã®ã¿ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã™ã‚‹\n",
        "            # 1.0 - 0.0 = 1.0 ã®çµ¶å¯¾å€¤ã§åˆ¤å®š\n",
        "            if abs(t1 - t2) == 1.0:\n",
        "                if res['max_bias'] > max_duel_bias:\n",
        "                    max_duel_bias = res['max_bias']\n",
        "                    best_idx = i\n",
        "\n",
        "    print(f\"ç™ºè¦‹ï¼ æœ€å¤§æ”»å®ˆè¡çªãƒã‚¤ã‚¢ã‚¹: {max_duel_bias:.4f} (Index: {best_idx})\")\n",
        "    return best_idx\n",
        "\n",
        "# 1. æ¿€ã—ã„ç«¶ã‚Šåˆã„ã‚·ãƒ¼ãƒ³ã‚’ç‰¹å®š\n",
        "duel_idx = find_most_intense_duel(model, test_set)\n",
        "\n",
        "# 2. å¯è¦–åŒ–\n",
        "if duel_idx != -1:\n",
        "    visualize_specific_scene(model, test_set, duel_idx, tau=1.5)\n",
        "else:\n",
        "    print(\"æ¡ä»¶ã«åˆã†ã‚·ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")"
      ],
      "metadata": {
        "id": "zJvzmHVsY30G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}