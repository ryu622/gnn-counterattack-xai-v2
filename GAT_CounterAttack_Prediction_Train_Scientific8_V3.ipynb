{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP4GbtlnFI0O1nEX29lEhlr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryu622/gnn-counterattack-xai-v2/blob/fix%2Ffile-clean/GAT_CounterAttack_Prediction_Train_Scientific8_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#シード値\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    # Python自体の乱数固定\n",
        "    random.seed(seed)\n",
        "    # OS環境の乱数固定\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    # Numpyの乱数固定\n",
        "    np.random.seed(seed)\n",
        "    # PyTorchの乱数固定\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # マルチGPUの場合\n",
        "    # 計算の決定論的挙動を強制（これを入れると少し遅くなることがありますが、再現性は完璧になります）\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# 好きな数字（42が一般的）で固定\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "QKyaHmiRGtjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcw4b0y1F0PK"
      },
      "outputs": [],
      "source": [
        "#GoogleDriveをマウント\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Driveを仮想ファイルシステムにマウント\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 必須モジュールのインポート\n",
        "!pip install torch_geometric\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch import optim\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import re\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# 表示設定\n",
        "np.set_printoptions(suppress=True, precision=3)\n",
        "pd.set_option('display.precision', 3)    # 小数点以下の表示桁\n",
        "pd.set_option('display.max_rows', 50)   # 表示する行数の上限\n",
        "pd.set_option('display.max_columns', 15)  # 表示する列数の上限\n",
        "%precision 3"
      ],
      "metadata": {
        "id": "K7DHFGzCGYlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用データは以前までと同様"
      ],
      "metadata": {
        "id": "EpfKnRXp6DxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "# ==========================================\n",
        "# 1. 補助関数: 試合単位のアンダーサンプリング\n",
        "# ==========================================\n",
        "def balance_dataset_by_undersampling(data_list):\n",
        "    \"\"\"\n",
        "    シークエンス単位で1:1に調整。訓練データにのみ適用。\n",
        "    \"\"\"\n",
        "    seq_groups = defaultdict(list)\n",
        "    for d in data_list:\n",
        "        sid = int(d.sequence_id.item()) if torch.is_tensor(d.sequence_id) else int(d.sequence_id)\n",
        "        seq_groups[sid].append(d)\n",
        "\n",
        "    l0_groups, l1_groups = [], []\n",
        "    for sid, frames in seq_groups.items():\n",
        "        label = int(frames[0].y.item())\n",
        "        if label == 0: l0_groups.append(frames)\n",
        "        else: l1_groups.append(frames)\n",
        "\n",
        "    # 少ない方（成功）に合わせて失敗を削る\n",
        "    min_size = min(len(l0_groups), len(l1_groups))\n",
        "    sampled_l0 = random.sample(l0_groups, min_size)\n",
        "    sampled_l1 = l1_groups # 成功は全数保持\n",
        "\n",
        "    balanced_list = [frame for group in (sampled_l0 + sampled_l1) for frame in group]\n",
        "    random.shuffle(balanced_list)\n",
        "\n",
        "    print(f\"    [Sampling] Success Seqs: {len(sampled_l1)} | Failure Seqs: {len(sampled_l0)} | Total Frames: {len(balanced_list)}\")\n",
        "    return balanced_list\n",
        "\n"
      ],
      "metadata": {
        "id": "Rc2Wl21OBF67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルの定義（PIGNNのオリジナルクラス）"
      ],
      "metadata": {
        "id": "ocDgeAXM6Ifi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_geometric.utils import softmax\n",
        "from torch_geometric.data import Data\n",
        "import os\n",
        "\n",
        "# ==========================================\n",
        "# 1. 前処理関数の定義\n",
        "# ==========================================\n",
        "def preprocess_batch(data, device):\n",
        "    # スケーリングはBuilder側で行われているため、ここでは型変換とDevice転送に集中\n",
        "    data.x = data.x.float()\n",
        "    data.pos = data.pos.float()\n",
        "    data.vel = data.vel.float()\n",
        "    return data.to(device)\n",
        "\n",
        "# ==========================================\n",
        "# 2. モデル定義（チーム属性によるメッセージ分岐の実装）\n",
        "# ==========================================\n",
        "class PIGNNLayer(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, tau=1.5):\n",
        "        super(PIGNNLayer, self).__init__(aggr='add')\n",
        "        self.tau = tau\n",
        "        self.lin = nn.Linear(in_channels, out_channels)\n",
        "        self.att = nn.Parameter(torch.Tensor(1, out_channels * 2))\n",
        "        nn.init.xavier_uniform_(self.att)\n",
        "\n",
        "    def forward(self, x, edge_index, pos, vel):\n",
        "        h = self.lin(x)\n",
        "        # チームID(index 6)をメッセージパッシングに渡す\n",
        "        return self.propagate(edge_index, x=h, pos=pos, vel=vel, team=x[:, 6:7])\n",
        "\n",
        "    def message(self, x_i, x_j, pos_i, pos_j, vel_i, vel_j, edge_index_i, team_i, team_j):\n",
        "        # 理論1: 未来位置予測ベースのバイアス\n",
        "        pos_i_pred = pos_i + vel_i * self.tau\n",
        "        pos_j_pred = pos_j + vel_j * self.tau\n",
        "        dist_future = torch.norm(pos_i_pred - pos_j_pred, dim=-1, keepdim=True)\n",
        "        physics_bias = torch.exp(-dist_future / 2.0)\n",
        "\n",
        "        # 【追加】チーム関係分岐: 味方なら+0.5, 敵なら-0.5の注目度補正\n",
        "        is_teammate = (team_i == team_j).float()\n",
        "        team_bias = torch.where(is_teammate > 0.5, 0.5, -0.5)\n",
        "\n",
        "        alpha = torch.cat([x_i, x_j], dim=-1)\n",
        "        alpha = (alpha * self.att).sum(dim=-1, keepdim=True)\n",
        "        # 物理バイアス + チーム属性バイアス を統合\n",
        "        alpha = softmax(F.leaky_relu(alpha) + physics_bias + team_bias, edge_index_i)\n",
        "\n",
        "        return alpha * x_j\n",
        "\n",
        "class PIGNNClassifier(nn.Module):\n",
        "    def __init__(self, hidden_channels=64):\n",
        "        super(PIGNNClassifier, self).__init__()\n",
        "        self.conv1 = PIGNNLayer(7, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        x = F.elu(self.conv1(x, edge_index, pos, vel))\n",
        "        x = self.conv2(x, edge_index, pos, vel)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return F.log_softmax(self.lin(x), dim=1)\n",
        "\n",
        "# ==========================================\n",
        "# 3. 理論修正：集団運動学的制約（L_phys）\n",
        "# ==========================================\n",
        "def pignn_theoretical_loss(output, target, data, alpha=0.1):\n",
        "    # L_task: クラス重み付き（Success:3.3倍）\n",
        "    weights = torch.tensor([1.0, 3.3], device=output.device)\n",
        "    loss_task = F.nll_loss(output, target, weight=weights)\n",
        "\n",
        "    # L_phys: 集団推進力制約\n",
        "    # Success確率が高いほど、チーム全体の重心(全ノード平均)が右(+vx)であることを求める\n",
        "    probs = torch.exp(output)[:, 1]\n",
        "\n",
        "    # グラフごとの平均vxを算出（選手全員の動きを統合）\n",
        "    # data.batch を用いて各グラフ(シーン)の平均vxを計算\n",
        "    batch_size = output.size(0)\n",
        "    # 各グラフの平均vxを計算\n",
        "    avg_vxs = []\n",
        "    for i in range(batch_size):\n",
        "        mask = (data.batch == i)\n",
        "        avg_vxs.append(torch.mean(data.vel[mask, 0])) # 各シーンの全ノード平均vx\n",
        "\n",
        "    avg_vxs = torch.stack(avg_vxs)\n",
        "\n",
        "    # 物理損失：Success確率 × ReLU(-平均vx)\n",
        "    # シーン全体が左に流れているのに「成功」と出すと強く罰せられる\n",
        "    loss_phys = torch.mean(probs * torch.relu(-avg_vxs))\n",
        "\n",
        "    total_loss = loss_task + (alpha * loss_phys)\n",
        "    return total_loss, loss_task, loss_phys\n",
        "\n",
        "# ==========================================\n",
        "# 4. 学習・評価ループ\n",
        "# ==========================================\n",
        "'''\n",
        "def train_pignn_epoch_dynamic(model, loader, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    total_loss, total_phys = 0, 0\n",
        "\n",
        "    # アニーリング（後半10エポック以降で物理を強化）\n",
        "    if epoch <= 10:\n",
        "        current_alpha = 0.1\n",
        "    else:\n",
        "        current_alpha = min(0.1 + (epoch - 10) * 0.2, 3.0)\n",
        "\n",
        "    for data in loader:\n",
        "        data = preprocess_batch(data, device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(data)\n",
        "        loss, _, l_phys = pignn_theoretical_loss(out, data.y.view(-1), data, alpha=current_alpha)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "        total_phys += l_phys.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(loader.dataset), total_phys / len(loader.dataset), current_alpha'''\n",
        "\n",
        "def train_pignn_epoch_fixed(model, loader, optimizer, device, alpha_p):\n",
        "    \"\"\"\n",
        "    論文の実験(A案)を再現するための、alpha固定学習ループ。\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss, total_phys = 0, 0\n",
        "\n",
        "    # 動的な変更を廃止し、引数で受け取った固定値を使用\n",
        "    current_alpha = alpha_p\n",
        "\n",
        "    for data in loader:\n",
        "        data = preprocess_batch(data, device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(data)\n",
        "        # 修正されたalphaを損失関数に渡す\n",
        "        loss, _, l_phys = pignn_theoretical_loss(out, data.y.view(-1), data, alpha=current_alpha)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "        total_phys += l_phys.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(loader.dataset), total_phys / len(loader.dataset), current_alpha\n",
        "\n",
        "def test_pignn(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = preprocess_batch(data, device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == data.y.view(-1)).sum().item()\n",
        "    return correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "hIyIVn-LHCPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. メイン実行セクション (CVループ) - 幽霊データ排除版\n",
        "# ==========================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 論文同様、固定値で評価（0, 1.0, 10.0など）\n",
        "FIXED_ALPHA = 1.0\n",
        "\n",
        "v16_load_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_v16_final.pt\"\n",
        "print(f\"CV用マスターデータをロード中: {v16_load_path}\")\n",
        "checkpoint = torch.load(v16_load_path, weights_only=False)\n",
        "all_data_list = checkpoint['all_data']\n",
        "\n",
        "# --- 【修正】推測ロジックを削除し、刻印されたIDを直接取得 ---\n",
        "# 既に保存側で全データに match_id が付与されている前提です\n",
        "match_ids = sorted(list(set([int(d.match_id.item()) for d in all_data_list])))\n",
        "print(f\"検出された試合ID: {match_ids} (計 {len(match_ids)} 試合)\")\n",
        "\n",
        "# ハイパーパラメータ\n",
        "EPOCHS_CV = 30\n",
        "LR = 0.0005\n",
        "cv_final_reports = []\n",
        "best_overall_f1 = 0\n",
        "\n",
        "print(f\"PIGNN クロスバリデーション開始 (Alpha={FIXED_ALPHA} 固定モード)\\n\")\n",
        "\n",
        "for test_match in match_ids:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\" Round: Match {test_match} をテストに使用\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # 1. 刻印された match_id を信じて切り分け\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match]\n",
        "\n",
        "    # 2. 訓練データのみ1:1アンダーサンプリング\n",
        "    # ※balance_dataset_by_undersampling 関数は以前のものをそのまま使用\n",
        "    cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "\n",
        "    cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "    cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    cv_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    cv_optimizer = torch.optim.Adam(cv_model.parameters(), lr=LR)\n",
        "\n",
        "    # 3. 学習ループ (固定Alpha版)\n",
        "    for epoch in range(1, EPOCHS_CV + 1):\n",
        "        # アンパックエラーを避けるため戻り値3つを正しく受け取る\n",
        "        loss, phys, _ = train_pignn_epoch_fixed(cv_model, cv_train_loader, cv_optimizer, device, FIXED_ALPHA)\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f\"  Epoch {epoch:02d} | Loss: {loss:.4f} | Phys_L: {phys:.6f}\")\n",
        "\n",
        "    # 4. 評価\n",
        "    cv_model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in cv_test_loader:\n",
        "            data = data.to(device)\n",
        "            out = cv_model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    # スコア集計\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    current_f1 = report['1']['f1-score']\n",
        "\n",
        "    cv_final_reports.append({\n",
        "        'match': test_match,\n",
        "        'recall': report['1']['recall'],\n",
        "        'precision': report['1']['precision'],\n",
        "        'f1': current_f1\n",
        "    })\n",
        "\n",
        "    # --- α別にフォルダを分けて整理保存 ---\n",
        "    import os\n",
        "    # 保存ルートディレクトリ\n",
        "    base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "    # αごとの専用サブフォルダを作成\n",
        "    alpha_dir = os.path.join(base_model_dir, f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}\")\n",
        "    os.makedirs(alpha_dir, exist_ok=True)\n",
        "\n",
        "    # 1. 各試合(Round)ごとの重みを保存\n",
        "    model_filename = f'pignn_testmatch_{test_match}.pth'\n",
        "    model_path = os.path.join(alpha_dir, model_filename)\n",
        "    torch.save(cv_model.state_dict(), model_path)\n",
        "    print(f\" >> [Alpha {FIXED_ALPHA}] Match {test_match} weight saved.\")\n",
        "\n",
        "    # 2. そのαにおける「最高傑作」を保存\n",
        "    if current_f1 > best_overall_f1:\n",
        "        best_overall_f1 = current_f1\n",
        "        best_model_path = os.path.join(alpha_dir, f'best_overall_alpha_{FIXED_ALPHA}.pth')\n",
        "        torch.save(cv_model.state_dict(), best_model_path)\n",
        "        print(f\" [Alpha {FIXED_ALPHA}] New Best Model Saved: F1={best_overall_f1:.4f}\")\n",
        "\n",
        "    if current_f1 > best_overall_f1:\n",
        "        best_overall_f1 = current_f1\n",
        "        torch.save(cv_model.state_dict(), f'best_pignn_alpha_{FIXED_ALPHA}.pth')\n",
        "\n",
        "    print(f\" >> Result: Recall={report['1']['recall']:.4f}, Precision={report['1']['precision']:.4f}, F1={current_f1:.4f}\")\n",
        "\n",
        "# 5. 最終集計\n",
        "print(f\"\\n\\n{'#'*60}\\n Alpha={FIXED_ALPHA} CV 最終平均結果\\n{'#'*60}\")\n",
        "avg_recall = np.mean([r['recall'] for r in cv_final_reports])\n",
        "avg_f1 = np.mean([r['f1'] for r in cv_final_reports])\n",
        "avg_precision = np.mean([r['precision'] for r in cv_final_reports])\n",
        "\n",
        "print(f\"\\n[OVERALL] Avg Success Recall:    {avg_recall:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success Precision: {avg_precision:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success F1-score:  {avg_f1:.4f}\")"
      ],
      "metadata": {
        "id": "0uKC0kgrBrjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. メイン実行セクション (CVループ) - 幽霊データ排除版\n",
        "# ==========================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 論文同様、固定値で評価（0, 1.0, 10.0など）\n",
        "FIXED_ALPHA = 0\n",
        "\n",
        "v16_load_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_v16_final.pt\"\n",
        "print(f\"CV用マスターデータをロード中: {v16_load_path}\")\n",
        "checkpoint = torch.load(v16_load_path, weights_only=False)\n",
        "all_data_list = checkpoint['all_data']\n",
        "\n",
        "# --- 【修正】推測ロジックを削除し、刻印されたIDを直接取得 ---\n",
        "# 既に保存側で全データに match_id が付与されている前提です\n",
        "match_ids = sorted(list(set([int(d.match_id.item()) for d in all_data_list])))\n",
        "print(f\"検出された試合ID: {match_ids} (計 {len(match_ids)} 試合)\")\n",
        "\n",
        "# ハイパーパラメータ\n",
        "EPOCHS_CV = 30\n",
        "LR = 0.0005\n",
        "cv_final_reports = []\n",
        "best_overall_f1 = 0\n",
        "\n",
        "print(f\"PIGNN クロスバリデーション開始 (Alpha={FIXED_ALPHA} 固定モード)\\n\")\n",
        "\n",
        "for test_match in match_ids:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\" Round: Match {test_match} をテストに使用\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # 1. 刻印された match_id を信じて切り分け\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match]\n",
        "\n",
        "    # 2. 訓練データのみ1:1アンダーサンプリング\n",
        "    # ※balance_dataset_by_undersampling 関数は以前のものをそのまま使用\n",
        "    cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "\n",
        "    cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "    cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    cv_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    cv_optimizer = torch.optim.Adam(cv_model.parameters(), lr=LR)\n",
        "\n",
        "    # 3. 学習ループ (固定Alpha版)\n",
        "    for epoch in range(1, EPOCHS_CV + 1):\n",
        "        # アンパックエラーを避けるため戻り値3つを正しく受け取る\n",
        "        loss, phys, _ = train_pignn_epoch_fixed(cv_model, cv_train_loader, cv_optimizer, device, FIXED_ALPHA)\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f\"  Epoch {epoch:02d} | Loss: {loss:.4f} | Phys_L: {phys:.6f}\")\n",
        "\n",
        "    # 4. 評価\n",
        "    cv_model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in cv_test_loader:\n",
        "            data = data.to(device)\n",
        "            out = cv_model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    # スコア集計\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "    current_f1 = report['1']['f1-score']\n",
        "\n",
        "    cv_final_reports.append({\n",
        "        'match': test_match,\n",
        "        'recall': report['1']['recall'],\n",
        "        'precision': report['1']['precision'],\n",
        "        'f1': current_f1\n",
        "    })\n",
        "\n",
        "    # --- α別にフォルダを分けて整理保存 ---\n",
        "    import os\n",
        "    # 保存ルートディレクトリ\n",
        "    base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "    # αごとの専用サブフォルダを作成\n",
        "    alpha_dir = os.path.join(base_model_dir, f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}\")\n",
        "    os.makedirs(alpha_dir, exist_ok=True)\n",
        "\n",
        "    # 1. 各試合(Round)ごとの重みを保存\n",
        "    model_filename = f'pignn_testmatch_{test_match}.pth'\n",
        "    model_path = os.path.join(alpha_dir, model_filename)\n",
        "    torch.save(cv_model.state_dict(), model_path)\n",
        "    print(f\" >> [Alpha {FIXED_ALPHA}] Match {test_match} weight saved.\")\n",
        "\n",
        "    # 2. そのαにおける「最高傑作」を保存\n",
        "    if current_f1 > best_overall_f1:\n",
        "        best_overall_f1 = current_f1\n",
        "        best_model_path = os.path.join(alpha_dir, f'best_overall_alpha_{FIXED_ALPHA}.pth')\n",
        "        torch.save(cv_model.state_dict(), best_model_path)\n",
        "        print(f\" [Alpha {FIXED_ALPHA}] New Best Model Saved: F1={best_overall_f1:.4f}\")\n",
        "\n",
        "    if current_f1 > best_overall_f1:\n",
        "        best_overall_f1 = current_f1\n",
        "        torch.save(cv_model.state_dict(), f'best_pignn_alpha_{FIXED_ALPHA}.pth')\n",
        "\n",
        "    print(f\" >> Result: Recall={report['1']['recall']:.4f}, Precision={report['1']['precision']:.4f}, F1={current_f1:.4f}\")\n",
        "\n",
        "# 5. 最終集計\n",
        "print(f\"\\n\\n{'#'*60}\\n Alpha={FIXED_ALPHA} CV 最終平均結果\\n{'#'*60}\")\n",
        "avg_recall = np.mean([r['recall'] for r in cv_final_reports])\n",
        "avg_f1 = np.mean([r['f1'] for r in cv_final_reports])\n",
        "avg_precision = np.mean([r['precision'] for r in cv_final_reports])\n",
        "\n",
        "print(f\"\\n[OVERALL] Avg Success Recall:    {avg_recall:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success Precision: {avg_precision:.4f}\")\n",
        "print(f\"[OVERALL] Avg Success F1-score:  {avg_f1:.4f}\")"
      ],
      "metadata": {
        "id": "ANiHTpO0OOmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# ==========================================\n",
        "# 設定：検証したい Alpha を指定\n",
        "# ==========================================\n",
        "FIXED_ALPHA = 0  # 比較のためにここを 0 や 1.0 に切り替えて実行\n",
        "base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "alpha_folder = f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}\"\n",
        "model_load_dir = os.path.join(base_model_dir, alpha_folder)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "success_team_vxs = []\n",
        "\n",
        "print(f\"Alpha={FIXED_ALPHA} の全試合モデルを統合評価中...\")\n",
        "\n",
        "# --- CVの結果を再現するために、各試合のモデルを個別にロードしてテスト ---\n",
        "# match_ids は CV実行時と同じ [1, 2, 3, 4, 5, 6, 7]\n",
        "for test_match in match_ids:\n",
        "    # 1. その試合用のデータを抽出\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # 2. その試合で訓練されたモデル重みをロード\n",
        "    model_path = os.path.join(model_load_dir, f'pignn_testmatch_{test_match}.pth')\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Skip: {model_path} が見つかりません\")\n",
        "        continue\n",
        "\n",
        "    model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # 3. 予測と物理情報の抽出\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            # 各シーン(グラフ)ごとの物理量抽出\n",
        "            for i in range(out.size(0)):\n",
        "                mask = (data.batch == i)\n",
        "                # 論文(cite: 12)のPermutation Importanceで最重要視されたvxを計算\n",
        "                avg_vx = torch.mean(data.vel[mask, 0]).item()\n",
        "\n",
        "                if pred[i] == 1: # AIが成功(1)と予測した時のみ\n",
        "                    success_team_vxs.append(avg_vx)\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# ==========================================\n",
        "# 4. レポート表示セクション\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\" PIGNN 最終クラシフィケーションレポート (Alpha={FIXED_ALPHA})\")\n",
        "print(\"=\"*60)\n",
        "# 論文(cite: 115)の「naïve baseline」との比較を念頭に置いた出力\n",
        "print(classification_report(all_labels, all_preds, target_names=['Failure', 'Success'], zero_division=0))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\" 物理的整合性 検証 (vx = Byline to Byline Speed)\")\n",
        "print(\"=\"*60)\n",
        "if len(success_team_vxs) > 0:\n",
        "    avg_team_vx = np.mean(success_team_vxs)\n",
        "    # 論文(cite: 196)で「highest impact」とされたvxの方向性を確認\n",
        "    positive_ratio = np.sum(np.array(success_team_vxs) > 0) / len(success_team_vxs)\n",
        "\n",
        "    print(f\"成功予測シーンの平均 vx: {avg_team_vx:.4f} m/s\")\n",
        "    print(f\"右向き(正方向)への推進力割合: {positive_ratio*100:.1f} %\")\n",
        "\n",
        "    # 論文(cite: 6, 33)の「high speed attack」の定義に基づき評価\n",
        "    if positive_ratio > 0.65:\n",
        "        print(\">> 判定: 物理的妥当。モデルは論文の定義通り『前方への速度』を重視しています。\")\n",
        "    else:\n",
        "        print(\">> 判定: 物理的乖離あり。戦術的特徴よりもノイズを学習している可能性があります。\")\n",
        "else:\n",
        "    print(\"Successと予測されたフレームがありませんでした。\")\n",
        "\n",
        "# 5. 混同行列の描画\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens' if FIXED_ALPHA > 0 else 'Blues',\n",
        "            xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title(f'Confusion Matrix (Alpha={FIXED_ALPHA})')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YobBTnxNrLh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# ==========================================\n",
        "# 設定：検証したい Alpha を指定\n",
        "# ==========================================\n",
        "FIXED_ALPHA = 1.0  # 比較のためにここを 0 や 1.0 に切り替えて実行\n",
        "base_model_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models\"\n",
        "alpha_folder = f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}\"\n",
        "model_load_dir = os.path.join(base_model_dir, alpha_folder)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "success_team_vxs = []\n",
        "\n",
        "print(f\"Alpha={FIXED_ALPHA} の全試合モデルを統合評価中...\")\n",
        "\n",
        "# --- CVの結果を再現するために、各試合のモデルを個別にロードしてテスト ---\n",
        "# match_ids は CV実行時と同じ [1, 2, 3, 4, 5, 6, 7]\n",
        "for test_match in match_ids:\n",
        "    # 1. その試合用のデータを抽出\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # 2. その試合で訓練されたモデル重みをロード\n",
        "    model_path = os.path.join(model_load_dir, f'pignn_testmatch_{test_match}.pth')\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\" Skip: {model_path} が見つかりません\")\n",
        "        continue\n",
        "\n",
        "    model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # 3. 予測と物理情報の抽出\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            # 各シーン(グラフ)ごとの物理量抽出\n",
        "            for i in range(out.size(0)):\n",
        "                mask = (data.batch == i)\n",
        "                # 論文(cite: 12)のPermutation Importanceで最重要視されたvxを計算\n",
        "                avg_vx = torch.mean(data.vel[mask, 0]).item()\n",
        "\n",
        "                if pred[i] == 1: # AIが成功(1)と予測した時のみ\n",
        "                    success_team_vxs.append(avg_vx)\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# ==========================================\n",
        "# 4. レポート表示セクション\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\" PIGNN 最終クラシフィケーションレポート (Alpha={FIXED_ALPHA})\")\n",
        "print(\"=\"*60)\n",
        "# 論文(cite: 115)の「naïve baseline」との比較を念頭に置いた出力\n",
        "print(classification_report(all_labels, all_preds, target_names=['Failure', 'Success'], zero_division=0))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\" 物理的整合性 検証 (vx = Byline to Byline Speed)\")\n",
        "print(\"=\"*60)\n",
        "if len(success_team_vxs) > 0:\n",
        "    avg_team_vx = np.mean(success_team_vxs)\n",
        "    # 論文(cite: 196)で「highest impact」とされたvxの方向性を確認\n",
        "    positive_ratio = np.sum(np.array(success_team_vxs) > 0) / len(success_team_vxs)\n",
        "\n",
        "    print(f\"成功予測シーンの平均 vx: {avg_team_vx:.4f} m/s\")\n",
        "    print(f\"右向き(正方向)への推進力割合: {positive_ratio*100:.1f} %\")\n",
        "\n",
        "    # 論文(cite: 6, 33)の「high speed attack」の定義に基づき評価\n",
        "    if positive_ratio > 0.65:\n",
        "        print(\">> 判定: 物理的妥当。モデルは論文の定義通り『前方への速度』を重視しています。\")\n",
        "    else:\n",
        "        print(\">> 判定: 物理的乖離あり。戦術的特徴よりもノイズを学習している可能性があります。\")\n",
        "else:\n",
        "    print(\"Successと予測されたフレームがありませんでした。\")\n",
        "\n",
        "# 5. 混同行列の描画\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens' if FIXED_ALPHA > 0 else 'Blues',\n",
        "            xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title(f'Confusion Matrix (Alpha={FIXED_ALPHA})')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "83Y6OBvdrfJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "def run_leakage_diagnostic(model, loader, device):\n",
        "    model.eval()\n",
        "    # 特徴量ラベル: 0:x, 1:y, 2:vx, 3:vy, 4:dist_goal, 5:dist_ball, 6:team_id\n",
        "    feature_names = [\"x\", \"y\", \"vx\", \"vy\", \"dist_goal\", \"dist_ball\", \"team_id\"]\n",
        "\n",
        "    print(f\"{'Removed Feature':<15} | {'Test Acc':<10} | {'Recall (S)':<10}\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    # ベースライン（全特徴量あり）\n",
        "    base_acc = test_pignn(model, loader, device)\n",
        "    print(f\"{'None (Baseline)':<15} | {base_acc:.4f}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(7):\n",
        "            correct = 0\n",
        "            tp = 0 # True Positive\n",
        "            fn = 0 # False Negative\n",
        "\n",
        "            for data in loader:\n",
        "                data = preprocess_batch(data, device)\n",
        "\n",
        "                # 特定の特徴量をゼロに置き換える\n",
        "                x_shuffled = data.x.clone()\n",
        "                x_shuffled[:, i] = 0.0\n",
        "\n",
        "                # 推論\n",
        "                out = model(Data(x=x_shuffled, edge_index=data.edge_index,\n",
        "                                 batch=data.batch, pos=data.pos, vel=data.vel))\n",
        "                pred = out.argmax(dim=1)\n",
        "\n",
        "                # 精度計算\n",
        "                y_true = data.y.view(-1)\n",
        "                correct += (pred == y_true).sum().item()\n",
        "\n",
        "                # Recall (Success) 計算\n",
        "                tp += ((pred == 1) & (y_true == 1)).sum().item()\n",
        "                fn += ((pred == 0) & (y_true == 1)).sum().item()\n",
        "\n",
        "            acc = correct / len(loader.dataset)\n",
        "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            print(f\"{feature_names[i]:<15} | {acc:.4f}     | {recall:.4f}\")\n",
        "\n",
        "# 実行\n",
        "run_leakage_diagnostic(model, test_loader, device)"
      ],
      "metadata": {
        "id": "4iQw8_UUA99Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ただのMLP"
      ],
      "metadata": {
        "id": "Dhd4oLJUFBWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# 1. MLPモデルの定義 (PIGNNと入力を完全に揃える)\n",
        "# ==========================================\n",
        "class SimpleMLPClassifier(nn.Module):\n",
        "    def __init__(self, in_channels=7, hidden_channels=64): # あなたのデータ(7次元)に合わせる\n",
        "        super(SimpleMLPClassifier, self).__init__()\n",
        "        # グラフ畳み込みを行わず、全結合層のみで判定\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_channels, hidden_channels),\n",
        "            nn.BatchNorm1d(hidden_channels), # 勾配消失を防ぎ学習を安定化\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_channels, hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_channels, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        # 重要なポイント:\n",
        "        # メッセージパッシング（近接選手の相互作用理解）をバイパスし、\n",
        "        # 全選手の平均的な統計量だけで予測を行う\n",
        "        x = global_mean_pool(data.x, data.batch) # グラフ構造を無視した平均化\n",
        "        return F.log_softmax(self.mlp(x), dim=1)\n",
        "\n",
        "# ==========================================\n",
        "# 2. MLP用のクロスバリデーション実行関数\n",
        "# ==========================================\n",
        "def run_mlp_cv(all_data_list, match_ids, device):\n",
        "    FIXED_ALPHA = \"MLP\"\n",
        "    # 保存先を独立させる\n",
        "    model_save_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/MLP_Baseline\"\n",
        "    os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "    cv_final_reports = []\n",
        "    print(f\"MLP Baseline CV開始 (Input: 7 channels)\\n\")\n",
        "\n",
        "    for test_match in match_ids:\n",
        "        print(f\"Round: Match {test_match} (MLP)\")\n",
        "\n",
        "        # PIGNNと全く同じルールで切り分け\n",
        "        test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "        train_candidates = [d for d in all_data_list if int(d.match_id.item()) != test_match]\n",
        "\n",
        "        # 論文[cite: 105]同様、訓練データのみ1:1にアンダーサンプリング\n",
        "        cv_train_set = balance_dataset_by_undersampling(train_candidates)\n",
        "\n",
        "        cv_train_loader = DataLoader(cv_train_set, batch_size=32, shuffle=True)\n",
        "        cv_test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "        # モデル初期化 (7次元)\n",
        "        model = SimpleMLPClassifier(in_channels=7).to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "        # 30エポック学習 (物理損失なしの標準的な学習)\n",
        "        model.train()\n",
        "        for epoch in range(1, 31):\n",
        "            for d in cv_train_loader:\n",
        "                d = d.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                out = model(d)\n",
        "                # 論文同様[cite: 115]の50/50ベースラインに近い条件でNLLLossを使用\n",
        "                loss = F.nll_loss(out, d.y.view(-1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # 評価 (実戦の不均衡比率のままテスト)\n",
        "        model.eval()\n",
        "        y_true, y_pred = [], []\n",
        "        with torch.no_grad():\n",
        "            for d in cv_test_loader:\n",
        "                d = d.to(device)\n",
        "                out = model(d)\n",
        "                y_true.extend(d.y.view(-1).cpu().numpy())\n",
        "                y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "        # レポート生成\n",
        "        report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "        cv_final_reports.append({\n",
        "            'match': test_match,\n",
        "            'recall': report['1']['recall'],\n",
        "            'precision': report['1']['precision'],\n",
        "            'f1': report['1']['f1-score']\n",
        "        })\n",
        "\n",
        "        # 保存 (後で可視化比較に使う)\n",
        "        torch.save(model.state_dict(), os.path.join(model_save_dir, f'mlp_match_{test_match}.pth'))\n",
        "        print(f\" >> Result: Precision={report['1']['precision']:.4f}, F1={report['1']['f1-score']:.4f}\")\n",
        "\n",
        "    # 全体の平均を計算\n",
        "    avg_precision = np.mean([r['precision'] for r in cv_final_reports])\n",
        "    avg_f1 = np.mean([r['f1'] for r in cv_final_reports])\n",
        "\n",
        "    print(f\"\\nMLP CV Final Results\")\n",
        "    print(f\"Avg Precision: {avg_precision:.4f}\")\n",
        "    print(f\"Avg F1-score:  {avg_f1:.4f}\")\n",
        "\n",
        "    return cv_final_reports\n",
        "\n",
        "# 実行\n",
        "mlp_results = run_mlp_cv(all_data_list, match_ids, device)"
      ],
      "metadata": {
        "id": "cCob8uypFD02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================\n",
        "# 1. 設定：MLPモデルのロード\n",
        "# ==========================================\n",
        "model_save_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/MLP_Baseline\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "all_preds_mlp = []\n",
        "all_labels_mlp = []\n",
        "success_vxs_mlp = []\n",
        "\n",
        "print(f\"MLP Baseline 最終統合評価を開始します...\")\n",
        "\n",
        "# CVで保存した全試合のMLPモデルをロードして評価\n",
        "for test_match in match_ids:\n",
        "    # データの抽出\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # モデルの準備\n",
        "    model = SimpleMLPClassifier(in_channels=7).to(device)\n",
        "    model_path = os.path.join(model_save_dir, f'mlp_match_{test_match}.pth')\n",
        "\n",
        "    if not os.path.exists(model_path):\n",
        "        continue\n",
        "\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            # 物理的妥当性の計算\n",
        "            for i in range(out.size(0)):\n",
        "                mask = (data.batch == i)\n",
        "                # 全ノードの平均vx（集団の推進力）\n",
        "                avg_vx = torch.mean(data.vel[mask, 0]).item()\n",
        "\n",
        "                if pred[i] == 1: # Successと予測した時のみ記録\n",
        "                    success_vxs_mlp.append(avg_vx)\n",
        "\n",
        "            all_preds_mlp.extend(pred.cpu().numpy())\n",
        "            all_labels_mlp.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# ==========================================\n",
        "# 2. レポート出力\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" MLP Baseline 最終クラシフィケーションレポート\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(all_labels_mlp, all_preds_mlp, target_names=['Fail', 'Success'], zero_division=0))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\" 物理的整合性 検証 (MLP vs 物理法則)\")\n",
        "print(\"=\"*60)\n",
        "if len(success_vxs_mlp) > 0:\n",
        "    avg_vx_mlp = np.mean(success_vxs_mlp)\n",
        "    pos_ratio_mlp = np.sum(np.array(success_vxs_mlp) > 0) / len(success_vxs_mlp)\n",
        "\n",
        "    print(f\"MLPが『成功』と予測したシーンの平均 vx: {avg_vx_mlp:.4f} m/s\")\n",
        "    print(f\"右向き(攻撃方向)への推進力割合: {pos_ratio_mlp*100:.1f} %\")\n",
        "\n",
        "    # 考察用コメント\n",
        "    if pos_ratio_mlp > 0.80:\n",
        "        print(\">> 考察: MLPは極めて高い確率で『右への速度』を成功の根拠としています。\")\n",
        "        print(\">> これは空間構造を無視し、単純な物理量のみに依存している可能性を示唆します。\")\n",
        "else:\n",
        "    print(\"Success予測なし\")\n",
        "\n",
        "# 混同行列の表示\n",
        "cm = confusion_matrix(all_labels_mlp, all_preds_mlp)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title('Confusion Matrix (MLP Baseline)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XokrwR8VFWuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PFI"
      ],
      "metadata": {
        "id": "RPuh0ClPxg2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# ==========================================\n",
        "# 1. 確定した特徴量名の定義\n",
        "# ==========================================\n",
        "feature_names = [\n",
        "    'Position_X',        # Index 0\n",
        "    'Position_Y',        # Index 1\n",
        "    'Velocity_X',        # Index 2\n",
        "    'Velocity_Y',        # Index 3\n",
        "    'Distance_to_Goal',  # Index 4\n",
        "    'Distance_to_Ball',  # Index 5\n",
        "    'Team_Flag'          # Index 6\n",
        "]\n",
        "\n",
        "def calculate_pfi_refined(model, loader, device, feature_names, model_type=\"GNN\"):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    # --- Step 1: ベースラインのF1スコアを計算 ---\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            out = model(data)\n",
        "            y_true.extend(data.y.view(-1).cpu().numpy())\n",
        "            y_pred.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "    baseline_f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "    print(f\"[{model_type}] Baseline F1: {baseline_f1:.4f}\")\n",
        "\n",
        "    importance_scores = {}\n",
        "\n",
        "    # --- Step 2: 各特徴量を順番にシャッフルして影響を測定 ---\n",
        "    for i, f_name in enumerate(feature_names):\n",
        "        shuffled_f1_list = []\n",
        "\n",
        "        # 3回試行して平均をとる（安定化のため）\n",
        "        for seed in range(3):\n",
        "            y_true_s, y_pred_s = [], []\n",
        "            with torch.no_grad():\n",
        "                for data in loader:\n",
        "                    # データをコピーして特定の特徴量だけシャッフル\n",
        "                    data_s = copy.deepcopy(data).to(device)\n",
        "                    # ノード単位でシャッフル（全選手のその項目だけをバラバラにする）\n",
        "                    perm = torch.randperm(data_s.x.size(0))\n",
        "                    data_s.x[:, i] = data_s.x[perm, i]\n",
        "\n",
        "                    out_s = model(data_s)\n",
        "                    y_true_s.extend(data_s.y.view(-1).cpu().numpy())\n",
        "                    y_pred_s.extend(out_s.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "            shuffled_f1 = f1_score(y_true_s, y_pred_s, zero_division=0)\n",
        "            shuffled_f1_list.append(shuffled_f1)\n",
        "\n",
        "        # 重要度 = ベースラインF1 - シャッフル後F1（下がれば下がるほど重要）\n",
        "        importance_scores[f_name] = baseline_f1 - np.mean(shuffled_f1_list)\n",
        "        print(f\"  > Done: {f_name}\")\n",
        "\n",
        "    return importance_scores\n",
        "\n",
        "# ==========================================\n",
        "# 2. 実行（MLPとPIGNNの両方で回す）\n",
        "# ==========================================\n",
        "\n",
        "# 1. PIGNN (Alpha=1.0) の計算\n",
        "# ※ 既にメモリ上にある最新の PIGNN モデルと test_loader を使用\n",
        "print(\"\\n PIGNN (Alpha=1.0) の重要度を算出中...\")\n",
        "pfi_pignn = calculate_pfi_refined(model, test_loader, device, feature_names, \"PIGNN\")\n",
        "\n",
        "# 2. MLP の計算\n",
        "# ※ 先ほど作成した mlp_model と test_loader を使用\n",
        "print(\"\\n MLP Baseline の重要度を算出中...\")\n",
        "# --- MLPモデルの器を再作成 ---\n",
        "mlp_model = SimpleMLPClassifier(in_channels=7).to(device)\n",
        "\n",
        "# --- Match 1 など、特定の学習済み重みをロード ---\n",
        "# (PFIはテストデータとモデルのペアが必要なため、保存したファイルを指定します)\n",
        "mlp_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/MLP_Baseline/mlp_match_1.pth\"\n",
        "\n",
        "if os.path.exists(mlp_path):\n",
        "    mlp_model.load_state_dict(torch.load(mlp_path, map_location=device))\n",
        "    print(f\" MLP Model loaded from: {mlp_path}\")\n",
        "\n",
        "    # --- 改めて PFI を実行 ---\n",
        "    print(\"\\n MLP Baseline の重要度を算出中...\")\n",
        "    pfi_mlp = calculate_pfi_refined(mlp_model, test_loader, device, feature_names, \"MLP\")\n",
        "else:\n",
        "    print(f\" ファイルが見つかりません: {mlp_path}\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. 卒論用グラフの作成\n",
        "# ==========================================\n",
        "df_pfi = pd.DataFrame({\n",
        "    'PIGNN (α=1.0)': pfi_pignn,\n",
        "    'MLP (Baseline)': pfi_mlp\n",
        "})\n",
        "\n",
        "# 横棒グラフで比較\n",
        "df_pfi.plot(kind='barh', figsize=(10, 6), width=0.8)\n",
        "plt.axvline(0, color='black', linewidth=0.8)\n",
        "plt.title('Permutation Feature Importance: PIGNN vs MLP', fontsize=14)\n",
        "plt.xlabel('Drop in F1-score (Higher means more important)', fontsize=12)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "q4OeLAlowlDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import copy\n",
        "\n",
        "# ==========================================\n",
        "# 設定：検証モード（カンニングをオフにする）\n",
        "# ==========================================\n",
        "CHEATING_OFF = True  # True にすると Index 4 (Distance_to_Goal) を 0 に固定\n",
        "# PIGNN(α=1.0) または MLP を指定して比較してください\n",
        "MODEL_TYPE = \"PIGNN\" # \"PIGNN\" または \"MLP\"\n",
        "FIXED_ALPHA = 1.0     # PIGNN の場合はフォルダ特定に使用\n",
        "\n",
        "if MODEL_TYPE == \"PIGNN\":\n",
        "    alpha_folder = f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}\"\n",
        "    model_load_dir = os.path.join(\"/content/drive/MyDrive/GNN_Football_Analysis/Models\", alpha_folder)\n",
        "else:\n",
        "    model_load_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/MLP_Baseline\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "print(f\" [{MODEL_TYPE}] 評価開始 (Distance_to_Goal 遮断: {CHEATING_OFF})\")\n",
        "\n",
        "for test_match in match_ids:\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # モデルの初期化とロード\n",
        "    if MODEL_TYPE == \"PIGNN\":\n",
        "        model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "        model_path = os.path.join(model_load_dir, f'pignn_testmatch_{test_match}.pth')\n",
        "    else:\n",
        "        model = SimpleMLPClassifier(in_channels=7).to(device)\n",
        "        model_path = os.path.join(model_load_dir, f'mlp_match_{test_match}.pth')\n",
        "\n",
        "    if not os.path.exists(model_path): continue\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "\n",
        "            # --- 【重要】入力データの加工 ---\n",
        "            if CHEATING_OFF:\n",
        "                # データをコピーし、Index 4 (Distance_to_Goal) を 0.0 で上書き\n",
        "                # これにより、モデルは「ゴールに近いかどうか」の情報を使えなくなる\n",
        "                data.x[:, 4] = 0.0\n",
        "\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# ==========================================\n",
        "# 結果表示\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "status = \"CLEAN (Cheating Off)\" if CHEATING_OFF else \"RAW (Cheating On)\"\n",
        "print(f\"  {MODEL_TYPE} 最終レポート - {status}\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(all_labels, all_preds, target_names=['Fail', 'Success'], zero_division=0))\n",
        "\n",
        "# 論文(cite: 115)の指標に基づき、F1スコアの低下率を記録しておくと考察に便利です\n",
        "final_f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "print(f\"\\nFinal F1 Score: {final_f1:.4f}\")"
      ],
      "metadata": {
        "id": "DsTo-sl2yR_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "import copy\n",
        "\n",
        "# ==========================================\n",
        "# 設定：検証モード（カンニングをオフにする）\n",
        "# ==========================================\n",
        "CHEATING_OFF = True  # True にすると Index 4 (Distance_to_Goal) を 0 に固定\n",
        "# PIGNN(α=1.0) または MLP を指定して比較してください\n",
        "MODEL_TYPE = \"MLP\" # \"PIGNN\" または \"MLP\"\n",
        "FIXED_ALPHA = 1.0     # PIGNN の場合はフォルダ特定に使用\n",
        "\n",
        "if MODEL_TYPE == \"PIGNN\":\n",
        "    alpha_folder = f\"alpha_{str(FIXED_ALPHA).replace('.', '_')}\"\n",
        "    model_load_dir = os.path.join(\"/content/drive/MyDrive/GNN_Football_Analysis/Models\", alpha_folder)\n",
        "else:\n",
        "    model_load_dir = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/MLP_Baseline\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "print(f\" [{MODEL_TYPE}] 評価開始 (Distance_to_Goal 遮断: {CHEATING_OFF})\")\n",
        "\n",
        "for test_match in match_ids:\n",
        "    test_indices = [d for d in all_data_list if int(d.match_id.item()) == test_match]\n",
        "    test_loader = DataLoader(test_indices, batch_size=32, shuffle=False)\n",
        "\n",
        "    # モデルの初期化とロード\n",
        "    if MODEL_TYPE == \"PIGNN\":\n",
        "        model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "        model_path = os.path.join(model_load_dir, f'pignn_testmatch_{test_match}.pth')\n",
        "    else:\n",
        "        model = SimpleMLPClassifier(in_channels=7).to(device)\n",
        "        model_path = os.path.join(model_load_dir, f'mlp_match_{test_match}.pth')\n",
        "\n",
        "    if not os.path.exists(model_path): continue\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "\n",
        "            # --- 【重要】入力データの加工 ---\n",
        "            if CHEATING_OFF:\n",
        "                # データをコピーし、Index 4 (Distance_to_Goal) を 0.0 で上書き\n",
        "                # これにより、モデルは「ゴールに近いかどうか」の情報を使えなくなる\n",
        "                data.x[:, 4] = 0.0\n",
        "\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# ==========================================\n",
        "# 結果表示\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "status = \"CLEAN (Cheating Off)\" if CHEATING_OFF else \"RAW (Cheating On)\"\n",
        "print(f\"  {MODEL_TYPE} 最終レポート - {status}\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(all_labels, all_preds, target_names=['Fail', 'Success'], zero_division=0))\n",
        "\n",
        "# 論文(cite: 115)の指標に基づき、F1スコアの低下率を記録しておくと考察に便利です\n",
        "final_f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "print(f\"\\nFinal F1 Score: {final_f1:.4f}\")"
      ],
      "metadata": {
        "id": "YRB8_Kp_ytJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# 1. 実験データの入力 (得られた数値を代入)\n",
        "# ==========================================\n",
        "# カンニングあり (Raw) の時の数値を以前の結果から推定・入力してください\n",
        "# ここでは比較のために概算値を設定しています\n",
        "labels = ['PIGNN (α=1.0)', 'MLP Baseline']\n",
        "\n",
        "# F1スコアの変化\n",
        "f1_cheating_on = [0.3654, 0.4200]  # カンニングあり\n",
        "f1_cheating_off = [0.1646, 0.1475] # カンニングなし (今回の結果)\n",
        "\n",
        "# 再現率 (Recall) の変化\n",
        "recall_cheating_on = [0.65, 0.64]   # 推定値（PIGNN）, MLP（レポート値）\n",
        "recall_cheating_off = [0.89, 0.23]  # 今回の結果\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "# ==========================================\n",
        "# 2. グラフ描画：F1スコアの低下比較\n",
        "# ==========================================\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# --- 左図：F1スコアの変化 ---\n",
        "ax1.bar(x - width/2, f1_cheating_on, width, label='With Goal Distance', color='skyblue', alpha=0.6)\n",
        "ax1.bar(x + width/2, f1_cheating_off, width, label='Without Goal Distance', color='blue')\n",
        "ax1.set_ylabel('F1 Score')\n",
        "ax1.set_title('Robustness Comparison: F1 Score Drop')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(labels)\n",
        "ax1.legend()\n",
        "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# --- 右図：再現率(Recall)の維持能力 ---\n",
        "ax2.bar(x - width/2, recall_cheating_on, width, label='With Goal Distance', color='salmon', alpha=0.6)\n",
        "ax2.bar(x + width/2, recall_cheating_off, width, label='Without Goal Distance', color='red')\n",
        "ax2.set_ylabel('Recall (Success detection)')\n",
        "ax2.set_title('Robustness Comparison: Recall (Tactical Awareness)')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(labels)\n",
        "ax2.legend()\n",
        "ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 3. 統計データのサマリー出力\n",
        "# ==========================================\n",
        "print(\"--- 考察用サマリー ---\")\n",
        "for i in range(len(labels)):\n",
        "    drop = (f1_cheating_on[i] - f1_cheating_off[i]) / f1_cheating_on[i] * 100\n",
        "    print(f\"{labels[i]}: F1スコア低下率 {drop:.1f}%\")"
      ],
      "metadata": {
        "id": "xC3No2xX1s2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "アテンション係数の可視化"
      ],
      "metadata": {
        "id": "nNhUKPcP6W4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "アテンション係数の可視化のためにはアテンションを戻り値として返すように修正が必要。そのためにモデルを再定義する。"
      ],
      "metadata": {
        "id": "vysk9sK24FaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PIGNNLayer(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, tau=1.5):\n",
        "        super(PIGNNLayer, self).__init__(aggr='add')\n",
        "        self.tau = tau\n",
        "        self.lin = nn.Linear(in_channels, out_channels)\n",
        "        self.att = nn.Parameter(torch.Tensor(1, out_channels * 2))\n",
        "        nn.init.xavier_uniform_(self.att)\n",
        "\n",
        "    # --- 引数 return_attention を追加 ---\n",
        "    def forward(self, x, edge_index, pos, vel, return_attention=False):\n",
        "        h = self.lin(x)\n",
        "        # チームID(index 6)をメッセージパッシングに渡す\n",
        "        out = self.propagate(edge_index, x=h, pos=pos, vel=vel, team=x[:, 6:7])\n",
        "\n",
        "        # 可視化モードの時は、内部で計算された alpha を取得する\n",
        "        if return_attention:\n",
        "            # 内部変数を保持するために一時的な保存が必要ですが、\n",
        "            # シンプルにするため、ここでは出力と共にエッジインデックスと直近のアテンションを返せるように設計します\n",
        "            return out, (edge_index, self._last_att if hasattr(self, '_last_att') else None)\n",
        "        return out\n",
        "\n",
        "    def message(self, x_i, x_j, pos_i, pos_j, vel_i, vel_j, edge_index_i, team_i, team_j):\n",
        "        pos_i_pred = pos_i + vel_i * self.tau\n",
        "        pos_j_pred = pos_j + vel_j * self.tau\n",
        "        dist_future = torch.norm(pos_i_pred - pos_j_pred, dim=-1, keepdim=True)\n",
        "        physics_bias = torch.exp(-dist_future / 2.0)\n",
        "\n",
        "        is_teammate = (team_i == team_j).float()\n",
        "        team_bias = torch.where(is_teammate > 0.5, 0.5, -0.5)\n",
        "\n",
        "        alpha = torch.cat([x_i, x_j], dim=-1)\n",
        "        alpha = (alpha * self.att).sum(dim=-1, keepdim=True)\n",
        "        alpha = softmax(F.leaky_relu(alpha) + physics_bias + team_bias, edge_index_i)\n",
        "\n",
        "        # 【可視化用】計算されたアテンションを一時保存\n",
        "        self._last_att = alpha\n",
        "        return alpha * x_j\n",
        "\n",
        "class PIGNNClassifier(nn.Module):\n",
        "    def __init__(self, hidden_channels=64):\n",
        "        super(PIGNNClassifier, self).__init__()\n",
        "        self.conv1 = PIGNNLayer(7, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    # --- 引数 return_attention を追加 ---\n",
        "    def forward(self, data, return_attention=False):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        if return_attention:\n",
        "            # conv1 からアテンションを抽出\n",
        "            x, (edge_idx_out, att_weights) = self.conv1(x, edge_index, pos, vel, return_attention=True)\n",
        "            x = F.elu(x)\n",
        "            x = self.conv2(x, edge_index, pos, vel)\n",
        "        else:\n",
        "            x = F.elu(self.conv1(x, edge_index, pos, vel))\n",
        "            x = self.conv2(x, edge_index, pos, vel)\n",
        "\n",
        "        x_pool = global_mean_pool(x, batch)\n",
        "        logits = F.log_softmax(self.lin(x_pool), dim=1)\n",
        "\n",
        "        if return_attention:\n",
        "            return logits, (edge_idx_out, att_weights)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "ExHECXGQ4Mkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "def visualize_pignn_tactical_analysis(model, data_item, device, title=\"PIGNN Tactical Analysis\"):\n",
        "    \"\"\"\n",
        "    PIGNNのアテンション係数と物理状態（座標・速度）をピッチ上に可視化する\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # データを1つだけのバッチとして扱う\n",
        "    data_item = data_item.to(device)\n",
        "\n",
        "    # モデルから予測結果とアテンション係数を抽出\n",
        "    # ※ forwardメソッドが return_attention=True で (out, (edge_index, att_weights)) を返す前提\n",
        "    with torch.no_grad():\n",
        "        out, (edge_index, att_weights) = model(data_item, return_attention=True)\n",
        "        prob = torch.softmax(out, dim=1)[0, 1].item()\n",
        "        pred = out.argmax(dim=1).item()\n",
        "        label = data_item.y.item()\n",
        "\n",
        "    # --- 1. 座標と速度の復元 ---\n",
        "    # 正規化された値 (-1~1) を実際のピッチサイズ (105m x 68m) に戻す\n",
        "    pos = data_item.pos.cpu().numpy()\n",
        "    vel = data_item.vel.cpu().numpy()\n",
        "\n",
        "    pos_plot = np.zeros_like(pos)\n",
        "    pos_plot[:, 0] = pos[:, 0] * 52.5  # X座標: -52.5 to 52.5\n",
        "    pos_plot[:, 1] = pos[:, 1] * 34.0  # Y座標: -34.0 to 34.0\n",
        "\n",
        "    # 速度ベクトルの描画用スケーリング（1秒間の移動距離を強調）\n",
        "    vel_plot = vel * 3.0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "    # --- 2. サッカー場の描画 (芝生の色 #2e7d32) ---\n",
        "    ax.set_facecolor('#2e7d32')\n",
        "    # 外枠\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=True, color='#388e3c', zorder=0))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=False, color='white', lw=3, zorder=1))\n",
        "\n",
        "    # センターライン & センターサークル\n",
        "    ax.plot([0, 0], [-34, 34], color='white', lw=3, zorder=1)\n",
        "    ax.add_patch(patches.Circle((0, 0), 9.15, edgecolor=\"white\", facecolor=\"none\", lw=3, zorder=1))\n",
        "\n",
        "    # ペナルティエリア\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((52.5-16.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "\n",
        "    # --- 3. アテンション係数（黄色い光 #FFFF00）の描画 ---\n",
        "    # 全エッジのうち、影響力の強い上位のエッジを光の線で表現\n",
        "    att_weights = att_weights.cpu().numpy().flatten()\n",
        "    edge_index = edge_index.cpu().numpy()\n",
        "\n",
        "    if len(att_weights) > 0:\n",
        "        threshold = np.percentile(att_weights, 95) # 上位○%のエッジのみ表示\n",
        "        max_att = att_weights.max()\n",
        "        for i in range(edge_index.shape[1]):\n",
        "            if att_weights[i] > threshold:\n",
        "                src, dst = edge_index[0, i], edge_index[1, i]\n",
        "                # 強さに応じて透明度(alpha)を変化させる\n",
        "                alpha_val = (att_weights[i] - threshold) / (max_att - threshold + 1e-9)\n",
        "                ax.plot([pos_plot[src, 0], pos_plot[dst, 0]],\n",
        "                        [pos_plot[src, 1], pos_plot[dst, 1]],\n",
        "                        color='#FFFF00', alpha=alpha_val * 0.7, lw=2.0 + alpha_val*3, zorder=2)\n",
        "\n",
        "    # --- 4. 選手とボールの描画 ---\n",
        "    team_ids = data_item.x[:, 6].cpu().numpy() # 7次元目のTeam_Flagを取得\n",
        "    num_nodes = pos.shape[0]\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        # チームに応じた色分け\n",
        "        if team_ids[i] == 2.0: # ボール (Gold)\n",
        "            color, marker, size, z = 'gold', '*', 600, 15\n",
        "        elif team_ids[i] == 0.0: # 攻撃チーム (Blue #0288d1)\n",
        "            color, marker, size, z = '#0288d1', 'o', 300, 10\n",
        "        else: # 守備チーム (Red #d32f2f)\n",
        "            color, marker, size, z = '#d32f2f', 'o', 300, 10\n",
        "\n",
        "        # 本体描画\n",
        "        ax.scatter(pos_plot[i, 0], pos_plot[i, 1], c=color, marker=marker, s=size,\n",
        "                   edgecolors='white', linewidth=1.5, zorder=z)\n",
        "\n",
        "        # 速度ベクトル矢印 (物理的推進力の可視化)\n",
        "        if team_ids[i] != 2.0: # 選手のみ矢印を表示\n",
        "            ax.arrow(pos_plot[i, 0], pos_plot[i, 1], vel_plot[i, 0], vel_plot[i, 1],\n",
        "                     head_width=1.0, head_length=1.2, fc='white', ec='white', alpha=0.5, zorder=z-1)\n",
        "\n",
        "    # 情報テキストの表示\n",
        "    label_str = \"SUCCESS\" if label == 1 else \"FAILURE\"\n",
        "    pred_str = \"SUCCESS\" if pred == 1 else \"FAILURE\"\n",
        "    match_result = \"CORRECT\" if label == pred else \"INCORRECT\"\n",
        "\n",
        "    ax.set_title(f\"{title}\\nActual: {label_str} | Predicted: {pred_str} ({prob:.1%})\\nResult: {match_result}\",\n",
        "                 fontsize=18, fontweight='bold', pad=20)\n",
        "\n",
        "    ax.set_xlim(-60, 60)\n",
        "    ax.set_ylim(-40, 40)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 5. 実行：成功シーンと失敗シーンの自動抽出と可視化\n",
        "# ==========================================\n",
        "def run_comparison_visualizer(model, data_list, device):\n",
        "    success_case = None\n",
        "    failure_case = None\n",
        "\n",
        "    model.eval()\n",
        "    for data in data_list:\n",
        "        with torch.no_grad():\n",
        "            out, _ = model(data.to(device), return_attention=True)\n",
        "            pred = out.argmax(dim=1).item()\n",
        "            label = data.y.item()\n",
        "\n",
        "            # AIが正解したケースから1つずつピックアップ\n",
        "            if pred == label:\n",
        "                if label == 1 and success_case is None:\n",
        "                    success_case = data\n",
        "                elif label == 0 and failure_case is None:\n",
        "                    failure_case = data\n",
        "\n",
        "        if success_case and failure_case:\n",
        "            break\n",
        "\n",
        "    if success_case:\n",
        "        visualize_pignn_tactical_analysis(model, success_case, device, title=\"Tactical Analysis: Successful Counter\")\n",
        "    if failure_case:\n",
        "        visualize_pignn_tactical_analysis(model, failure_case, device, title=\"Tactical Analysis: Failed Counter\")\n",
        "\n",
        "# 実行コマンド\n",
        "# 1. モデルのインスタンス化\n",
        "pignn_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "\n",
        "# 2. 重みのロード\n",
        "# alpha_param は現在のモデルに定義されていないため、strict=False で無視させます\n",
        "pignn_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/alpha_1_0/pignn_testmatch_1.pth\"\n",
        "state_dict = torch.load(pignn_path, map_location=device)\n",
        "pignn_model.load_state_dict(state_dict, strict=False)\n",
        "print(\" 重みのロード完了。可視化準備が整いました。\")\n",
        "\n",
        "# 3. 可視化実行\n",
        "# 先ほど作成した visualize_pignn_attention_v3 や run_comparison_visualizer を実行\n",
        "run_comparison_visualizer(pignn_model, all_data_list, device)"
      ],
      "metadata": {
        "id": "GHpvrXqn3ig2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "速度ベクトルも表示"
      ],
      "metadata": {
        "id": "ZL1PSDQx6nHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# ==========================================\n",
        "# 1. 可視化メイン関数 (速度ベクトル改善版)\n",
        "# ==========================================\n",
        "def visualize_pignn_tactical_analysis(model, data_item, device, title=\"PIGNN Tactical Analysis\"):\n",
        "    model.eval()\n",
        "    data_item = data_item.to(device)\n",
        "\n",
        "    # 推論とアテンション抽出\n",
        "    with torch.no_grad():\n",
        "        out, (edge_index, att_weights) = model(data_item, return_attention=True)\n",
        "        prob = torch.softmax(out, dim=1)[0, 1].item()\n",
        "        pred = out.argmax(dim=1).item()\n",
        "        label = data_item.y.item()\n",
        "\n",
        "    # --- 座標の復元 ---\n",
        "    pos = data_item.pos.cpu().numpy()\n",
        "    vel = data_item.vel.cpu().numpy()\n",
        "    pos_plot = np.zeros_like(pos)\n",
        "    pos_plot[:, 0] = pos[:, 0] * 52.5\n",
        "    pos_plot[:, 1] = pos[:, 1] * 34.0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "    # --- サッカー場の描画 ---\n",
        "    ax.set_facecolor('#2e7d32')\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=True, color='#388e3c', zorder=0))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=False, color='white', lw=3, zorder=1))\n",
        "    ax.plot([0, 0], [-34, 34], color='white', lw=3, zorder=1)\n",
        "    ax.add_patch(patches.Circle((0, 0), 9.15, edgecolor=\"white\", facecolor=\"none\", lw=3, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((52.5-16.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "\n",
        "    # --- アテンションの描画 (zorder=2) ---\n",
        "    att_weights = att_weights.cpu().numpy().flatten()\n",
        "    edge_index = edge_index.cpu().numpy()\n",
        "\n",
        "    if len(att_weights) > 0:\n",
        "        threshold = np.percentile(att_weights, 98) # 上位5%を表示\n",
        "        max_att = att_weights.max()\n",
        "        for i in range(edge_index.shape[1]):\n",
        "            if att_weights[i] > threshold:\n",
        "                src, dst = edge_index[0, i], edge_index[1, i]\n",
        "                alpha_val = (att_weights[i] - threshold) / (max_att - threshold + 1e-9)\n",
        "                ax.plot([pos_plot[src, 0], pos_plot[dst, 0]],\n",
        "                        [pos_plot[src, 1], pos_plot[dst, 1]],\n",
        "                        color='#FFFF00', alpha=alpha_val * 0.8, lw=2.0 + alpha_val*4, zorder=2)\n",
        "\n",
        "    # --- 速度ベクトルと選手の描画 (zorder=10-20) ---\n",
        "    team_ids = data_item.x[:, 6].cpu().numpy()\n",
        "    num_nodes = pos.shape[0]\n",
        "\n",
        "    #  速度描画用の設定\n",
        "    vel_scale = 20.0 # 矢印をはっきり見せるためのスケーリング\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        if team_ids[i] == 2.0: # ボール\n",
        "            color, marker, size, z = 'gold', '*', 600, 15\n",
        "        elif team_ids[i] == 0.0: # 攻撃\n",
        "            color, marker, size, z = '#0288d1', 'o', 300, 10\n",
        "        else: # 守備\n",
        "            color, marker, size, z = '#d32f2f', 'o', 300, 10\n",
        "\n",
        "        #  改善：ax.quiver で速度を最前面に描画 (zorder=20)\n",
        "        if team_ids[i] != 2.0:\n",
        "            ax.quiver(pos_plot[i, 0], pos_plot[i, 1],\n",
        "                      vel[i, 0], vel[i, 1],\n",
        "                      color='white', alpha=0.9,\n",
        "                      angles='xy', scale_units='xy', scale=1/vel_scale,\n",
        "                      width=0.005, headwidth=4, headlength=5, zorder=20)\n",
        "\n",
        "        # 選手ノード本体\n",
        "        ax.scatter(pos_plot[i, 0], pos_plot[i, 1], c=color, marker=marker, s=size,\n",
        "                   edgecolors='white', linewidth=1.5, zorder=15)\n",
        "\n",
        "    # テキスト表示\n",
        "    res_text = \"SUCCESS\" if pred == 1 else \"FAILURE\"\n",
        "    match_status = \"CORRECT\" if label == pred else \"INCORRECT\"\n",
        "    ax.set_title(f\"{title}\\nActual: {'SUCCESS' if label==1 else 'FAILURE'} | Predicted: {res_text} ({prob:.1%})\\nResult: {match_status}\",\n",
        "                 fontsize=18, fontweight='bold', pad=20)\n",
        "\n",
        "    ax.set_xlim(-60, 60); ax.set_ylim(-40, 40)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 2. 自動抽出 & 実行ループ\n",
        "# ==========================================\n",
        "def run_comparison_visualizer(model, data_list, device):\n",
        "    success_case, failure_case = None, None\n",
        "    model.eval()\n",
        "\n",
        "    for data in data_list:\n",
        "        with torch.no_grad():\n",
        "            # データを転送\n",
        "            d_gpu = data.to(device)\n",
        "            out, _ = model(d_gpu, return_attention=True)\n",
        "            pred = out.argmax(dim=1).item()\n",
        "            label = d_gpu.y.item()\n",
        "\n",
        "            # 正解シーンの中から抽出\n",
        "            if pred == label:\n",
        "                if label == 1 and success_case is None: success_case = data\n",
        "                elif label == 0 and failure_case is None: failure_case = data\n",
        "\n",
        "        if success_case and failure_case: break\n",
        "\n",
        "    if success_case:\n",
        "        visualize_pignn_tactical_analysis(model, success_case, device, title=\"Tactical Analysis: Successful Counter\")\n",
        "    if failure_case:\n",
        "        visualize_pignn_tactical_analysis(model, failure_case, device, title=\"Tactical Analysis: Failed Counter\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. モデルロード & 実行\n",
        "# ==========================================\n",
        "# 1. インスタンス化\n",
        "pignn_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "\n",
        "# 2. ロード (strict=False)\n",
        "pignn_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/alpha_1_0/pignn_testmatch_1.pth\"\n",
        "if os.path.exists(pignn_path):\n",
        "    state_dict = torch.load(pignn_path, map_location=device)\n",
        "    pignn_model.load_state_dict(state_dict, strict=False)\n",
        "    print(\" PIGNN重みのロードに成功しました。\")\n",
        "    # 3. 実行\n",
        "    run_comparison_visualizer(pignn_model, all_data_list, device)\n",
        "else:\n",
        "    print(f\" ファイルが見つかりません: {pignn_path}\")"
      ],
      "metadata": {
        "id": "j-H8O2ef6pWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "アテンションの数値の追加"
      ],
      "metadata": {
        "id": "_3bO8VAW-KLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "import copy\n",
        "import os\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# ==========================================\n",
        "# 1. 可視化 & 数値抽出メイン関数\n",
        "# ==========================================\n",
        "def visualize_pignn_tactical_analysis(model, data_item, device, title=\"PIGNN Tactical Analysis\"):\n",
        "    model.eval()\n",
        "    data_item = data_item.to(device)\n",
        "\n",
        "    # --- 推論とアテンション抽出 ---\n",
        "    with torch.no_grad():\n",
        "        out, (edge_index, att_weights) = model(data_item, return_attention=True)\n",
        "        prob = torch.softmax(out, dim=1)[0, 1].item()\n",
        "        pred = out.argmax(dim=1).item()\n",
        "        label = data_item.y.item()\n",
        "\n",
        "    # --- 座標と速度の復元 ---\n",
        "    pos = data_item.pos.cpu().numpy()\n",
        "    vel = data_item.vel.cpu().numpy()\n",
        "    pos_plot = np.zeros_like(pos)\n",
        "    pos_plot[:, 0] = pos[:, 0] * 52.5\n",
        "    pos_plot[:, 1] = pos[:, 1] * 34.0\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 10))\n",
        "\n",
        "    # --- サッカー場の描画 (zorder=0-1) ---\n",
        "    ax.set_facecolor('#2e7d32')\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=True, color='#388e3c', zorder=0))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=False, color='white', lw=3, zorder=1))\n",
        "    ax.plot([0, 0], [-34, 34], color='white', lw=3, zorder=1)\n",
        "    ax.add_patch(patches.Circle((0, 0), 9.15, edgecolor=\"white\", facecolor=\"none\", lw=3, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "    ax.add_patch(patches.Rectangle((52.5-16.5, -20.15), 16.5, 40.3, fill=False, color='white', lw=2, zorder=1))\n",
        "\n",
        "    # --- 2. アテンション係数の描画 & 数値出力 (zorder=2) ---\n",
        "    att_weights = att_weights.cpu().numpy().flatten()\n",
        "    edge_index = edge_index.cpu().numpy()\n",
        "\n",
        "    if len(att_weights) > 0:\n",
        "        threshold = np.percentile(att_weights, 98) # 上位2%に絞り込み\n",
        "        max_att = att_weights.max()\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\" {title} - TOP 2% ATTENTION DETAILS\")\n",
        "        print(f\"{'='*50}\")\n",
        "        print(f\"{'Source':<8} | {'Dest':<8} | {'Weight':<10} | {'Team Relation'}\")\n",
        "        print(f\"{'-'*50}\")\n",
        "\n",
        "        # チームID取得 (0:攻撃, 1:守備, 2:ボール)\n",
        "        team_ids = data_item.x[:, 6].cpu().numpy()\n",
        "\n",
        "        for i in range(edge_index.shape[1]):\n",
        "            if att_weights[i] > threshold:\n",
        "                src, dst = edge_index[0, i], edge_index[1, i]\n",
        "                weight = att_weights[i]\n",
        "\n",
        "                # チーム関係の言語化\n",
        "                rel = \"Teammate\" if team_ids[src] == team_ids[dst] else \"Opponent\"\n",
        "                if team_ids[src] == 2.0: rel = \"Ball -> Player\"\n",
        "\n",
        "                # コンソールに数値を表示\n",
        "                print(f\"Node {src:2d} -> Node {dst:2d} | {weight:.4f}     | {rel}\")\n",
        "\n",
        "                # 描画\n",
        "                alpha_val = (weight - threshold) / (max_att - threshold + 1e-9)\n",
        "                ax.plot([pos_plot[src, 0], pos_plot[dst, 0]],\n",
        "                        [pos_plot[src, 1], pos_plot[dst, 1]],\n",
        "                        color='#FFFF00', alpha=alpha_val * 0.8, lw=2.0 + alpha_val*4, zorder=2)\n",
        "\n",
        "    # --- 3. 選手と速度ベクトルの描画 (zorder=10-20) ---\n",
        "    num_nodes = pos.shape[0]\n",
        "    vel_scale = 15.0 # 速度ベクトルの視認性を確保\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        if team_ids[i] == 2.0: # ボール\n",
        "            color, marker, size, z = 'gold', '*', 600, 15\n",
        "        elif team_ids[i] == 0.0: # 攻撃チーム\n",
        "            color, marker, size, z = '#0288d1', 'o', 300, 10\n",
        "        else: # 守備チーム\n",
        "            color, marker, size, z = '#d32f2f', 'o', 300, 10\n",
        "\n",
        "        # 速度ベクトル (ax.quiver で確実に描画)\n",
        "        if team_ids[i] != 2.0:\n",
        "            ax.quiver(pos_plot[i, 0], pos_plot[i, 1],\n",
        "                      vel[i, 0], vel[i, 1],\n",
        "                      color='white', alpha=0.9,\n",
        "                      angles='xy', scale_units='xy', scale=1/vel_scale,\n",
        "                      width=0.005, headwidth=4, headlength=5, zorder=20)\n",
        "\n",
        "        # 選手ノード\n",
        "        ax.scatter(pos_plot[i, 0], pos_plot[i, 1], c=color, marker=marker, s=size,\n",
        "                   edgecolors='white', linewidth=1.5, zorder=15)\n",
        "\n",
        "    # --- タイトルと表示設定 ---\n",
        "    res_text = \"SUCCESS\" if pred == 1 else \"FAILURE\"\n",
        "    match_status = \"CORRECT\" if label == pred else \"INCORRECT\"\n",
        "    ax.set_title(f\"{title}\\nActual: {'SUCCESS' if label==1 else 'FAILURE'} | Predicted: {res_text} ({prob:.1%})\\nResult: {match_status}\",\n",
        "                 fontsize=18, fontweight='bold', pad=20)\n",
        "\n",
        "    ax.set_xlim(-60, 60); ax.set_ylim(-40, 40)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 2. 自動比較実行ループ\n",
        "# ==========================================\n",
        "def run_comparison_visualizer(model, data_list, device):\n",
        "    success_case, failure_case = None, None\n",
        "    model.eval()\n",
        "\n",
        "    for data in data_list:\n",
        "        with torch.no_grad():\n",
        "            d_gpu = data.to(device)\n",
        "            out, _ = model(d_gpu, return_attention=True)\n",
        "            pred = out.argmax(dim=1).item()\n",
        "            label = d_gpu.y.item()\n",
        "\n",
        "            if pred == label:\n",
        "                if label == 1 and success_case is None: success_case = data\n",
        "                elif label == 0 and failure_case is None: failure_case = data\n",
        "\n",
        "        if success_case and failure_case: break\n",
        "\n",
        "    if success_case:\n",
        "        visualize_pignn_tactical_analysis(model, success_case, device, title=\"Tactical Analysis: Successful Counter\")\n",
        "    if failure_case:\n",
        "        visualize_pignn_tactical_analysis(model, failure_case, device, title=\"Tactical Analysis: Failed Counter\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. 実行セクション\n",
        "# ==========================================\n",
        "# PIGNNモデルの準備\n",
        "pignn_model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "\n",
        "pignn_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Models/alpha_1_0/pignn_testmatch_1.pth\"\n",
        "if os.path.exists(pignn_path):\n",
        "    state_dict = torch.load(pignn_path, map_location=device)\n",
        "    pignn_model.load_state_dict(state_dict, strict=False)\n",
        "    print(\" PIGNN重みのロードに成功しました。\")\n",
        "\n",
        "    # 実行\n",
        "    run_comparison_visualizer(pignn_model, all_data_list, device)\n",
        "else:\n",
        "    print(f\" パスが見つかりません: {pignn_path}\")"
      ],
      "metadata": {
        "id": "fQrj-UNc-OaW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}