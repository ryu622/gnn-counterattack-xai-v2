{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOW53TQDFJYlPoBmQ4TXHOI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryu622/gnn-counterattack-xai-v2/blob/fix%2Ffile-clean/GAT_CounterAttack_Prediction_Train_Scientific8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#シード値\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    # Python自体の乱数固定\n",
        "    random.seed(seed)\n",
        "    # OS環境の乱数固定\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    # Numpyの乱数固定\n",
        "    np.random.seed(seed)\n",
        "    # PyTorchの乱数固定\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # マルチGPUの場合\n",
        "    # 計算の決定論的挙動を強制（これを入れると少し遅くなることがありますが、再現性は完璧になります）\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# 好きな数字（42が一般的）で固定\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "QKyaHmiRGtjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcw4b0y1F0PK"
      },
      "outputs": [],
      "source": [
        "#GoogleDriveをマウント\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Driveを仮想ファイルシステムにマウント\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 必須モジュールのインポート\n",
        "!pip install torch_geometric\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch import optim\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import re\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# 表示設定\n",
        "np.set_printoptions(suppress=True, precision=3)\n",
        "pd.set_option('display.precision', 3)    # 小数点以下の表示桁\n",
        "pd.set_option('display.max_rows', 50)   # 表示する行数の上限\n",
        "pd.set_option('display.max_columns', 15)  # 表示する列数の上限\n",
        "%precision 3"
      ],
      "metadata": {
        "id": "K7DHFGzCGYlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用データは以前までと同様"
      ],
      "metadata": {
        "id": "EpfKnRXp6DxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# ==========================================\n",
        "# ロード・クリーンアップ・最終確認\n",
        "# ==========================================\n",
        "v7_load_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_scientific_v9_final.pt\"\n",
        "\n",
        "try:\n",
        "    print(f\"v7 最終データをロード中: {v7_load_path}\")\n",
        "    # 統合済みファイルをロード\n",
        "    checkpoint = torch.load(v7_load_path, weights_only=False)\n",
        "\n",
        "    # v7 の Builder ですでに 7次元特徴量 (x, y, vx, vy, dist_goal, dist_ball, team_id)\n",
        "    # を付与しているため、基本的にはそのまま DataLoader に渡せます。\n",
        "    train_set = checkpoint['train_data']\n",
        "    test_set = checkpoint['test_data']\n",
        "\n",
        "    # DataLoader を構築 (バッチサイズはメモリに合わせて調整してください)\n",
        "    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
        "\n",
        "    print(f\"--- ロード完了 ---\")\n",
        "    print(f\"訓練セット: {len(train_set)} 枚\")\n",
        "    print(f\"テストセット: {len(test_set)} 枚\")\n",
        "\n",
        "    # 【最重要チェック】ノード数と次元数の確認\n",
        "    sample_train = train_set[0]\n",
        "    sample_test = test_set[0]\n",
        "\n",
        "    # 期待値: [23, 7] (22人 + ボール1つ、7種類の特徴量)\n",
        "    print(f\"訓練データの形状: {sample_train.x.shape}\")\n",
        "    print(f\"テストデータの形状: {sample_test.x.shape}\")\n",
        "\n",
        "    # 1. 次元数チェック\n",
        "    if sample_train.x.shape[1] == 7:\n",
        "        print(\"次元数: OK (7次元)\")\n",
        "    else:\n",
        "        print(f\"次元数警告: {sample_train.x.shape[1]}次元になっています。\")\n",
        "\n",
        "    # 2. ノード数チェック\n",
        "    if sample_train.x.shape[0] == 23:\n",
        "        print(\"ノード数: OK (23ノード固定)\")\n",
        "    else:\n",
        "        print(f\"ノード数警告: {sample_train.x.shape[0]}ノードになっています。\")\n",
        "\n",
        "    # 3. 速度データの存在チェック\n",
        "    # 2列目(vx)の絶対値平均が0でなければ、速度が正しく入っています\n",
        "    v_mean = torch.abs(sample_train.x[:, 2]).mean().item()\n",
        "    if v_mean > 0.01:\n",
        "        print(f\"物理量チェック: OK (平均速度属性を確認)\")\n",
        "    else:\n",
        "        print(\"物理量警告: 速度が0に張り付いています。Builderを再確認してください。\")\n",
        "\n",
        "    if sample_train.x.shape[1] == 7 and v_mean > 0.01:\n",
        "        print(\"\\nすべての準備が整いました。PIGNN 学習を開始してください。\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ファイルが見つかりません。パスを確認してください: {v7_load_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"エラー発生: {e}\")"
      ],
      "metadata": {
        "id": "XjQznJNPUTHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルの定義（PIGNNのオリジナルクラス）"
      ],
      "metadata": {
        "id": "ocDgeAXM6Ifi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_geometric.utils import softmax\n",
        "import os\n",
        "\n",
        "# ==========================================\n",
        "# 1. 前処理関数の定義\n",
        "# ==========================================\n",
        "def preprocess_batch(data, device):\n",
        "    # vx, vy の正規化（10で割る）\n",
        "    data.x[:, 2:4] = data.x[:, 2:4] / 10.0\n",
        "    # d_goal, d_ball の正規化（100で割る）\n",
        "    data.x[:, 4:6] = data.x[:, 4:6] / 100.0\n",
        "\n",
        "    data.x = data.x.float()\n",
        "    data.pos = data.pos.float()\n",
        "    data.vel = data.vel.float()\n",
        "    return data.to(device)\n",
        "\n",
        "# ==========================================\n",
        "# 2. モデル定義 (PIGNN v9: 勾配を維持する物理レイヤー)\n",
        "# ==========================================\n",
        "class PIGNNLayer(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, tau=1.5):\n",
        "        super(PIGNNLayer, self).__init__(aggr='add')\n",
        "        self.tau = tau\n",
        "        self.lin = nn.Linear(in_channels, out_channels)\n",
        "        self.att = nn.Parameter(torch.Tensor(1, out_channels * 2))\n",
        "        nn.init.xavier_uniform_(self.att)\n",
        "\n",
        "    def forward(self, x, edge_index, pos, vel):\n",
        "        h = self.lin(x)\n",
        "        return self.propagate(edge_index, x=h, pos=pos, vel=vel)\n",
        "\n",
        "    def message(self, x_i, x_j, pos_i, pos_j, vel_i, vel_j, edge_index_i):\n",
        "        pos_i_pred = pos_i + vel_i * self.tau\n",
        "        pos_j_pred = pos_j + vel_j * self.tau\n",
        "        dist_future = torch.norm(pos_i_pred - pos_j_pred, dim=-1, keepdim=True)\n",
        "\n",
        "        # 【修正】物理バイアスの緩和: ハードルを下げて、モデルが微調整できるようにする\n",
        "        # exp(-dist/0.5)*10.0 -> exp(-dist/2.0)*1.0\n",
        "        physics_bias = torch.exp(-dist_future / 2.0) * 1.0\n",
        "\n",
        "        alpha = torch.cat([x_i, x_j], dim=-1)\n",
        "        alpha = (alpha * self.att).sum(dim=-1, keepdim=True)\n",
        "        alpha = F.leaky_relu(alpha) + physics_bias\n",
        "        alpha = softmax(alpha, edge_index_i)\n",
        "\n",
        "        return alpha * x_j, physics_bias\n",
        "\n",
        "    def aggregate(self, inputs, index, ptr=None, dim_size=None):\n",
        "        out_x, out_bias = inputs\n",
        "        aggr_x = torch.zeros(dim_size, out_x.size(-1), device=out_x.device)\n",
        "        aggr_x.scatter_add_(0, index.unsqueeze(-1).expand_as(out_x), out_x)\n",
        "        return aggr_x, out_bias.mean()\n",
        "\n",
        "class PIGNNClassifier(nn.Module):\n",
        "    def __init__(self, hidden_channels=64):\n",
        "        super(PIGNNClassifier, self).__init__()\n",
        "        self.conv1 = PIGNNLayer(7, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        x, b1 = self.conv1(x, edge_index, pos, vel)\n",
        "        x = F.elu(x)\n",
        "        x, b2 = self.conv2(x, edge_index, pos, vel)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        # 二値分類のため log_softmax を使用\n",
        "        out = F.log_softmax(self.lin(x), dim=1)\n",
        "\n",
        "        total_bias = (b1 + b2) / 2.0\n",
        "        return out, total_bias\n",
        "\n",
        "# ==========================================\n",
        "# 2. 学習ループ (重み付き損失 & 物理重み制限)\n",
        "# ==========================================\n",
        "def train_pignn_epoch_dynamic(model, loader, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    total_loss, total_phys = 0, 0\n",
        "\n",
        "    # 【修正】Alphaの上限を10倍下げる (5.0 -> 0.5)\n",
        "    if epoch <= 10:\n",
        "        current_alpha = 0.01\n",
        "    else:\n",
        "        current_alpha = min(0.01 + (epoch - 10) * 0.02, 0.5)\n",
        "\n",
        "    # 【修正】クラス重みの導入: Success (1) を当てた時の報酬を3.3倍にする\n",
        "    # Failure: 244, Success: 74 なので 244/74 ≒ 3.3\n",
        "    weights = torch.tensor([1.0, 3.3], device=device)\n",
        "\n",
        "    for data in loader:\n",
        "        data = preprocess_batch(data, device)\n",
        "        optimizer.zero_grad()\n",
        "        out, bias_val = model(data)\n",
        "\n",
        "        # 【修正】weight引数を追加\n",
        "        c_loss = F.nll_loss(out, data.y.view(-1), weight=weights)\n",
        "\n",
        "        v_mag = torch.norm(data.vel, dim=-1)\n",
        "        m_loss = torch.mean(F.relu(v_mag - 12.0)**2)\n",
        "        p_loss = bias_val + m_loss\n",
        "\n",
        "        # 加算方式\n",
        "        loss = c_loss + current_alpha * p_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "        total_phys += p_loss.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(loader.dataset), total_phys / len(loader.dataset), current_alpha"
      ],
      "metadata": {
        "id": "hIyIVn-LHCPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 【完全統合版】これを実行すれば学習が始まります\n",
        "# ==========================================\n",
        "\n",
        "def train_pignn_epoch_dynamic(model, loader, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    total_loss, total_phys = 0, 0\n",
        "\n",
        "    # 加算アニーリング方式\n",
        "    if epoch <= 10:\n",
        "        current_alpha = 0.1\n",
        "    else:\n",
        "        current_alpha = min(0.1 + (epoch - 10) * 0.2, 5.0)\n",
        "\n",
        "    for data in loader:\n",
        "        data = preprocess_batch(data, device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out, bias_val = model(data)\n",
        "        c_loss = F.nll_loss(out, data.y.view(-1))\n",
        "\n",
        "        v_mag = torch.norm(data.vel, dim=-1)\n",
        "        motion_penalty = torch.mean(F.relu(v_mag - 12.0)**2)\n",
        "        p_loss = bias_val + motion_penalty\n",
        "\n",
        "        # 加算方式で安定化\n",
        "        loss = c_loss + current_alpha * p_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "        total_phys += p_loss.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(loader.dataset), total_phys / len(loader.dataset), current_alpha\n"
      ],
      "metadata": {
        "id": "9lN9gST6Lfxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. 実行セクション (保存・履歴管理・物理統合版)\n",
        "# ==========================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ハイパーパラメータ\n",
        "EPOCHS = 50\n",
        "LR = 0.0005\n",
        "# 物理損失の初期重み（学習ループ内で動的に変化します）\n",
        "ALPHA_P = 0.5\n",
        "\n",
        "history = {\n",
        "    'total_loss': [],\n",
        "    'physics_loss': [],\n",
        "    'test_acc': [],\n",
        "    'alpha': []\n",
        "}\n",
        "\n",
        "# 7次元入力を考慮したモデルと最適化手法\n",
        "model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# 指定されたファイル名\n",
        "save_path = 'best_pignn_physics_integrated_v1.pth'\n",
        "\n",
        "print(f\"PIGNN学習開始 (Device: {device} | 物理・分類 統合学習モード)\")\n",
        "print(f\"Input Features: 7 [x, y, vx, vy, (1-px), dist_ball, team_id]\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # 【重要】動的アニーリング版の学習関数を呼び出し\n",
        "    avg_loss, avg_phys, current_alpha = train_pignn_epoch_dynamic(model, train_loader, optimizer, device, epoch)\n",
        "\n",
        "    # テスト評価\n",
        "    acc = test_pignn(model, test_loader, device)\n",
        "\n",
        "    # 履歴の保存\n",
        "    history['total_loss'].append(avg_loss)\n",
        "    history['physics_loss'].append(avg_phys)\n",
        "    history['test_acc'].append(acc)\n",
        "    history['alpha'].append(current_alpha)\n",
        "\n",
        "    # 進捗表示\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | Alpha: {current_alpha:.2f} | Loss: {avg_loss:.4f} | Phys_L: {avg_phys:.4f} | Acc: {acc:.4f}\")\n",
        "\n",
        "    # モデルの保存ロジック (提示されたロジックを参考)\n",
        "    # 初回、またはこれまでの最高精度を更新した場合に保存\n",
        "    if epoch == 1 or acc > max(history['test_acc'][:-1]):\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\" >> Model Saved: {save_path} (Best Acc: {acc:.4f})\")\n",
        "\n",
        "print(f\"\\n学習が正常に完了しました。最高精度: {max(history['test_acc']):.4f}\")"
      ],
      "metadata": {
        "id": "XvcuJczK-BWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. 最終評価とレポート (PIGNN 物理損失統合版)\n",
        "# ==========================================\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. 保存した物理統合版のベストモデルをロード\n",
        "# 学習時に保存した名前に合わせてください\n",
        "model_path = 'best_pignn_physics_integrated_v1.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# 2. テストデータで最終予測\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        data = preprocess_batch(data, device)\n",
        "        # 【修正】戻り値が(予測, バイアス)なので2つで受ける\n",
        "        out, _ = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# 3. レポート表示\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"       PIGNN 最終評価結果 (物理損失+チーム属性統合版)\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(all_labels, all_preds, target_names=['Failure (0)', 'Success (1)']))\n",
        "\n",
        "# 4. 混同行列の描画\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title('Confusion Matrix (Physics-Integrated PIGNN)')\n",
        "plt.ylabel('Actual (True)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YFyM9YpTHYO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以上がPIGNNのベースラインモデル"
      ],
      "metadata": {
        "id": "sVwMYowN6Qvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PFI"
      ],
      "metadata": {
        "id": "IzPgBIdiveAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# 1. 保存した最新の「物理損失統合版」モデルパスを指定\n",
        "# 学習時に保存した名前に正確に書き換えてください\n",
        "save_path = \"best_pignn_physics_integrated_v1.pth\"\n",
        "\n",
        "# 2. モデルのインスタンス化\n",
        "model = PIGNNClassifier(hidden_channels=64)\n",
        "\n",
        "# 3. 重みのロード\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if os.path.exists(save_path):\n",
        "    # ロード（最新のPyTorchでは weights_only=True が推奨）\n",
        "    checkpoint = torch.load(save_path, map_location=device, weights_only=True)\n",
        "    model.load_state_dict(checkpoint)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"物理制約統合済み PIGNNモデル のロードに成功しました！\")\n",
        "    print(f\"ロード元: {save_path}\")\n",
        "\n",
        "    # 【テスト実行】戻り値の形式が変わっているか確認\n",
        "    sample_data = next(iter(test_loader)).to(device)\n",
        "    try:\n",
        "        sample_data = preprocess_batch(sample_data, device)\n",
        "        test_out = model(sample_data)\n",
        "        if isinstance(test_out, tuple):\n",
        "             print(f\"物理バイアス出力確認: 正常 (戻り値は {len(test_out)} 要素)\")\n",
        "        else:\n",
        "             print(f\"警告: 戻り値が1つです。物理損失統合前のモデルかもしれません。\")\n",
        "    except Exception as e:\n",
        "        print(f\"動作確認中にエラー: {e}\")\n",
        "else:\n",
        "    print(f\"エラー: {save_path} が見つかりません。\")"
      ],
      "metadata": {
        "id": "Qm0xYfDpuD6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最初のバッチを流してエラーが出ないかテスト\n",
        "sample_data = next(iter(test_loader))\n",
        "\n",
        "# 1. まずモデルをデバイスに送る（再確認）\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 2. データをデバイスに送る (preprocess_batch 内で .to(device) されているか確認)\n",
        "sample_data = preprocess_batch(sample_data, device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # 3. 推論を実行\n",
        "    out = model(sample_data)\n",
        "    print(\"推論テスト成功！ 出力形状:\", out.shape) # [16, 2]"
      ],
      "metadata": {
        "id": "kpu0mYUXBN7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_feature_importance_pignn(model, loader, device):\n",
        "    model.eval()\n",
        "    data_list = list(loader)\n",
        "\n",
        "    # 特徴量リスト (6次元)\n",
        "    node_features = ['pos_x', 'pos_y', 'vel_x', 'vel_y', 'dist_goal', 'dist_ball']\n",
        "    all_names = node_features\n",
        "    importances = []\n",
        "\n",
        "    # ベースライン精度計測\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in data_list:\n",
        "            data = preprocess_batch(data.clone())\n",
        "            out = model(data)\n",
        "            all_preds.extend(out.argmax(dim=1).cpu().numpy())\n",
        "            all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "    baseline_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "    for i in range(len(all_names)):\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for data in data_list:\n",
        "                batch_data = data.clone()\n",
        "                batch_data = preprocess_batch(batch_data)\n",
        "                # 該当特徴量を破壊\n",
        "                batch_data.x[:, i] = 0\n",
        "                out = model(batch_data)\n",
        "                preds.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "        acc = (np.array(preds) == np.array(all_labels)).mean()\n",
        "        importances.append(max(0, baseline_acc - acc))\n",
        "\n",
        "    return all_names, importances"
      ],
      "metadata": {
        "id": "dCdnhFeduP92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_feature_importance_pignn(model, loader, device):\n",
        "    model.eval()\n",
        "    data_list = list(loader)\n",
        "\n",
        "    # 【重要修正】 特徴量リストを7次元に更新\n",
        "    node_features = [\n",
        "        'pos_x', 'pos_y',\n",
        "        'vel_x', 'vel_y',\n",
        "        'dist_goal', 'dist_ball',\n",
        "        'team_id' # 7番目の新メンバー\n",
        "    ]\n",
        "    all_names = node_features\n",
        "    importances = []\n",
        "\n",
        "    # 1. ベースライン精度の計算\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in data_list:\n",
        "            batch_data = preprocess_batch(data.clone(), device)\n",
        "            out = model(batch_data)\n",
        "            all_preds.extend(out.argmax(dim=1).cpu().numpy())\n",
        "            all_labels.extend(batch_data.y.view(-1).cpu().numpy())\n",
        "\n",
        "    baseline_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "    print(f\"ベースライン精度 (Team-Aware v7): {baseline_acc:.4f}\")\n",
        "\n",
        "    # 2. 各特徴量を順番に破壊\n",
        "    for i in range(len(all_names)):\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for data in data_list:\n",
        "                batch_data = data.clone()\n",
        "                batch_data = preprocess_batch(batch_data, device)\n",
        "\n",
        "                # i番目の特徴量を破壊\n",
        "                batch_data.x[:, i] = 0\n",
        "\n",
        "                out = model(batch_data)\n",
        "                preds.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "        acc = (np.array(preds) == np.array(all_labels)).mean()\n",
        "        drop = baseline_acc - acc\n",
        "        importances.append(max(0, drop))\n",
        "        print(f\"特徴量 '{all_names[i]}' 破壊時の精度低下: {drop:.4f}\")\n",
        "\n",
        "    return all_names, importances\n",
        "\n",
        "# --- 実行 ---\n",
        "feature_names, importance_values = calculate_feature_importance_pignn(model, test_loader, device)"
      ],
      "metadata": {
        "id": "fSe8LVVaasiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "アテンション係数の可視化"
      ],
      "metadata": {
        "id": "nNhUKPcP6W4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#アテンション係数の可視化\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def visualize_pignn_final_v3(model, loader, sample_idx=0):\n",
        "    model.eval()\n",
        "    data = next(iter(loader)).to(device)\n",
        "\n",
        "    # 推論\n",
        "    input_data = data.clone()\n",
        "    input_data = preprocess_batch(input_data, device)\n",
        "    with torch.no_grad():\n",
        "        out = model(input_data)\n",
        "        probs = torch.exp(out)\n",
        "        preds = out.argmax(dim=1)\n",
        "\n",
        "    mask = (data.batch == sample_idx)\n",
        "    pos = data.pos[mask].cpu().numpy()\n",
        "    vel = data.vel[mask].cpu().numpy()\n",
        "\n",
        "    # ノード数を確認（36個想定）\n",
        "    num_nodes = pos.shape[0]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    ax.set_facecolor('#f0f0f0')\n",
        "\n",
        "    # ピッチ描画\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=False, color='black', lw=2))\n",
        "    ax.plot([0, 0], [-34, 34], color='black', alpha=0.3)\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        # --- 判定ロジックを「インデックス」に変更 ---\n",
        "        if i == num_nodes - 1: # 最後のノードがボール\n",
        "            color, marker, size, z = 'gold', '*', 450, 15\n",
        "        elif i < 11: # 最初の11人が味方（仮定）\n",
        "            color, marker, size, z = 'blue', 'o', 180, 10\n",
        "        elif i < 22: # 次の11人が敵（仮定）\n",
        "            color, marker, size, z = 'red', 'o', 180, 10\n",
        "        else: # それ以外のダミー等\n",
        "            color, marker, size, z = 'gray', 'o', 100, 5\n",
        "\n",
        "        # 描画\n",
        "        ax.scatter(pos[i, 0], pos[i, 1], c=color, marker=marker, s=size, edgecolors='black', zorder=z)\n",
        "        # ベクトル\n",
        "        ax.arrow(pos[i, 0], pos[i, 1], vel[i, 0]*1.5, vel[i, 1]*1.5,\n",
        "                 head_width=1.0, head_length=1.2, fc=color, ec=color, alpha=0.4, zorder=z-1)\n",
        "\n",
        "    plt.title(f\"PIGNN Fixed Visualization\\nPred: {preds[sample_idx]} | Prob: {probs[sample_idx, 1]:.2%}\")\n",
        "    plt.show()\n",
        "\n",
        "# 実行\n",
        "visualize_pignn_final_v3(model, test_loader, sample_idx=0)"
      ],
      "metadata": {
        "id": "t1Pw-F11bPc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習曲線"
      ],
      "metadata": {
        "id": "Y7mObh2d6Z73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 学習時のループ内で train_acc も記録するように修正して実行したと仮定\n",
        "# もし記録していなければ、このコードで現在の history からグラフを出します\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history['total_loss'], label='Train Loss (Total)')\n",
        "# もし train_acc を取っていればここに追加\n",
        "plt.plot(history['test_acc'], label='Test Accuracy', marker='o')\n",
        "\n",
        "plt.title('Check for Overfitting: Loss vs Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2lj45c8TDvdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "検証"
      ],
      "metadata": {
        "id": "Fq9_erp86ynd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ラベルをシャッフルした時の精度"
      ],
      "metadata": {
        "id": "SshWo4xj62Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_label_test(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = preprocess_batch(data, device)\n",
        "            # ラベルをランダムにシャッフルする\n",
        "            random_y = data.y[torch.randperm(data.y.size(0))]\n",
        "\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == random_y.view(-1)).sum().item()\n",
        "            total += data.num_graphs\n",
        "\n",
        "    print(f\"ラベルシャッフル時の精度: {correct / total:.4f}\")\n",
        "    print(\">> 0.5 (50%) 前後になれば、モデルは正しくラベルと特徴の関係を学んでいます。\")\n",
        "\n",
        "shuffle_label_test(model, test_loader, device)"
      ],
      "metadata": {
        "id": "xuFcHaVED2B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "物理損失（損失関数に付け加えた物理項）"
      ],
      "metadata": {
        "id": "Hnh1Z9O767qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 物理損失の平均値を算出\n",
        "avg_phys = sum(history['physics_loss']) / len(history['physics_loss'])\n",
        "print(f\"平均物理損失: {avg_phys:.4f}\")\n",
        "\n",
        "if avg_phys < 25: # 18前後なら非常に優秀\n",
        "    print(\">> 物理的整合性は保たれています。AIは現実的な動きの範囲内で予測しています。\")\n",
        "else:\n",
        "    print(\">> 物理損失が高いです。AIが異常な速度を想定して予測している可能性があります。\")"
      ],
      "metadata": {
        "id": "PP6lAzBVD6GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "過学習を防ぐためにドロップアウト層を追加した修正版PIGNNモデル"
      ],
      "metadata": {
        "id": "8CW6TCnB7Bff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "class PIGNNClassifier_drop(nn.Module):\n",
        "    def __init__(self, hidden_channels=64, dropout_rate=0.3):\n",
        "        super(PIGNNClassifier_drop, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # 修正: 7次元入力 [x, y, vx, vy, d_goal, d_ball, team_id]\n",
        "        self.conv1 = PIGNNLayer(7, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        # 1層目 + Dropout\n",
        "        x = self.conv1(x, edge_index, pos, vel)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        # 2層目 + Dropout\n",
        "        x = self.conv2(x, edge_index, pos, vel)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # 最終出力\n",
        "        return F.log_softmax(self.lin(x), dim=1)\n",
        "\n",
        "class PIGNNClassifier_v7_Final(nn.Module):\n",
        "    def __init__(self, hidden_channels=64, dropout_rate=0.3):\n",
        "        super(PIGNNClassifier_v7_Final, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # 修正: 7次元 [x, y, vx, vy, d_goal, d_ball, team_id]\n",
        "        self.conv1 = PIGNNLayer(7, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        # 1層目 + Dropout\n",
        "        x = F.elu(self.conv1(x, edge_index, pos, vel))\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        # 2層目 + Dropout\n",
        "        x = F.elu(self.conv2(x, edge_index, pos, vel))\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return F.log_softmax(self.lin(x), dim=1)"
      ],
      "metadata": {
        "id": "BCUmvO4FYues"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 新しいモデルをインスタンス化\n",
        "model_dropout = PIGNNClassifier_drop(hidden_channels=64, dropout_rate=0.3).to(device)\n",
        "optimizer_dropout = torch.optim.Adam(model_dropout.parameters(), lr=LR)\n",
        "\n",
        "# 2. 履歴保存用（名前を分ける：history_dropout）\n",
        "history_dropout = {\n",
        "    'total_loss': [],\n",
        "    'physics_loss': [],\n",
        "    'test_acc': []\n",
        "}\n",
        "\n",
        "print(f\"PIGNN学習開始（ドロップアウト版）\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # model_dropout を使う\n",
        "    avg_loss, avg_phys = train_pignn_epoch(model_dropout, train_loader, optimizer_dropout, ALPHA_P, device)\n",
        "    acc = test_pignn(model_dropout, test_loader, device)\n",
        "\n",
        "    history_dropout['total_loss'].append(avg_loss)\n",
        "    history_dropout['physics_loss'].append(avg_phys)\n",
        "    history_dropout['test_acc'].append(acc)\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | Loss: {avg_loss:.4f} | Phys_L: {avg_phys:.4f} | Acc: {acc:.4f}\")\n",
        "\n",
        "    # 保存ファイル名を変える（重要！）\n",
        "    if epoch == 1 or acc > max(history_dropout['test_acc'][:-1]):\n",
        "        torch.save(model_dropout.state_dict(), 'best_pignn_model_v6_dropout.pth')\n",
        "        print(f\" >> Model Saved (Best Dropout Acc: {acc:.4f})\")"
      ],
      "metadata": {
        "id": "cjks8bFpY2Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "精度曲線"
      ],
      "metadata": {
        "id": "2Xe4VMee7JJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ドロップアウト層なしとありを重ねて描画\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history['test_acc'], label='Base Model (No Dropout)', alpha=0.6)\n",
        "plt.plot(history_dropout['test_acc'], label='Improved Model (With Dropout)', linewidth=2)\n",
        "plt.title('Effect of Dropout on Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MQvfkBq5ZbCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ベースラインより乱高下の幅が小さくなっている→改善ポイント"
      ],
      "metadata": {
        "id": "OT_gMd-h7QKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# ==========================================\n",
        "# 5. 最終評価とレポート (Dropout版・専用)\n",
        "# ==========================================\n",
        "\n",
        "# 1. ドロップアウト版のモデルをロード\n",
        "# 学習時に保存した「dropout」という名前の方を読み込みます\n",
        "model_drop_eval = PIGNNClassifier_drop(hidden_channels=64, dropout_rate=0.3).to(device)\n",
        "model_drop_eval.load_state_dict(torch.load('best_pignn_model_v6_dropout.pth'))\n",
        "model_drop_eval.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# 2. テストデータで最終予測\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        data = preprocess_batch(data, device)\n",
        "        out = model_drop_eval(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# 3. レポート表示\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"       PIGNN 最終評価結果 (Dropout改良版)\")\n",
        "print(\"=\"*50)\n",
        "print(classification_report(all_labels, all_preds, target_names=['Failure (0)', 'Success (1)']))\n",
        "\n",
        "# 4. 混同行列の描画\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', # 区別するために色をオレンジ系に\n",
        "            xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title('Confusion Matrix (PIGNN with Dropout)')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EvaPTIG8b-m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "精度は下がったが、モデルは改善していると言える。"
      ],
      "metadata": {
        "id": "D1k1IKXCcmXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "検証"
      ],
      "metadata": {
        "id": "7biH1yrO7eMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_label_test_dropout_ver(model_to_test, loader, device):\n",
        "    model_to_test.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # 判定用の閾値（不均衡データの場合、多数派の割合に引っ張られるため）\n",
        "    # 今回のテストデータ Failure:239, Success:90 なので、ランダムなら\n",
        "    # (239/329)^2 + (90/329)^2 ≒ 0.6 くらいになるのが統計学的な「勘」の限界です。\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = preprocess_batch(data, device)\n",
        "\n",
        "            # ラベルをランダムにシャッフル\n",
        "            random_y = data.y[torch.randperm(data.y.size(0))]\n",
        "\n",
        "            out = model_to_test(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            correct += (pred == random_y.view(-1)).sum().item()\n",
        "            total += data.num_graphs\n",
        "\n",
        "    shuffle_acc = correct / total\n",
        "    print(f\"ラベルシャッフル時の精度: {shuffle_acc:.4f}\")\n",
        "\n",
        "    if shuffle_acc < 0.58: # 0.62から下がっていれば改善\n",
        "        print(\">> 合格：暗記（過学習）が抑制され、特徴量とラベルの真の相関を学んでいます。\")\n",
        "    else:\n",
        "        print(\">> 警告：依然としてデータの偏り（初期配置など）を強く覚えすぎている可能性があります。\")\n",
        "\n",
        "# 実行（ドロップアウト版のモデルを指定）\n",
        "shuffle_label_test_dropout_ver(model_drop_eval, test_loader, device)"
      ],
      "metadata": {
        "id": "hm1YcFwvc21q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "なぜ「0.59」で正解なのか？\n",
        "今回のデータセット（不均衡データ）において、**0.59 という数値は実質的な「0 (ゼロ) 点」**を意味します。\n",
        "\n",
        "データの比率: 失敗（Fail）が約 73%、成功（Success）が約 27% です。\n",
        "\n",
        "偶然の期待値: 統計学的に、この比率のデータで適当にラベルを振って当てずっぽうで予測すると、約 0.6 (60%) 前後の精度になるのが正常な挙動です。\n",
        "\n",
        "判定: つまり、0.5927 というのは「デタラメな答えを与えられたら、AIは何も当てる術を持っていない」ことを示しています。もしこれが 0.8 とかであれば「答えを見なくてもパターンで当てている（暗記）」ことになりますが、0.59 なら**「暗記していない」**と言い切れます。"
      ],
      "metadata": {
        "id": "yUtN7OxNeKfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "物理損失"
      ],
      "metadata": {
        "id": "DW3LormL7h9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 物理的整合性の最終確認 (Dropout版)\n",
        "# ==========================================\n",
        "\n",
        "# history_dropout の中身を使って計算します\n",
        "if 'physics_loss' in history_dropout and len(history_dropout['physics_loss']) > 0:\n",
        "    avg_phys = sum(history_dropout['physics_loss']) / len(history_dropout['physics_loss'])\n",
        "    print(f\"ドロップアウト版 平均物理損失: {avg_phys:.4f}\")\n",
        "\n",
        "    # 判定\n",
        "    if avg_phys < 25:\n",
        "        print(\">> 合格：物理的整合性は保たれています。\")\n",
        "        print(\">> ドロップアウトを導入しても、AIは現実的な物理法則（速度ベクトル）を無視していません。\")\n",
        "    else:\n",
        "        print(\">> 警告：物理損失が増大しています。\")\n",
        "        print(\">> 汎化性能を優先するあまり、物理レイヤーの制約が弱まっている可能性があります。\")\n",
        "else:\n",
        "    print(\">> エラー：history_dropout に physics_loss が記録されていません。\")"
      ],
      "metadata": {
        "id": "LGzGGSqSdA8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.explain import Explainer, GNNExplainer\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# --- 修正の核心：重要度を抽出する1行 ---\n",
        "# node_mask は (ノード数, 特徴量数) の形状なので、全ノードで平均して特徴量ごとの重要度にする\n",
        "importances = explanation.node_mask.mean(dim=0)\n",
        "\n",
        "# --- 以降、あなたの可視化コードへ続く ---\n",
        "if torch.is_tensor(importances):\n",
        "    importances = importances.cpu().numpy()\n",
        "\n",
        "# 1. バラバラの引数を「dataオブジェクト」に梱包してモデルに渡すラッパー\n",
        "class ExplainerWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None, **kwargs):\n",
        "        # GNNExplainerから届く各テンソルを、モデルが期待する Data オブジェクトに擬似再現\n",
        "        # pos と vel は x の中に入っている、あるいは別途渡されることを想定\n",
        "        # あなたのモデル定義に合わせて x から pos, vel を切り出す、\n",
        "        # または data.pos, data.vel としてアクセスできるようにします。\n",
        "\n",
        "        # 仮に x の 0-1列目が pos, 2-3列目が vel だと想定される場合：\n",
        "        # (モデルの入力に合わせて調整してください。x自体に全て含まれているならそのままでも可)\n",
        "        tmp_data = Data(x=x, edge_index=edge_index, batch=batch)\n",
        "\n",
        "        # もしモデルが data.pos や data.vel を直接参照しているなら、ここで代入\n",
        "        # x の構成が [特徴量...] で、pos/velが別管理なら以下のように復元\n",
        "        tmp_data.pos = x[:, :2]  # 例: 最初の2列が座標\n",
        "        tmp_data.vel = x[:, 2:4] # 例: 次の2列が速度\n",
        "\n",
        "        return self.model(tmp_data)\n",
        "\n",
        "# ラップしたモデルを作成\n",
        "wrapped_model = ExplainerWrapper(model_drop_eval)\n",
        "\n",
        "# 2. Explainerの設定\n",
        "model_config = {\n",
        "    'mode': 'multiclass_classification',\n",
        "    'task_level': 'graph',\n",
        "    'return_type': 'log_probs',\n",
        "}\n",
        "\n",
        "explainer = Explainer(\n",
        "    model=wrapped_model,\n",
        "    algorithm=GNNExplainer(epochs=200),\n",
        "    explanation_type='model',\n",
        "    node_mask_type='attributes',\n",
        "    edge_mask_type='object',\n",
        "    model_config=model_config,\n",
        ")\n",
        "\n",
        "# --- データの準備 ---\n",
        "test_batch = next(iter(test_loader))\n",
        "test_batch = preprocess_batch(test_batch, device)\n",
        "data_list = test_batch.to_data_list()\n",
        "data_single = data_list[0]\n",
        "\n",
        "# 3. 重要度の算出\n",
        "explanation = explainer(\n",
        "    x=data_single.x,\n",
        "    edge_index=data_single.edge_index,\n",
        "    batch=torch.zeros(data_single.x.size(0), dtype=torch.long).to(device)\n",
        ")\n",
        "\n",
        "print(\"重要度の算出に成功しました！\")\n",
        "\n",
        "# --- 修正版：重要度の可視化コード ---\n",
        "\n",
        "# ラベルを7次元用に更新\n",
        "labels = ['x', 'y', 'vx', 'vy', 'dist_goal', 'dist_ball', 'team_id']\n",
        "\n",
        "# importances が numpy 形式であることを確認\n",
        "if torch.is_tensor(importances):\n",
        "    importances = importances.cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "# ここで次元数を自動で合わせます\n",
        "plt.bar(labels, importances, color='teal')\n",
        "\n",
        "plt.title('GNNExplainer: Feature Importance (PIGNN v7)')\n",
        "plt.ylabel('Importance Score')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# 値を棒の上に表示\n",
        "for i, v in enumerate(importances):\n",
        "    plt.text(i, v + 0.01, f\"{v:.3f}\", ha='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9i9AUGXx97sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. 解析の設定\n",
        "num_samples = 100\n",
        "all_node_importances = []\n",
        "\n",
        "print(f\"{num_samples}シーンの解析を開始します。勾配計算を有効にして最適化を行うため、少し時間がかかります...\")\n",
        "\n",
        "# モデルを評価モードにしつつ、GNNExplainer内部の学習は許可する\n",
        "model_drop_eval.eval()\n",
        "\n",
        "# 進捗管理用のカウンタ\n",
        "count = 0\n",
        "\n",
        "for data in tqdm(test_loader):\n",
        "    if count >= num_samples:\n",
        "        break\n",
        "\n",
        "    data = preprocess_batch(data, device)\n",
        "    data_list = data.to_data_list()\n",
        "\n",
        "    for data_single in data_list:\n",
        "        if count >= num_samples:\n",
        "            break\n",
        "\n",
        "        # --- 修正ポイント：with torch.no_grad() を削除 ---\n",
        "        # GNNExplainerは内部でロスを計算し .backward() を呼ぶため勾配が必要\n",
        "        explanation = explainer(\n",
        "            x=data_single.x,\n",
        "            edge_index=data_single.edge_index,\n",
        "            batch=torch.zeros(data_single.x.size(0), dtype=torch.long).to(device)\n",
        "        )\n",
        "\n",
        "        node_importance = explanation.node_mask.abs().mean(dim=0).cpu().numpy()\n",
        "        all_node_importances.append(node_importance)\n",
        "        count += 1\n",
        "\n",
        "# 3. 平均と標準誤差の計算\n",
        "avg_importance = np.mean(all_node_importances, axis=0)\n",
        "std_importance = np.std(all_node_importances, axis=0) / np.sqrt(len(all_node_importances))\n",
        "\n",
        "# --- 修正版：ラベルとグラフ描画 ---\n",
        "\n",
        "# 1. ラベルを現在の7次元仕様に完全に合わせる\n",
        "# [x, y, vx, vy, dist_goal, dist_ball, team_id]\n",
        "labels = ['PosX', 'PosY', 'VelX', 'VelY', 'DistGoal', 'DistBall', 'TeamID']\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# 2. データの数とラベルの数が一致しているか確認して描画\n",
        "# avg_importance と std_importance が 7要素であることを前提とします\n",
        "bars = plt.bar(labels, avg_importance, yerr=std_importance,\n",
        "               color='teal', edgecolor='navy', capsize=5, alpha=0.8)\n",
        "\n",
        "plt.title('GNNExplainer: Mean Feature Importance over 100 Scenes (v7)', fontsize=14)\n",
        "plt.ylabel('Importance Score', fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# 棒の上に数値を表示\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fztqYk2oBvLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 成功と失敗の比較解析 (PIGNN v7対応版)\n",
        "# ==========================================\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. データの仕分け用リスト\n",
        "success_importances = []\n",
        "failure_importances = []\n",
        "\n",
        "num_samples = 150  # 統計的安定性のために150シーンを推奨\n",
        "count = 0\n",
        "\n",
        "print(f\"{num_samples}シーンを成功/失敗別に解析します...\")\n",
        "\n",
        "# モデルを評価モードに\n",
        "model_drop_eval.eval()\n",
        "\n",
        "# tqdmで進捗を表示しながらループ\n",
        "for data in tqdm(test_loader):\n",
        "    if count >= num_samples:\n",
        "        break\n",
        "\n",
        "    # バッチをデバイスに送り、個別データに分解\n",
        "    data = preprocess_batch(data, device)\n",
        "    data_list = data.to_data_list()\n",
        "\n",
        "    for data_single in data_list:\n",
        "        if count >= num_samples:\n",
        "            break\n",
        "\n",
        "        # GNNExplainerで重要度算出\n",
        "        explanation = explainer(\n",
        "            x=data_single.x,\n",
        "            edge_index=data_single.edge_index,\n",
        "            # batchテンソルもデバイスに合わせる\n",
        "            batch=torch.zeros(data_single.x.size(0), dtype=torch.long).to(device)\n",
        "        )\n",
        "\n",
        "        # ノード特徴量の重要度（絶対値の平均）を取得\n",
        "        node_importance = explanation.node_mask.abs().mean(dim=0).cpu().numpy()\n",
        "\n",
        "        # 正解ラベル(y)に基づいて仕分け\n",
        "        if data_single.y.item() == 1:\n",
        "            success_importances.append(node_importance)\n",
        "        else:\n",
        "            failure_importances.append(node_importance)\n",
        "\n",
        "        count += 1\n",
        "\n",
        "# 2. 【重要】ラベルを7次元（v7仕様）に更新\n",
        "labels = ['PosX', 'PosY', 'VelX', 'VelY', 'DistGoal', 'DistBall', 'TeamID']\n",
        "\n",
        "# 各グループの平均を算出\n",
        "# ここで avg_success の shape は (7,) になります\n",
        "avg_success = np.mean(success_importances, axis=0)\n",
        "avg_failure = np.mean(failure_importances, axis=0)\n",
        "\n",
        "# 3. 比較グラフの描画\n",
        "x = np.arange(len(labels)) # 0から6までのインデックス\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# 棒グラフの描画\n",
        "rects1 = ax.bar(x - width/2, avg_success, width, label='Success (1)', color='forestgreen', alpha=0.8)\n",
        "rects2 = ax.bar(x + width/2, avg_failure, width, label='Failure (0)', color='crimson', alpha=0.8)\n",
        "\n",
        "# グラフの装飾\n",
        "ax.set_ylabel('Mean Importance Score', fontsize=12)\n",
        "ax.set_title('Feature Importance Comparison: Success vs Failure (PIGNN v7)', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels, fontsize=11)\n",
        "ax.legend(fontsize=12)\n",
        "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
        "\n",
        "# 数値ラベルを表示する補助関数\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.3f}',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3), # 3pt上に表示\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. 数値の要約表示\n",
        "print(\"\\n--- 解析結果の要約 ---\")\n",
        "for i, label in enumerate(labels):\n",
        "    diff = avg_success[i] - avg_failure[i]\n",
        "    trend = \"↑ Successで重視\" if diff > 0 else \"↓ Failureで重視\"\n",
        "    print(f\"[{label}] Success: {avg_success[i]:.4f} | Failure: {avg_failure[i]:.4f} | {trend}\")"
      ],
      "metadata": {
        "id": "Jqps80gtE7Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "バイアスの可視化"
      ],
      "metadata": {
        "id": "sYGevTJQeSGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 描画の前にこれを入れてください\n",
        "sample = next(iter(test_loader))\n",
        "# 最初の1バッチ分のチームIDの中身をすべて表示\n",
        "print(\"--- TeamID Raw Data Check ---\")\n",
        "print(sample.x[:, 6])\n",
        "print(\"-----------------------------\")\n",
        "\n",
        "# もしここで 0.0 しか出てこないなら、データの作り直しが必要"
      ],
      "metadata": {
        "id": "acgKVh70hQLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_physics_bias(model, data, tau=1.5):\n",
        "    \"\"\"\n",
        "    モデル内の物理バイアスを解析する関数（名前を修正）\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pos = data.pos\n",
        "        vel = data.vel\n",
        "        edge_index = data.edge_index\n",
        "\n",
        "        # 未来位置の予測\n",
        "        pos_pred = pos + vel * tau\n",
        "\n",
        "        # エッジごとの未来距離と物理バイアス\n",
        "        row, col = edge_index\n",
        "        dist_future = torch.norm(pos_pred[row] - pos_pred[col], dim=-1)\n",
        "        physics_bias = torch.exp(-dist_future / 1.0)#5.0から1.0に修正\n",
        "\n",
        "        # 最大バイアスの取得\n",
        "        top_idx = torch.argmax(physics_bias)\n",
        "        top_pair = (edge_index[0, top_idx].item(), edge_index[1, top_idx].item())\n",
        "\n",
        "    # 戻り値のキーを 'max_bias' に修正してエラーを解消\n",
        "    return {\n",
        "        \"top_pair\": top_pair,\n",
        "        \"max_bias\": physics_bias[top_idx].item(), # ここを bias_value から max_bias へ修正\n",
        "        \"pos_pred\": pos_pred\n",
        "    }"
      ],
      "metadata": {
        "id": "b6RnU4u7DYFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_pignn_v7_absolute_colors(model, loader, sample_idx=0, tau=1.5):\n",
        "    model.eval()\n",
        "    # データを1つ取得\n",
        "    batch = next(iter(loader)).to(device)\n",
        "    input_data = preprocess_batch(batch.clone(), device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(input_data)\n",
        "        probs = torch.exp(out)\n",
        "        preds = out.argmax(dim=1)\n",
        "\n",
        "    # 描画対象のインデックスを抽出\n",
        "    mask = (batch.batch == sample_idx)\n",
        "    pos = batch.pos[mask].cpu().numpy()\n",
        "    vel = batch.vel[mask].cpu().numpy()\n",
        "\n",
        "    # 【最重要】team_id (index 6) を直接取得して中身を確認\n",
        "    team_ids = input_data.x[mask, 6].cpu().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    # ピッチ背景\n",
        "    ax.set_facecolor('#f0f0f0')\n",
        "    ax.add_patch(plt.Rectangle((-52.5, -34), 105, 68, fill=False, color='black', lw=2))\n",
        "    ax.plot([0, 0], [-34, 34], color='black', alpha=0.3)\n",
        "\n",
        "    for i in range(len(pos)):\n",
        "        t_val = team_ids[i]\n",
        "\n",
        "        # --- 判定ロジックを「範囲」にして誤差を許容 ---\n",
        "        if t_val > 1.5:          # Ball (2.0)\n",
        "            color, marker, size, z = '#FFD700', '*', 500, 15 # Gold\n",
        "            lbl = \"Ball\"\n",
        "        elif t_val > 0.5:        # Defender (1.0)\n",
        "            color, marker, size, z = '#EE3333', 'o', 250, 10 # Red\n",
        "            lbl = \"Defender (Away)\"\n",
        "        else:                    # Attacker (0.0)\n",
        "            color, marker, size, z = '#3366FF', 'o', 250, 10 # Blue\n",
        "            lbl = \"Attacker (Home)\"\n",
        "\n",
        "        # 描画\n",
        "        ax.scatter(pos[i, 0], pos[i, 1], c=color, marker=marker, s=size,\n",
        "                   edgecolors='black', linewidths=1.2, zorder=z, label=lbl)\n",
        "\n",
        "        # 速度ベクトル\n",
        "        ax.arrow(pos[i, 0], pos[i, 1], vel[i, 0]*tau, vel[i, 1]*tau,\n",
        "                 head_width=0.8, head_length=1.0, fc=color, ec=color,\n",
        "                 alpha=0.3, zorder=z-1)\n",
        "\n",
        "    # 物理バイアスの描画（緑のX印と点線）\n",
        "    res = analyze_physics_bias(model, batch.to_data_list()[sample_idx], tau=tau)\n",
        "    p1, p2 = res[\"top_pair\"] # タプルなのでそのまま受け取るだけでOK\n",
        "    p1, p2 = int(p1), int(p2) # 念のため整数型に変換\n",
        "    ax.plot([pos[p1,0], pos[p2,0]], [pos[p1,1], pos[p2,1]], 'green', linestyle='--', lw=2, alpha=0.6)\n",
        "    ax.scatter(res[\"pos_pred\"][p1,0].cpu(), res[\"pos_pred\"][p1,1].cpu(),\n",
        "               color='green', marker='X', s=350, edgecolors='white', label='Conflict Point', zorder=20)\n",
        "\n",
        "    # テキスト情報\n",
        "    res_str = \"SUCCESS\" if preds[sample_idx] == 1 else \"FAILURE\"\n",
        "    plt.title(f\"PIGNN v7 Tactical Analysis: {res_str}\\n\"\n",
        "              f\"AI Prediction Prob: {probs[sample_idx, 1]:.2%} | Max Physics Bias: {res['max_bias']:.3f}\",\n",
        "              fontsize=14, fontweight='bold')\n",
        "\n",
        "    # 凡例の重複を削除\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    by_label = dict(zip(labels, handles))\n",
        "    ax.legend(by_label.values(), by_label.keys(), loc='upper right', frameon=True, shadow=True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 実行\n",
        "visualize_pignn_v7_absolute_colors(model, test_loader, sample_idx=0)"
      ],
      "metadata": {
        "id": "qCRYWGTXgvJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# ==========================================\n",
        "# 1. テストセット全体から最小バイアスを探索\n",
        "# ==========================================\n",
        "model.eval()\n",
        "min_bias = float('inf')\n",
        "low_bias_data = None\n",
        "low_bias_idx = -1\n",
        "\n",
        "print(\"テストセットから最も物理的に安定したシーンを探索中...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    # test_set は Data オブジェクトのリストであることを前提としています\n",
        "    for i, data in enumerate(test_set):\n",
        "        # データのデバイス移動\n",
        "        data_to_device = data.to(device)\n",
        "\n",
        "        # 解析関数を呼び出し（tau=1.5秒後の未来を予測）\n",
        "        res = analyze_physics_bias(model, data_to_device, tau=1.5)\n",
        "\n",
        "        current_max_bias = res['max_bias']\n",
        "\n",
        "        # 最小値を更新\n",
        "        if current_max_bias < min_bias:\n",
        "            min_bias = current_max_bias\n",
        "            low_bias_idx = i\n",
        "            low_bias_data = data_to_device\n",
        "\n",
        "print(f\"探索完了\")\n",
        "print(f\"発見された最小バイアス: {min_bias:.4f}\")\n",
        "print(f\"該当データのインデックス: {low_bias_idx}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. 修正版：IndexError回避用可視化呼び出し\n",
        "# ==========================================\n",
        "\n",
        "def visualize_specific_scene(model, data_list, target_idx, tau=1.5):\n",
        "    \"\"\"\n",
        "    特定のインデックスのデータだけを抽出し、\n",
        "    バッチとして可視化関数に渡すことで IndexError を防ぐ\n",
        "    \"\"\"\n",
        "    # ターゲットのデータ1枚だけを含むリストを作成\n",
        "    single_data_list = [data_list[target_idx]]\n",
        "\n",
        "    # バッチサイズ1の専用ローダーを作成\n",
        "    single_loader = DataLoader(single_data_list, batch_size=1)\n",
        "\n",
        "    # 既存の可視化関数を呼び出し\n",
        "    # バッチ内のインデックスは必ず 0 になる\n",
        "    visualize_pignn_v7_absolute_colors(model, single_loader, sample_idx=0, tau=tau)\n",
        "\n",
        "# 実行：物理的に最も「綺麗」なシーンを描画\n",
        "visualize_specific_scene(model, test_set, low_bias_idx, tau=1.5)"
      ],
      "metadata": {
        "id": "Ur3rTejiX-3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_most_intense_duel(model, data_list, tau=1.5):\n",
        "    model.eval()\n",
        "    max_duel_bias = -1.0\n",
        "    best_idx = -1\n",
        "\n",
        "    print(\"攻守が最も激しく『ぶつかる』シーンを探索中...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(data_list):\n",
        "            data_to_device = data.to(device)\n",
        "            # 全ペアのバイアス詳細を取得\n",
        "            res = analyze_physics_bias(model, data_to_device, tau=tau)\n",
        "\n",
        "            p1, p2 = res[\"top_pair\"]\n",
        "            # チームIDを取得 (index 6)\n",
        "            t1 = data.x[p1, 6].item()\n",
        "            t2 = data.x[p2, 6].item()\n",
        "\n",
        "            # 異なるチーム同士（0.0:Home vs 1.0:Away）の衝突のみをターゲットにする\n",
        "            # 1.0 - 0.0 = 1.0 の絶対値で判定\n",
        "            if abs(t1 - t2) == 1.0:\n",
        "                if res['max_bias'] > max_duel_bias:\n",
        "                    max_duel_bias = res['max_bias']\n",
        "                    best_idx = i\n",
        "\n",
        "    print(f\"発見！ 最大攻守衝突バイアス: {max_duel_bias:.4f} (Index: {best_idx})\")\n",
        "    return best_idx\n",
        "\n",
        "# 1. 激しい競り合いシーンを特定\n",
        "duel_idx = find_most_intense_duel(model, test_set)\n",
        "\n",
        "# 2. 可視化\n",
        "if duel_idx != -1:\n",
        "    visualize_specific_scene(model, test_set, duel_idx, tau=1.5)\n",
        "else:\n",
        "    print(\"条件に合うシーンが見つかりませんでした。\")"
      ],
      "metadata": {
        "id": "zJvzmHVsY30G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}