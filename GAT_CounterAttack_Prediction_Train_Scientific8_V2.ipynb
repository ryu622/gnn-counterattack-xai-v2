{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPuGS3QT9fV6AWXcczFG8Yc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryu622/gnn-counterattack-xai-v2/blob/fix%2Ffile-clean/GAT_CounterAttack_Prediction_Train_Scientific8_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#シード値\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    # Python自体の乱数固定\n",
        "    random.seed(seed)\n",
        "    # OS環境の乱数固定\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    # Numpyの乱数固定\n",
        "    np.random.seed(seed)\n",
        "    # PyTorchの乱数固定\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # マルチGPUの場合\n",
        "    # 計算の決定論的挙動を強制（これを入れると少し遅くなることがありますが、再現性は完璧になります）\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# 好きな数字（42が一般的）で固定\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "QKyaHmiRGtjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcw4b0y1F0PK"
      },
      "outputs": [],
      "source": [
        "#GoogleDriveをマウント\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Google Driveを仮想ファイルシステムにマウント\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 必須モジュールのインポート\n",
        "!pip install torch_geometric\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from sklearn.metrics import classification_report\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch import optim\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import re\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# 表示設定\n",
        "np.set_printoptions(suppress=True, precision=3)\n",
        "pd.set_option('display.precision', 3)    # 小数点以下の表示桁\n",
        "pd.set_option('display.max_rows', 50)   # 表示する行数の上限\n",
        "pd.set_option('display.max_columns', 15)  # 表示する列数の上限\n",
        "%precision 3"
      ],
      "metadata": {
        "id": "K7DHFGzCGYlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用データは以前までと同様"
      ],
      "metadata": {
        "id": "EpfKnRXp6DxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# ==========================================\n",
        "# ロード・クリーンアップ・最終確認\n",
        "# ==========================================\n",
        "v7_load_path = \"/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_v14_final.pt\"\n",
        "\n",
        "try:\n",
        "    print(f\"v7 最終データをロード中: {v7_load_path}\")\n",
        "    # 統合済みファイルをロード\n",
        "    checkpoint = torch.load(v7_load_path, weights_only=False)\n",
        "\n",
        "    # v7 の Builder ですでに 7次元特徴量 (x, y, vx, vy, dist_goal, dist_ball, team_id)\n",
        "    # を付与しているため、基本的にはそのまま DataLoader に渡せます。\n",
        "    train_set = checkpoint['train_data']\n",
        "    test_set = checkpoint['test_data']\n",
        "\n",
        "    # DataLoader を構築 (バッチサイズはメモリに合わせて調整してください)\n",
        "    train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
        "\n",
        "    print(f\"--- ロード完了 ---\")\n",
        "    print(f\"訓練セット: {len(train_set)} 枚\")\n",
        "    print(f\"テストセット: {len(test_set)} 枚\")\n",
        "\n",
        "    # 【最重要チェック】ノード数と次元数の確認\n",
        "    sample_train = train_set[0]\n",
        "    sample_test = test_set[0]\n",
        "\n",
        "    # 期待値: [23, 7] (22人 + ボール1つ、7種類の特徴量)\n",
        "    print(f\"訓練データの形状: {sample_train.x.shape}\")\n",
        "    print(f\"テストデータの形状: {sample_test.x.shape}\")\n",
        "\n",
        "    # 1. 次元数チェック\n",
        "    if sample_train.x.shape[1] == 7:\n",
        "        print(\"次元数: OK (7次元)\")\n",
        "    else:\n",
        "        print(f\"次元数警告: {sample_train.x.shape[1]}次元になっています。\")\n",
        "\n",
        "    # 2. ノード数チェック\n",
        "    if sample_train.x.shape[0] == 23:\n",
        "        print(\"ノード数: OK (23ノード固定)\")\n",
        "    else:\n",
        "        print(f\"ノード数警告: {sample_train.x.shape[0]}ノードになっています。\")\n",
        "\n",
        "    # 3. 速度データの存在チェック\n",
        "    # 2列目(vx)の絶対値平均が0でなければ、速度が正しく入っています\n",
        "    v_mean = torch.abs(sample_train.x[:, 2]).mean().item()\n",
        "    if v_mean > 0.01:\n",
        "        print(f\"物理量チェック: OK (平均速度属性を確認)\")\n",
        "    else:\n",
        "        print(\"物理量警告: 速度が0に張り付いています。Builderを再確認してください。\")\n",
        "\n",
        "    if sample_train.x.shape[1] == 7 and v_mean > 0.01:\n",
        "        print(\"\\nすべての準備が整いました。PIGNN 学習を開始してください。\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"ファイルが見つかりません。パスを確認してください: {v7_load_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"エラー発生: {e}\")"
      ],
      "metadata": {
        "id": "XjQznJNPUTHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルの定義（PIGNNのオリジナルクラス）"
      ],
      "metadata": {
        "id": "ocDgeAXM6Ifi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_geometric.utils import softmax\n",
        "from torch_geometric.data import Data\n",
        "import os\n",
        "\n",
        "# ==========================================\n",
        "# 1. 前処理関数の定義\n",
        "# ==========================================\n",
        "def preprocess_batch(data, device):\n",
        "    # スケーリングはBuilder側で行われているため、ここでは型変換とDevice転送に集中\n",
        "    data.x = data.x.float()\n",
        "    data.pos = data.pos.float()\n",
        "    data.vel = data.vel.float()\n",
        "    return data.to(device)\n",
        "\n",
        "# ==========================================\n",
        "# 2. モデル定義（チーム属性によるメッセージ分岐の実装）\n",
        "# ==========================================\n",
        "class PIGNNLayer(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, tau=1.5):\n",
        "        super(PIGNNLayer, self).__init__(aggr='add')\n",
        "        self.tau = tau\n",
        "        self.lin = nn.Linear(in_channels, out_channels)\n",
        "        self.att = nn.Parameter(torch.Tensor(1, out_channels * 2))\n",
        "        nn.init.xavier_uniform_(self.att)\n",
        "\n",
        "    def forward(self, x, edge_index, pos, vel):\n",
        "        h = self.lin(x)\n",
        "        # チームID(index 6)をメッセージパッシングに渡す\n",
        "        return self.propagate(edge_index, x=h, pos=pos, vel=vel, team=x[:, 6:7])\n",
        "\n",
        "    def message(self, x_i, x_j, pos_i, pos_j, vel_i, vel_j, edge_index_i, team_i, team_j):\n",
        "        # 理論1: 未来位置予測ベースのバイアス\n",
        "        pos_i_pred = pos_i + vel_i * self.tau\n",
        "        pos_j_pred = pos_j + vel_j * self.tau\n",
        "        dist_future = torch.norm(pos_i_pred - pos_j_pred, dim=-1, keepdim=True)\n",
        "        physics_bias = torch.exp(-dist_future / 2.0)\n",
        "\n",
        "        # 【追加】チーム関係分岐: 味方なら+0.5, 敵なら-0.5の注目度補正\n",
        "        is_teammate = (team_i == team_j).float()\n",
        "        team_bias = torch.where(is_teammate > 0.5, 0.5, -0.5)\n",
        "\n",
        "        alpha = torch.cat([x_i, x_j], dim=-1)\n",
        "        alpha = (alpha * self.att).sum(dim=-1, keepdim=True)\n",
        "        # 物理バイアス + チーム属性バイアス を統合\n",
        "        alpha = softmax(F.leaky_relu(alpha) + physics_bias + team_bias, edge_index_i)\n",
        "\n",
        "        return alpha * x_j\n",
        "\n",
        "class PIGNNClassifier(nn.Module):\n",
        "    def __init__(self, hidden_channels=64):\n",
        "        super(PIGNNClassifier, self).__init__()\n",
        "        self.conv1 = PIGNNLayer(7, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        x = F.elu(self.conv1(x, edge_index, pos, vel))\n",
        "        x = self.conv2(x, edge_index, pos, vel)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return F.log_softmax(self.lin(x), dim=1)\n",
        "\n",
        "# ==========================================\n",
        "# 3. 理論修正：集団運動学的制約（L_phys）\n",
        "# ==========================================\n",
        "def pignn_theoretical_loss(output, target, data, alpha=0.1):\n",
        "    # L_task: クラス重み付き（Success:3.3倍）\n",
        "    weights = torch.tensor([1.0, 3.3], device=output.device)\n",
        "    loss_task = F.nll_loss(output, target, weight=weights)\n",
        "\n",
        "    # L_phys: 集団推進力制約\n",
        "    # Success確率が高いほど、チーム全体の重心(全ノード平均)が右(+vx)であることを求める\n",
        "    probs = torch.exp(output)[:, 1]\n",
        "\n",
        "    # グラフごとの平均vxを算出（選手全員の動きを統合）\n",
        "    # data.batch を用いて各グラフ(シーン)の平均vxを計算\n",
        "    batch_size = output.size(0)\n",
        "    # 各グラフの平均vxを計算\n",
        "    avg_vxs = []\n",
        "    for i in range(batch_size):\n",
        "        mask = (data.batch == i)\n",
        "        avg_vxs.append(torch.mean(data.vel[mask, 0])) # 各シーンの全ノード平均vx\n",
        "\n",
        "    avg_vxs = torch.stack(avg_vxs)\n",
        "\n",
        "    # 物理損失：Success確率 × ReLU(-平均vx)\n",
        "    # シーン全体が左に流れているのに「成功」と出すと強く罰せられる\n",
        "    loss_phys = torch.mean(probs * torch.relu(-avg_vxs))\n",
        "\n",
        "    total_loss = loss_task + (alpha * loss_phys)\n",
        "    return total_loss, loss_task, loss_phys\n",
        "\n",
        "# ==========================================\n",
        "# 4. 学習・評価ループ\n",
        "# ==========================================\n",
        "def train_pignn_epoch_dynamic(model, loader, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    total_loss, total_phys = 0, 0\n",
        "\n",
        "    # アニーリング（後半10エポック以降で物理を強化）\n",
        "    if epoch <= 10:\n",
        "        current_alpha = 0.1\n",
        "    else:\n",
        "        current_alpha = min(0.1 + (epoch - 10) * 0.2, 5.0)\n",
        "\n",
        "    for data in loader:\n",
        "        data = preprocess_batch(data, device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(data)\n",
        "        loss, _, l_phys = pignn_theoretical_loss(out, data.y.view(-1), data, alpha=current_alpha)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "        total_phys += l_phys.item() * data.num_graphs\n",
        "\n",
        "    return total_loss / len(loader.dataset), total_phys / len(loader.dataset), current_alpha\n",
        "\n",
        "def test_pignn(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = preprocess_batch(data, device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == data.y.view(-1)).sum().item()\n",
        "    return correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "hIyIVn-LHCPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tSAGHtHk_5qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#実行\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ハイパーパラメータ（あなたの設定を維持）\n",
        "EPOCHS = 50\n",
        "LR = 0.0005\n",
        "\n",
        "history = {\n",
        "    'total_loss': [],\n",
        "    'physics_loss': [],\n",
        "    'test_acc': [],\n",
        "    'alpha': []\n",
        "}\n",
        "\n",
        "# モデルと最適化手法の初期化\n",
        "model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# ファイル名\n",
        "save_path = 'best_pignn_theoretical_V1.pth'\n",
        "\n",
        "print(f\"PIGNN学習開始 (Device: {device} | 物理・分類 理論統合モード)\")\n",
        "print(f\"Input Features: 7 [x, y, vx, vy, (1-px), dist_ball, team_id]\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # 先ほど修正した、勾配が繋がった train_pignn_epoch_dynamic を呼び出し\n",
        "    avg_loss, avg_phys, current_alpha = train_pignn_epoch_dynamic(model, train_loader, optimizer, device, epoch)\n",
        "\n",
        "    # テスト評価\n",
        "    acc = test_pignn(model, test_loader, device)\n",
        "\n",
        "    # 履歴の保存\n",
        "    history['total_loss'].append(avg_loss)\n",
        "    history['physics_loss'].append(avg_phys)\n",
        "    history['test_acc'].append(acc)\n",
        "    history['alpha'].append(current_alpha)\n",
        "\n",
        "    # 進捗表示：Phys_L が 0.7439 から変化しているか注目してください\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        # 理論に基づいた Phys_L の挙動を確認しやすく表示\n",
        "        print(f\"Epoch {epoch:03d} | Alpha: {current_alpha:.2f} | Loss: {avg_loss:.4f} | Phys_L (Penalty): {avg_phys:.6f} | Acc: {acc:.4f}\")\n",
        "\n",
        "    # 保存ロジック：精度が向上したときのみ保存\n",
        "    if epoch == 1 or acc > max(history['test_acc'][:-1]):\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\" >> [Update] Model Saved: {save_path} (Best Acc: {acc:.4f})\")\n",
        "\n",
        "print(f\"\\n学習完了。最高精度: {max(history['test_acc']):.4f}\")"
      ],
      "metadata": {
        "id": "XvcuJczK-BWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. 最終評価と物理的妥当性レポート (集団運動学対応版)\n",
        "# ==========================================\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. 保存したモデルをロード\n",
        "model_path = 'best_pignn_theoretical_V1.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "success_team_vxs = [] # 成功と予測した時の「チーム平均速度」を記録\n",
        "\n",
        "# 2. テストデータで最終予測\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        data = preprocess_batch(data, device)\n",
        "\n",
        "        out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "\n",
        "        # --- 物理的妥当性のためのデータ抽出 ---\n",
        "        # 各グラフ(シーン)ごとのチーム平均vxを計算\n",
        "        for i in range(out.size(0)):\n",
        "            mask = (data.batch == i)\n",
        "            avg_vx = torch.mean(data.vel[mask, 0]).item() # シーン内の全ノード平均vx\n",
        "\n",
        "            # AIが「Success(1)」と予測したシーンの平均vxだけをリストに溜める\n",
        "            if pred[i] == 1:\n",
        "                success_team_vxs.append(avg_vx)\n",
        "\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# 3. レポート表示\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"       PIGNN 最終評価結果 (物理理論・集団運動統合モデル)\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(all_labels, all_preds, target_names=['Failure (0)', 'Success (1)']))\n",
        "\n",
        "# 4. 物理的妥当性の検証結果 (卒論のメイン考察)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"       物理的整合性 検証レポート (集団推進力ベース)\")\n",
        "print(\"=\"*60)\n",
        "if len(success_team_vxs) > 0:\n",
        "    avg_team_vx = np.mean(success_team_vxs)\n",
        "    positive_ratio = np.sum(np.array(success_team_vxs) > 0) / len(success_team_vxs)\n",
        "\n",
        "    print(f\"成功予測シーンにおける『チーム平均速度』vx: {avg_team_vx:.4f} m/s\")\n",
        "    print(f\"成功予測シーンにおけるチーム右向き(正)の割合: {positive_ratio*100:.1f} %\")\n",
        "\n",
        "    # 基準をデータの現実に合わせて調整 (60-70%以上なら物理的に機能しているとみなす)\n",
        "    if positive_ratio > 0.65:\n",
        "        print(\">> 判定: 物理的整合性あり。モデルはチーム全体の『集団推進力』を根拠にしています。\")\n",
        "    else:\n",
        "        print(\">> 判定: 物理的矛盾あり。データの反転ミスか、学習のバイアスが強すぎます。\")\n",
        "else:\n",
        "    print(\"Successと予測されたデータがありません。\")\n",
        "\n",
        "# 5. 混同行列の描画\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title('Confusion Matrix (Team-Physics PIGNN)')\n",
        "plt.ylabel('Actual (True)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YFyM9YpTHYO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def verify_pt_file(file_path):\n",
        "    # ファイルをロード\n",
        "    data_list = torch.load(file_path, weights_only=False)\n",
        "    if isinstance(data_list, dict):\n",
        "        # もし train_data, test_data に分かれて保存されている場合\n",
        "        data_list = data_list.get('test_data', []) or data_list.get('train_data', [])\n",
        "\n",
        "    success_right = 0\n",
        "    success_total = 0\n",
        "\n",
        "    for data in data_list:\n",
        "        # ラベルがSuccess(1)のものだけを抽出\n",
        "        if data.y.item() == 1:\n",
        "            success_total += 1\n",
        "\n",
        "            # ボールのvx (ノード特徴量 x の index 2) を取得\n",
        "            # ノードの最後がボールという前提\n",
        "            ball_vx = data.x[-1, 2].item()\n",
        "\n",
        "            # 物理的に右(正)に動いているか判定\n",
        "            if ball_vx > 0:\n",
        "                success_right += 1\n",
        "\n",
        "    if success_total == 0:\n",
        "        print(\"Successラベルのデータが見つかりませんでした。\")\n",
        "        return\n",
        "\n",
        "    ratio = (success_right / success_total) * 100\n",
        "    print(f\"--- .ptファイル内部検証結果 ---\")\n",
        "    print(f\"Successラベルの総数: {success_total}\")\n",
        "    print(f\"そのうち物理的に右(vx > 0)を向いている数: {success_right}\")\n",
        "    print(f\"【結論】 成功シーンの物理的正解率: {ratio:.1f}%\")\n",
        "\n",
        "    if ratio < 50:\n",
        "        print(\"\\n警告: 前処理が失敗しています。\")\n",
        "        print(\"ラベルは『成功』なのに、座標上は『逆走』しているデータが過半数です。\")\n",
        "    elif ratio < 80:\n",
        "        print(\"\\n注意: 整合性が不十分です。バックパス等のノイズが多いか、反転ミスが混ざっています。\")\n",
        "    else:\n",
        "        print(\"\\n合格: 物理とラベルが一致しています。\")\n",
        "\n",
        "# 実行\n",
        "verify_pt_file('/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_v14_final.pt')"
      ],
      "metadata": {
        "id": "RJivqXgT8v5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def verify_pt_players_velocity(file_path):\n",
        "    # ファイルをロード\n",
        "    data_list = torch.load(file_path, weights_only=False)\n",
        "    if isinstance(data_list, dict):\n",
        "        data_list = data_list.get('test_data', []) or data_list.get('train_data', [])\n",
        "\n",
        "    success_total = 0\n",
        "    group_right_count = 0  # 全選手の平均が右向き\n",
        "    group_left_count = 0   # 全選手の平均が左向き\n",
        "\n",
        "    print(f\"--- 選手全員の速度ベクトルによる詳細検証 ---\")\n",
        "\n",
        "    for i, data in enumerate(data_list):\n",
        "        if data.y.item() == 1:  # Successラベルのみ\n",
        "            success_total += 1\n",
        "\n",
        "            # data.x[:, 2] は全ノード（選手+ボール）の vx\n",
        "            # ボール（最後）を除いた選手全員の平均 vx を計算\n",
        "            player_vxs = data.x[:-1, 2]\n",
        "            avg_player_vx = torch.mean(player_vxs).item()\n",
        "\n",
        "            if avg_player_vx > 0:\n",
        "                group_right_count += 1\n",
        "            else:\n",
        "                group_left_count += 1\n",
        "\n",
        "    print(f\"Successラベル総数: {success_total}\")\n",
        "    print(f\"  └ 選手集団が『右』へ移動中: {group_right_count} シーン ({group_right_count/success_total*100:.1f}%)\")\n",
        "    print(f\"  └ 選手集団が『左』へ移動中: {group_left_count} シーン ({group_left_count/success_total*100:.1f}%)\")\n",
        "\n",
        "    print(\"\\n【客観的な診断】\")\n",
        "    if group_left_count > group_right_count:\n",
        "        print(\"致命的エラー: 『成功シーン』の多くで、選手全員が左に向かって走っています。\")\n",
        "        print(\"   原因：flip_factor（反転処理）が真逆、または適用されていません。\")\n",
        "    else:\n",
        "        print(\"選手集団の方向は概ね一致していますが、ボールの方向とズレがある可能性があります。\")\n",
        "\n",
        "verify_pt_players_velocity('/content/drive/MyDrive/GNN_Football_Analysis/Processed_Data/gnn_data_v14_final.pt')"
      ],
      "metadata": {
        "id": "ql_zVGUr9ShM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "def run_leakage_diagnostic(model, loader, device):\n",
        "    model.eval()\n",
        "    # 特徴量ラベル: 0:x, 1:y, 2:vx, 3:vy, 4:dist_goal, 5:dist_ball, 6:team_id\n",
        "    feature_names = [\"x\", \"y\", \"vx\", \"vy\", \"dist_goal\", \"dist_ball\", \"team_id\"]\n",
        "\n",
        "    print(f\"{'Removed Feature':<15} | {'Test Acc':<10} | {'Recall (S)':<10}\")\n",
        "    print(\"-\" * 45)\n",
        "\n",
        "    # ベースライン（全特徴量あり）\n",
        "    base_acc = test_pignn(model, loader, device)\n",
        "    print(f\"{'None (Baseline)':<15} | {base_acc:.4f}\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(7):\n",
        "            correct = 0\n",
        "            tp = 0 # True Positive\n",
        "            fn = 0 # False Negative\n",
        "\n",
        "            for data in loader:\n",
        "                data = preprocess_batch(data, device)\n",
        "\n",
        "                # 特定の特徴量をゼロに置き換える\n",
        "                x_shuffled = data.x.clone()\n",
        "                x_shuffled[:, i] = 0.0\n",
        "\n",
        "                # 推論\n",
        "                out = model(Data(x=x_shuffled, edge_index=data.edge_index,\n",
        "                                 batch=data.batch, pos=data.pos, vel=data.vel))\n",
        "                pred = out.argmax(dim=1)\n",
        "\n",
        "                # 精度計算\n",
        "                y_true = data.y.view(-1)\n",
        "                correct += (pred == y_true).sum().item()\n",
        "\n",
        "                # Recall (Success) 計算\n",
        "                tp += ((pred == 1) & (y_true == 1)).sum().item()\n",
        "                fn += ((pred == 0) & (y_true == 1)).sum().item()\n",
        "\n",
        "            acc = correct / len(loader.dataset)\n",
        "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            print(f\"{feature_names[i]:<15} | {acc:.4f}     | {recall:.4f}\")\n",
        "\n",
        "# 実行\n",
        "run_leakage_diagnostic(model, test_loader, device)"
      ],
      "metadata": {
        "id": "4iQw8_UUA99Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ゴールへの距離は成功の定義が３５ｍ地点への侵入と定義してしまっているせいで、過度に依存している。そのため、この特徴量を抜いて修正してみる"
      ],
      "metadata": {
        "id": "DXI8YNvJEdpF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "修正"
      ],
      "metadata": {
        "id": "alrt14IzB1-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_geometric.utils import softmax\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# 1. 前処理関数の定義（ここでリークを遮断！）\n",
        "# ==========================================\n",
        "def preprocess_batch(data, device):\n",
        "    data.x = data.x.float()\n",
        "\n",
        "    # 【核心】index 4 (dist_goal) を除外してリークを物理的に遮断\n",
        "    # 元: [x, y, vx, vy, dist_goal, dist_ball, team_id] (7次元)\n",
        "    # 新: [x, y, vx, vy, dist_ball, team_id] (6次元)\n",
        "    x_no_leak = torch.cat([data.x[:, :4], data.x[:, 5:]], dim=1)\n",
        "    data.x = x_no_leak\n",
        "\n",
        "    data.pos = data.pos.float()\n",
        "    data.vel = data.vel.float()\n",
        "    return data.to(device)\n",
        "\n",
        "# ==========================================\n",
        "# 2. モデル定義（入力6次元・チーム属性分岐対応）\n",
        "# ==========================================\n",
        "class PIGNNLayer(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, tau=1.5):\n",
        "        super(PIGNNLayer, self).__init__(aggr='add')\n",
        "        self.tau = tau\n",
        "        self.lin = nn.Linear(in_channels, out_channels)\n",
        "        self.att = nn.Parameter(torch.Tensor(1, out_channels * 2))\n",
        "        nn.init.xavier_uniform_(self.att)\n",
        "\n",
        "    def forward(self, x, edge_index, pos, vel):\n",
        "        h = self.lin(x)\n",
        "        # 6次元化に伴い、team_id は index 5 に移動している\n",
        "        return self.propagate(edge_index, x=h, pos=pos, vel=vel, team=x[:, 5:6])\n",
        "\n",
        "    def message(self, x_i, x_j, pos_i, pos_j, vel_i, vel_j, edge_index_i, team_i, team_j):\n",
        "        # 理論1: 未来位置予測ベースのバイアス\n",
        "        pos_i_pred = pos_i + vel_i * self.tau\n",
        "        pos_j_pred = pos_j + vel_j * self.tau\n",
        "        dist_future = torch.norm(pos_i_pred - pos_j_pred, dim=-1, keepdim=True)\n",
        "        physics_bias = torch.exp(-dist_future / 2.0)\n",
        "\n",
        "        # チーム関係分岐: 味方なら注目度を上げ、敵なら抑制する\n",
        "        is_teammate = (team_i == team_j).float()\n",
        "        team_bias = torch.where(is_teammate > 0.5, 0.5, -0.5)\n",
        "\n",
        "        alpha = torch.cat([x_i, x_j], dim=-1)\n",
        "        alpha = (alpha * self.att).sum(dim=-1, keepdim=True)\n",
        "        alpha = softmax(F.leaky_relu(alpha) + physics_bias + team_bias, edge_index_i)\n",
        "\n",
        "        return alpha * x_j\n",
        "\n",
        "class PIGNNClassifier(nn.Module):\n",
        "    def __init__(self, hidden_channels=64):\n",
        "        super(PIGNNClassifier, self).__init__()\n",
        "        # 入力次元を 6 に変更\n",
        "        self.conv1 = PIGNNLayer(6, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        x = F.elu(self.conv1(x, edge_index, pos, vel))\n",
        "        x = self.conv2(x, edge_index, pos, vel)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return F.log_softmax(self.lin(x), dim=1)\n",
        "\n",
        "# ==========================================\n",
        "# 3. 損失関数の定義（集団運動学的制約）\n",
        "# ==========================================\n",
        "def pignn_theoretical_loss(output, target, data, alpha=0.1):\n",
        "    weights = torch.tensor([1.0, 3.3], device=output.device)\n",
        "    loss_task = F.nll_loss(output, target, weight=weights)\n",
        "\n",
        "    probs = torch.exp(output)[:, 1]\n",
        "\n",
        "    # グラフごとのチーム平均vxを算出\n",
        "    batch_size = output.size(0)\n",
        "    avg_vxs = []\n",
        "    for i in range(batch_size):\n",
        "        mask = (data.batch == i)\n",
        "        avg_vxs.append(torch.mean(data.vel[mask, 0]))\n",
        "\n",
        "    avg_vxs = torch.stack(avg_vxs)\n",
        "    loss_phys = torch.mean(probs * torch.relu(-avg_vxs))\n",
        "\n",
        "    total_loss = loss_task + (alpha * loss_phys)\n",
        "    return total_loss, loss_task, loss_phys\n",
        "\n",
        "# ==========================================\n",
        "# 4. 学習・評価関数\n",
        "# ==========================================\n",
        "def train_pignn_epoch_dynamic(model, loader, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    total_loss, total_phys = 0, 0\n",
        "    if epoch <= 10:\n",
        "        current_alpha = 0.1\n",
        "    else:\n",
        "        current_alpha = min(0.1 + (epoch - 10) * 0.2, 5.0)\n",
        "\n",
        "    for data in loader:\n",
        "        data = preprocess_batch(data, device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss, _, l_phys = pignn_theoretical_loss(out, data.y.view(-1), data, alpha=current_alpha)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "        total_phys += l_phys.item() * data.num_graphs\n",
        "    return total_loss / len(loader.dataset), total_phys / len(loader.dataset), current_alpha\n",
        "\n",
        "def test_pignn(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = preprocess_batch(data, device)\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == data.y.view(-1)).sum().item()\n",
        "    return correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "JmZLAH1RCEDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ハイパーパラメータ\n",
        "EPOCHS = 50\n",
        "LR = 0.0005\n",
        "\n",
        "history = {\n",
        "    'total_loss': [],\n",
        "    'physics_loss': [],\n",
        "    'test_acc': [],\n",
        "    'alpha': []\n",
        "}\n",
        "\n",
        "# モデルの初期化（入力6次元版）\n",
        "model = PIGNNClassifier(hidden_channels=64).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "# ファイル名（リーク対策版として保存）\n",
        "save_path = 'best_pignn_v12_no_leak.pth'\n",
        "\n",
        "print(f\"PIGNN学習開始 (Device: {device} | リーク対策・集団物理統合モード)\")\n",
        "# 修正：モデルに実際に入力されるのは6次元\n",
        "print(f\"Input Features (Processed): 6 [x, y, vx, vy, dist_ball, team_id]\")\n",
        "print(f\"Skipped Feature: dist_goal (to prevent data leakage)\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # 内部で preprocess_batch が走り、6次元に変換される\n",
        "    avg_loss, avg_phys, current_alpha = train_pignn_epoch_dynamic(model, train_loader, optimizer, device, epoch)\n",
        "\n",
        "    # テスト評価\n",
        "    acc = test_pignn(model, test_loader, device)\n",
        "\n",
        "    # 履歴の保存\n",
        "    history['total_loss'].append(avg_loss)\n",
        "    history['physics_loss'].append(avg_phys)\n",
        "    history['test_acc'].append(acc)\n",
        "    history['alpha'].append(current_alpha)\n",
        "\n",
        "    # 進捗表示\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | Alpha: {current_alpha:.2f} | Loss: {avg_loss:.4f} | Phys_L: {avg_phys:.6f} | Acc: {acc:.4f}\")\n",
        "\n",
        "    # 最高精度の更新保存\n",
        "    if epoch == 1 or acc > max(history['test_acc'][:-1]):\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\" >> [Update] Model Saved: {save_path} (Best Acc: {acc:.4f})\")\n",
        "\n",
        "print(f\"\\n学習完了。最高精度: {max(history['test_acc']):.4f}\")"
      ],
      "metadata": {
        "id": "mNpSQcXaCWnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. 最終評価と物理的妥当性レポート (集団運動学対応版)\n",
        "# ==========================================\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. 保存した【リーク対策版】モデルをロード\n",
        "# 学習時に指定した save_path に合わせてください\n",
        "model_path = 'best_pignn_v12_no_leak.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "success_team_vxs = [] # 成功と予測した時の「チーム平均速度」を記録\n",
        "\n",
        "# 2. テストデータで最終予測\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        # 重要：評価時も preprocess_batch を通して 7次元→6次元 に変換する\n",
        "        data = preprocess_batch(data, device)\n",
        "\n",
        "        out = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "\n",
        "        # --- 物理的妥当性のためのデータ抽出 ---\n",
        "        for i in range(out.size(0)):\n",
        "            mask = (data.batch == i)\n",
        "            # data.vel は [全ノード, 2] なのでそのまま平均をとる\n",
        "            avg_vx = torch.mean(data.vel[mask, 0]).item()\n",
        "\n",
        "            if pred[i] == 1:\n",
        "                success_team_vxs.append(avg_vx)\n",
        "\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# 3. レポート表示\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"       PIGNN 最終評価結果 (リーク対策・集団物理統合モデル)\")\n",
        "print(\"=\"*60)\n",
        "# ターゲットネームを実際のデータに合わせて表示\n",
        "print(classification_report(all_labels, all_preds, target_names=['Failure (0)', 'Success (1)']))\n",
        "\n",
        "# 4. 物理的妥当性の検証結果 (卒論のメイン考察)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"       物理的整合性 検証レポート (集団推進力ベース)\")\n",
        "print(\"=\"*60)\n",
        "if len(success_team_vxs) > 0:\n",
        "    avg_team_vx = np.mean(success_team_vxs)\n",
        "    positive_ratio = np.sum(np.array(success_team_vxs) > 0) / len(success_team_vxs)\n",
        "\n",
        "    print(f\"成功予測シーンにおける『チーム平均速度』vx: {avg_team_vx:.4f} m/s\")\n",
        "    print(f\"成功予測シーンにおけるチーム右向き(正)の割合: {positive_ratio*100:.1f} %\")\n",
        "\n",
        "    # 判定基準：カオスなデータ(整合性60-70%)でも、50%を明確に超えれば物理制約の効果ありと言える\n",
        "    if positive_ratio > 0.60:\n",
        "        print(\">> 判定: 物理的整合性あり。モデルは『集団の推進力』を根拠としています。\")\n",
        "    else:\n",
        "        print(\">> 判定: 物理的矛盾またはデータの限界。Alpha値を調整するか、データの精査が必要です。\")\n",
        "else:\n",
        "    print(\"Successと予測されたデータがありません。\")\n",
        "\n",
        "# 5. 混同行列の描画\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title('Confusion Matrix (v12 No-Leak PIGNN)')\n",
        "plt.ylabel('Actual (True)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V5FtgJlMCc66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "以上がPIGNNのベースラインモデル"
      ],
      "metadata": {
        "id": "sVwMYowN6Qvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ただのMLP"
      ],
      "metadata": {
        "id": "Dhd4oLJUFBWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "# ==========================================\n",
        "# 1. MLPモデルの定義\n",
        "# ==========================================\n",
        "class SimpleMLPClassifier(nn.Module):\n",
        "    def __init__(self, in_channels=6, hidden_channels=64):\n",
        "        super(SimpleMLPClassifier, self).__init__()\n",
        "        # グラフ畳み込みを使わず、全結合層のみで構成\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_channels, hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_channels, hidden_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_channels, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, data):\n",
        "        # 1. グラフ構造を無視して、全ノードの平均をとる (Pooling)\n",
        "        # [num_nodes, 6] -> [batch_size, 6]\n",
        "        x = global_mean_pool(data.x, data.batch)\n",
        "\n",
        "        # 2. 全結合層に投入\n",
        "        return F.log_softmax(self.mlp(x), dim=1)\n",
        "\n",
        "# ==========================================\n",
        "# 2. MLP用の学習ループ (物理損失なしの標準的な学習)\n",
        "# ==========================================\n",
        "def train_mlp_baseline(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    # MLPは比較用なので、重み付きクロスエントロピーのみで学習\n",
        "    weights = torch.tensor([1.0, 3.3], device=device)\n",
        "\n",
        "    for data in loader:\n",
        "        data = preprocess_batch(data, device) # 6次元化\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = F.nll_loss(out, data.y.view(-1), weight=weights)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * data.num_graphs\n",
        "    return total_loss / len(loader.dataset)\n",
        "\n",
        "# --- 実行コード ---\n",
        "mlp_model = SimpleMLPClassifier(in_channels=6).to(device)\n",
        "mlp_optimizer = torch.optim.Adam(mlp_model.parameters(), lr=0.0005)\n",
        "\n",
        "print(\"MLP Baseline 学習開始...\")\n",
        "for epoch in range(1, 51):\n",
        "    loss = train_mlp_baseline(mlp_model, train_loader, mlp_optimizer, device)\n",
        "    if epoch % 10 == 0:\n",
        "        acc = test_pignn(mlp_model, test_loader, device) # 評価は共通\n",
        "        print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Acc: {acc:.4f}\")\n",
        "\n",
        "torch.save(mlp_model.state_dict(), 'mlp_baseline.pth')"
      ],
      "metadata": {
        "id": "cCob8uypFD02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLPの最終評価\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"       MLP Baseline 最終評価レポート\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "mlp_model.eval()\n",
        "all_preds_mlp = []\n",
        "all_labels_mlp = []\n",
        "success_vxs_mlp = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        data = preprocess_batch(data, device) # 6次元化\n",
        "        out = mlp_model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "\n",
        "        for i in range(out.size(0)):\n",
        "            mask = (data.batch == i)\n",
        "            avg_vx = torch.mean(data.vel[mask, 0]).item()\n",
        "            if pred[i] == 1:\n",
        "                success_vxs_mlp.append(avg_vx)\n",
        "\n",
        "        all_preds_mlp.extend(pred.cpu().numpy())\n",
        "        all_labels_mlp.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "print(classification_report(all_labels_mlp, all_preds_mlp, target_names=['Fail', 'Success']))\n",
        "\n",
        "if len(success_vxs_mlp) > 0:\n",
        "    pos_ratio_mlp = np.sum(np.array(success_vxs_mlp) > 0) / len(success_vxs_mlp)\n",
        "    print(f\"MLPの成功予測時・右向き整合性: {pos_ratio_mlp*100:.1f} %\")\n",
        "else:\n",
        "    print(\"Success予測なし\")"
      ],
      "metadata": {
        "id": "XokrwR8VFWuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "敵味方で条件分岐させて学習させるモデル？"
      ],
      "metadata": {
        "id": "mFfdMX218etK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PIGNNLayer(MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels, tau=1.5):\n",
        "        super(PIGNNLayer, self).__init__(aggr='add')\n",
        "        self.tau = tau\n",
        "        self.lin = nn.Linear(in_channels, out_channels)\n",
        "        self.att = nn.Parameter(torch.Tensor(1, out_channels * 2))\n",
        "        nn.init.xavier_uniform_(self.att)\n",
        "\n",
        "    def forward(self, x, edge_index, pos, vel):\n",
        "        h = self.lin(x)\n",
        "        # team_id (index 6) を抽出してメッセージパッシングに渡す\n",
        "        return self.propagate(edge_index, x=h, pos=pos, vel=vel, team=x[:, 6:7])\n",
        "\n",
        "    def message(self, x_i, x_j, pos_i, pos_j, vel_i, vel_j, edge_index_i, team_i, team_j):\n",
        "        # 1. 物理未来位置計算\n",
        "        pos_i_pred = pos_i + vel_i * self.tau\n",
        "        pos_j_pred = pos_j + vel_j * self.tau\n",
        "        dist_future = torch.norm(pos_i_pred - pos_j_pred, dim=-1, keepdim=True)\n",
        "        physics_bias = torch.exp(-dist_future / 2.0)\n",
        "\n",
        "        # 2. 【核心】チーム属性による分岐\n",
        "        # 味方なら 1.0, 敵なら 0.0\n",
        "        is_teammate = (team_i == team_j).float()\n",
        "\n",
        "        # 味方同士ならアテンションを強め、敵なら弱める（守備的干渉）\n",
        "        # この +0.5 と -0.5 が戦術的な意味を生む\n",
        "        team_bias = torch.where(is_teammate > 0.5, torch.tensor(0.5, device=is_teammate.device),\n",
        "                                torch.tensor(-0.5, device=is_teammate.device))\n",
        "\n",
        "        alpha = torch.cat([x_i, x_j], dim=-1)\n",
        "        alpha = (alpha * self.att).sum(dim=-1, keepdim=True)\n",
        "        alpha = softmax(F.leaky_relu(alpha) + physics_bias + team_bias, edge_index_i)\n",
        "\n",
        "        return alpha * x_j"
      ],
      "metadata": {
        "id": "ByCxn5nY8dbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PFI"
      ],
      "metadata": {
        "id": "IzPgBIdiveAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# 1. 保存した最新の「物理損失統合版」モデルパスを指定\n",
        "# 学習時に保存した名前に正確に書き換えてください\n",
        "save_path = \"best_pignn_physics_integrated_v1.pth\"\n",
        "\n",
        "# 2. モデルのインスタンス化\n",
        "model = PIGNNClassifier(hidden_channels=64)\n",
        "\n",
        "# 3. 重みのロード\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if os.path.exists(save_path):\n",
        "    # ロード（最新のPyTorchでは weights_only=True が推奨）\n",
        "    checkpoint = torch.load(save_path, map_location=device, weights_only=True)\n",
        "    model.load_state_dict(checkpoint)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    print(f\"物理制約統合済み PIGNNモデル のロードに成功しました！\")\n",
        "    print(f\"ロード元: {save_path}\")\n",
        "\n",
        "    # 【テスト実行】戻り値の形式が変わっているか確認\n",
        "    sample_data = next(iter(test_loader)).to(device)\n",
        "    try:\n",
        "        sample_data = preprocess_batch(sample_data, device)\n",
        "        test_out = model(sample_data)\n",
        "        if isinstance(test_out, tuple):\n",
        "             print(f\"物理バイアス出力確認: 正常 (戻り値は {len(test_out)} 要素)\")\n",
        "        else:\n",
        "             print(f\"警告: 戻り値が1つです。物理損失統合前のモデルかもしれません。\")\n",
        "    except Exception as e:\n",
        "        print(f\"動作確認中にエラー: {e}\")\n",
        "else:\n",
        "    print(f\"エラー: {save_path} が見つかりません。\")"
      ],
      "metadata": {
        "id": "Qm0xYfDpuD6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最初のバッチを流してエラーが出ないかテスト\n",
        "sample_data = next(iter(test_loader))\n",
        "\n",
        "# 1. まずモデルをデバイスに送る（再確認）\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 2. データをデバイスに送る (preprocess_batch 内で .to(device) されているか確認)\n",
        "sample_data = preprocess_batch(sample_data, device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # 3. 推論を実行\n",
        "    out = model(sample_data)\n",
        "    print(\"推論テスト成功！ 出力形状:\", out.shape) # [16, 2]"
      ],
      "metadata": {
        "id": "kpu0mYUXBN7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_feature_importance_pignn(model, loader, device):\n",
        "    model.eval()\n",
        "    data_list = list(loader)\n",
        "\n",
        "    # 特徴量リスト (6次元)\n",
        "    node_features = ['pos_x', 'pos_y', 'vel_x', 'vel_y', 'dist_goal', 'dist_ball']\n",
        "    all_names = node_features\n",
        "    importances = []\n",
        "\n",
        "    # ベースライン精度計測\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in data_list:\n",
        "            data = preprocess_batch(data.clone())\n",
        "            out = model(data)\n",
        "            all_preds.extend(out.argmax(dim=1).cpu().numpy())\n",
        "            all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "    baseline_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "\n",
        "    for i in range(len(all_names)):\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for data in data_list:\n",
        "                batch_data = data.clone()\n",
        "                batch_data = preprocess_batch(batch_data)\n",
        "                # 該当特徴量を破壊\n",
        "                batch_data.x[:, i] = 0\n",
        "                out = model(batch_data)\n",
        "                preds.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "        acc = (np.array(preds) == np.array(all_labels)).mean()\n",
        "        importances.append(max(0, baseline_acc - acc))\n",
        "\n",
        "    return all_names, importances"
      ],
      "metadata": {
        "id": "dCdnhFeduP92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_feature_importance_pignn(model, loader, device):\n",
        "    model.eval()\n",
        "    data_list = list(loader)\n",
        "\n",
        "    # 【重要修正】 特徴量リストを7次元に更新\n",
        "    node_features = [\n",
        "        'pos_x', 'pos_y',\n",
        "        'vel_x', 'vel_y',\n",
        "        'dist_goal', 'dist_ball',\n",
        "        'team_id' # 7番目の新メンバー\n",
        "    ]\n",
        "    all_names = node_features\n",
        "    importances = []\n",
        "\n",
        "    # 1. ベースライン精度の計算\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for data in data_list:\n",
        "            batch_data = preprocess_batch(data.clone(), device)\n",
        "            out = model(batch_data)\n",
        "            all_preds.extend(out.argmax(dim=1).cpu().numpy())\n",
        "            all_labels.extend(batch_data.y.view(-1).cpu().numpy())\n",
        "\n",
        "    baseline_acc = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "    print(f\"ベースライン精度 (Team-Aware v7): {baseline_acc:.4f}\")\n",
        "\n",
        "    # 2. 各特徴量を順番に破壊\n",
        "    for i in range(len(all_names)):\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for data in data_list:\n",
        "                batch_data = data.clone()\n",
        "                batch_data = preprocess_batch(batch_data, device)\n",
        "\n",
        "                # i番目の特徴量を破壊\n",
        "                batch_data.x[:, i] = 0\n",
        "\n",
        "                out = model(batch_data)\n",
        "                preds.extend(out.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "        acc = (np.array(preds) == np.array(all_labels)).mean()\n",
        "        drop = baseline_acc - acc\n",
        "        importances.append(max(0, drop))\n",
        "        print(f\"特徴量 '{all_names[i]}' 破壊時の精度低下: {drop:.4f}\")\n",
        "\n",
        "    return all_names, importances\n",
        "\n",
        "# --- 実行 ---\n",
        "feature_names, importance_values = calculate_feature_importance_pignn(model, test_loader, device)"
      ],
      "metadata": {
        "id": "fSe8LVVaasiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "アテンション係数の可視化"
      ],
      "metadata": {
        "id": "nNhUKPcP6W4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#アテンション係数の可視化\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def visualize_pignn_final_v3(model, loader, sample_idx=0):\n",
        "    model.eval()\n",
        "    data = next(iter(loader)).to(device)\n",
        "\n",
        "    # 推論\n",
        "    input_data = data.clone()\n",
        "    input_data = preprocess_batch(input_data, device)\n",
        "    with torch.no_grad():\n",
        "        out = model(input_data)\n",
        "        probs = torch.exp(out)\n",
        "        preds = out.argmax(dim=1)\n",
        "\n",
        "    mask = (data.batch == sample_idx)\n",
        "    pos = data.pos[mask].cpu().numpy()\n",
        "    vel = data.vel[mask].cpu().numpy()\n",
        "\n",
        "    # ノード数を確認（36個想定）\n",
        "    num_nodes = pos.shape[0]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    ax.set_facecolor('#f0f0f0')\n",
        "\n",
        "    # ピッチ描画\n",
        "    ax.add_patch(patches.Rectangle((-52.5, -34), 105, 68, fill=False, color='black', lw=2))\n",
        "    ax.plot([0, 0], [-34, 34], color='black', alpha=0.3)\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        # --- 判定ロジックを「インデックス」に変更 ---\n",
        "        if i == num_nodes - 1: # 最後のノードがボール\n",
        "            color, marker, size, z = 'gold', '*', 450, 15\n",
        "        elif i < 11: # 最初の11人が味方（仮定）\n",
        "            color, marker, size, z = 'blue', 'o', 180, 10\n",
        "        elif i < 22: # 次の11人が敵（仮定）\n",
        "            color, marker, size, z = 'red', 'o', 180, 10\n",
        "        else: # それ以外のダミー等\n",
        "            color, marker, size, z = 'gray', 'o', 100, 5\n",
        "\n",
        "        # 描画\n",
        "        ax.scatter(pos[i, 0], pos[i, 1], c=color, marker=marker, s=size, edgecolors='black', zorder=z)\n",
        "        # ベクトル\n",
        "        ax.arrow(pos[i, 0], pos[i, 1], vel[i, 0]*1.5, vel[i, 1]*1.5,\n",
        "                 head_width=1.0, head_length=1.2, fc=color, ec=color, alpha=0.4, zorder=z-1)\n",
        "\n",
        "    plt.title(f\"PIGNN Fixed Visualization\\nPred: {preds[sample_idx]} | Prob: {probs[sample_idx, 1]:.2%}\")\n",
        "    plt.show()\n",
        "\n",
        "# 実行\n",
        "visualize_pignn_final_v3(model, test_loader, sample_idx=0)"
      ],
      "metadata": {
        "id": "t1Pw-F11bPc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習曲線"
      ],
      "metadata": {
        "id": "Y7mObh2d6Z73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 学習時のループ内で train_acc も記録するように修正して実行したと仮定\n",
        "# もし記録していなければ、このコードで現在の history からグラフを出します\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history['total_loss'], label='Train Loss (Total)')\n",
        "# もし train_acc を取っていればここに追加\n",
        "plt.plot(history['test_acc'], label='Test Accuracy', marker='o')\n",
        "\n",
        "plt.title('Check for Overfitting: Loss vs Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2lj45c8TDvdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "検証"
      ],
      "metadata": {
        "id": "Fq9_erp86ynd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ラベルをシャッフルした時の精度"
      ],
      "metadata": {
        "id": "SshWo4xj62Oi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_label_test(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = preprocess_batch(data, device)\n",
        "            # ラベルをランダムにシャッフルする\n",
        "            random_y = data.y[torch.randperm(data.y.size(0))]\n",
        "\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == random_y.view(-1)).sum().item()\n",
        "            total += data.num_graphs\n",
        "\n",
        "    print(f\"ラベルシャッフル時の精度: {correct / total:.4f}\")\n",
        "    print(\">> 0.5 (50%) 前後になれば、モデルは正しくラベルと特徴の関係を学んでいます。\")\n",
        "\n",
        "shuffle_label_test(model, test_loader, device)"
      ],
      "metadata": {
        "id": "xuFcHaVED2B0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "物理損失（損失関数に付け加えた物理項）"
      ],
      "metadata": {
        "id": "Hnh1Z9O767qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 物理損失の平均値を算出\n",
        "avg_phys = sum(history['physics_loss']) / len(history['physics_loss'])\n",
        "print(f\"平均物理損失: {avg_phys:.4f}\")\n",
        "\n",
        "if avg_phys < 25: # 18前後なら非常に優秀\n",
        "    print(\">> 物理的整合性は保たれています。AIは現実的な動きの範囲内で予測しています。\")\n",
        "else:\n",
        "    print(\">> 物理損失が高いです。AIが異常な速度を想定して予測している可能性があります。\")"
      ],
      "metadata": {
        "id": "PP6lAzBVD6GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "過学習を防ぐためにドロップアウト層を追加した修正版PIGNNモデル"
      ],
      "metadata": {
        "id": "8CW6TCnB7Bff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "\n",
        "class PIGNNClassifier_drop(nn.Module):\n",
        "    def __init__(self, hidden_channels=64, dropout_rate=0.3):\n",
        "        super(PIGNNClassifier_drop, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # 修正: 7次元入力 [x, y, vx, vy, d_goal, d_ball, team_id]\n",
        "        self.conv1 = PIGNNLayer(7, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        # 1層目 + Dropout\n",
        "        x = self.conv1(x, edge_index, pos, vel)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        # 2層目 + Dropout\n",
        "        x = self.conv2(x, edge_index, pos, vel)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "\n",
        "        # 最終出力\n",
        "        return F.log_softmax(self.lin(x), dim=1)\n",
        "\n",
        "class PIGNNClassifier_v7_Final(nn.Module):\n",
        "    def __init__(self, hidden_channels=64, dropout_rate=0.3):\n",
        "        super(PIGNNClassifier_v7_Final, self).__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # 修正: 7次元 [x, y, vx, vy, d_goal, d_ball, team_id]\n",
        "        self.conv1 = PIGNNLayer(7, hidden_channels)\n",
        "        self.conv2 = PIGNNLayer(hidden_channels, hidden_channels)\n",
        "        self.lin = nn.Linear(hidden_channels, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        pos, vel = data.pos, data.vel\n",
        "\n",
        "        # 1層目 + Dropout\n",
        "        x = F.elu(self.conv1(x, edge_index, pos, vel))\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        # 2層目 + Dropout\n",
        "        x = F.elu(self.conv2(x, edge_index, pos, vel))\n",
        "        x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return F.log_softmax(self.lin(x), dim=1)"
      ],
      "metadata": {
        "id": "BCUmvO4FYues"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 新しいモデルをインスタンス化\n",
        "model_dropout = PIGNNClassifier_drop(hidden_channels=64, dropout_rate=0.3).to(device)\n",
        "optimizer_dropout = torch.optim.Adam(model_dropout.parameters(), lr=LR)\n",
        "\n",
        "# 2. 履歴保存用（名前を分ける：history_dropout）\n",
        "history_dropout = {\n",
        "    'total_loss': [],\n",
        "    'physics_loss': [],\n",
        "    'test_acc': []\n",
        "}\n",
        "\n",
        "print(f\"PIGNN学習開始（ドロップアウト版）\")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # model_dropout を使う\n",
        "    avg_loss, avg_phys = train_pignn_epoch(model_dropout, train_loader, optimizer_dropout, ALPHA_P, device)\n",
        "    acc = test_pignn(model_dropout, test_loader, device)\n",
        "\n",
        "    history_dropout['total_loss'].append(avg_loss)\n",
        "    history_dropout['physics_loss'].append(avg_phys)\n",
        "    history_dropout['test_acc'].append(acc)\n",
        "\n",
        "    if epoch % 5 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:03d} | Loss: {avg_loss:.4f} | Phys_L: {avg_phys:.4f} | Acc: {acc:.4f}\")\n",
        "\n",
        "    # 保存ファイル名を変える（重要！）\n",
        "    if epoch == 1 or acc > max(history_dropout['test_acc'][:-1]):\n",
        "        torch.save(model_dropout.state_dict(), 'best_pignn_model_v6_dropout.pth')\n",
        "        print(f\" >> Model Saved (Best Dropout Acc: {acc:.4f})\")"
      ],
      "metadata": {
        "id": "cjks8bFpY2Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "精度曲線"
      ],
      "metadata": {
        "id": "2Xe4VMee7JJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ドロップアウト層なしとありを重ねて描画\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history['test_acc'], label='Base Model (No Dropout)', alpha=0.6)\n",
        "plt.plot(history_dropout['test_acc'], label='Improved Model (With Dropout)', linewidth=2)\n",
        "plt.title('Effect of Dropout on Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MQvfkBq5ZbCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ベースラインより乱高下の幅が小さくなっている→改善ポイント"
      ],
      "metadata": {
        "id": "OT_gMd-h7QKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# ==========================================\n",
        "# 5. 最終評価とレポート (Dropout版・専用)\n",
        "# ==========================================\n",
        "\n",
        "# 1. ドロップアウト版のモデルをロード\n",
        "# 学習時に保存した「dropout」という名前の方を読み込みます\n",
        "model_drop_eval = PIGNNClassifier_drop(hidden_channels=64, dropout_rate=0.3).to(device)\n",
        "model_drop_eval.load_state_dict(torch.load('best_pignn_model_v6_dropout.pth'))\n",
        "model_drop_eval.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# 2. テストデータで最終予測\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        data = preprocess_batch(data, device)\n",
        "        out = model_drop_eval(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        all_labels.extend(data.y.view(-1).cpu().numpy())\n",
        "\n",
        "# 3. レポート表示\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"       PIGNN 最終評価結果 (Dropout改良版)\")\n",
        "print(\"=\"*50)\n",
        "print(classification_report(all_labels, all_preds, target_names=['Failure (0)', 'Success (1)']))\n",
        "\n",
        "# 4. 混同行列の描画\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(5,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges', # 区別するために色をオレンジ系に\n",
        "            xticklabels=['Fail', 'Success'], yticklabels=['Fail', 'Success'])\n",
        "plt.title('Confusion Matrix (PIGNN with Dropout)')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EvaPTIG8b-m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "精度は下がったが、モデルは改善していると言える。"
      ],
      "metadata": {
        "id": "D1k1IKXCcmXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "検証"
      ],
      "metadata": {
        "id": "7biH1yrO7eMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_label_test_dropout_ver(model_to_test, loader, device):\n",
        "    model_to_test.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # 判定用の閾値（不均衡データの場合、多数派の割合に引っ張られるため）\n",
        "    # 今回のテストデータ Failure:239, Success:90 なので、ランダムなら\n",
        "    # (239/329)^2 + (90/329)^2 ≒ 0.6 くらいになるのが統計学的な「勘」の限界です。\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = preprocess_batch(data, device)\n",
        "\n",
        "            # ラベルをランダムにシャッフル\n",
        "            random_y = data.y[torch.randperm(data.y.size(0))]\n",
        "\n",
        "            out = model_to_test(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            correct += (pred == random_y.view(-1)).sum().item()\n",
        "            total += data.num_graphs\n",
        "\n",
        "    shuffle_acc = correct / total\n",
        "    print(f\"ラベルシャッフル時の精度: {shuffle_acc:.4f}\")\n",
        "\n",
        "    if shuffle_acc < 0.58: # 0.62から下がっていれば改善\n",
        "        print(\">> 合格：暗記（過学習）が抑制され、特徴量とラベルの真の相関を学んでいます。\")\n",
        "    else:\n",
        "        print(\">> 警告：依然としてデータの偏り（初期配置など）を強く覚えすぎている可能性があります。\")\n",
        "\n",
        "# 実行（ドロップアウト版のモデルを指定）\n",
        "shuffle_label_test_dropout_ver(model_drop_eval, test_loader, device)"
      ],
      "metadata": {
        "id": "hm1YcFwvc21q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "物理損失"
      ],
      "metadata": {
        "id": "DW3LormL7h9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 物理的整合性の最終確認 (Dropout版)\n",
        "# ==========================================\n",
        "\n",
        "# history_dropout の中身を使って計算します\n",
        "if 'physics_loss' in history_dropout and len(history_dropout['physics_loss']) > 0:\n",
        "    avg_phys = sum(history_dropout['physics_loss']) / len(history_dropout['physics_loss'])\n",
        "    print(f\"ドロップアウト版 平均物理損失: {avg_phys:.4f}\")\n",
        "\n",
        "    # 判定\n",
        "    if avg_phys < 25:\n",
        "        print(\">> 合格：物理的整合性は保たれています。\")\n",
        "        print(\">> ドロップアウトを導入しても、AIは現実的な物理法則（速度ベクトル）を無視していません。\")\n",
        "    else:\n",
        "        print(\">> 警告：物理損失が増大しています。\")\n",
        "        print(\">> 汎化性能を優先するあまり、物理レイヤーの制約が弱まっている可能性があります。\")\n",
        "else:\n",
        "    print(\">> エラー：history_dropout に physics_loss が記録されていません。\")"
      ],
      "metadata": {
        "id": "LGzGGSqSdA8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.explain import Explainer, GNNExplainer\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# --- 修正の核心：重要度を抽出する1行 ---\n",
        "# node_mask は (ノード数, 特徴量数) の形状なので、全ノードで平均して特徴量ごとの重要度にする\n",
        "importances = explanation.node_mask.mean(dim=0)\n",
        "\n",
        "# --- 以降、あなたの可視化コードへ続く ---\n",
        "if torch.is_tensor(importances):\n",
        "    importances = importances.cpu().numpy()\n",
        "\n",
        "# 1. バラバラの引数を「dataオブジェクト」に梱包してモデルに渡すラッパー\n",
        "class ExplainerWrapper(torch.nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, x, edge_index, batch=None, **kwargs):\n",
        "        # GNNExplainerから届く各テンソルを、モデルが期待する Data オブジェクトに擬似再現\n",
        "        # pos と vel は x の中に入っている、あるいは別途渡されることを想定\n",
        "        # あなたのモデル定義に合わせて x から pos, vel を切り出す、\n",
        "        # または data.pos, data.vel としてアクセスできるようにします。\n",
        "\n",
        "        # 仮に x の 0-1列目が pos, 2-3列目が vel だと想定される場合：\n",
        "        # (モデルの入力に合わせて調整してください。x自体に全て含まれているならそのままでも可)\n",
        "        tmp_data = Data(x=x, edge_index=edge_index, batch=batch)\n",
        "\n",
        "        # もしモデルが data.pos や data.vel を直接参照しているなら、ここで代入\n",
        "        # x の構成が [特徴量...] で、pos/velが別管理なら以下のように復元\n",
        "        tmp_data.pos = x[:, :2]  # 例: 最初の2列が座標\n",
        "        tmp_data.vel = x[:, 2:4] # 例: 次の2列が速度\n",
        "\n",
        "        return self.model(tmp_data)\n",
        "\n",
        "# ラップしたモデルを作成\n",
        "wrapped_model = ExplainerWrapper(model_drop_eval)\n",
        "\n",
        "# 2. Explainerの設定\n",
        "model_config = {\n",
        "    'mode': 'multiclass_classification',\n",
        "    'task_level': 'graph',\n",
        "    'return_type': 'log_probs',\n",
        "}\n",
        "\n",
        "explainer = Explainer(\n",
        "    model=wrapped_model,\n",
        "    algorithm=GNNExplainer(epochs=200),\n",
        "    explanation_type='model',\n",
        "    node_mask_type='attributes',\n",
        "    edge_mask_type='object',\n",
        "    model_config=model_config,\n",
        ")\n",
        "\n",
        "# --- データの準備 ---\n",
        "test_batch = next(iter(test_loader))\n",
        "test_batch = preprocess_batch(test_batch, device)\n",
        "data_list = test_batch.to_data_list()\n",
        "data_single = data_list[0]\n",
        "\n",
        "# 3. 重要度の算出\n",
        "explanation = explainer(\n",
        "    x=data_single.x,\n",
        "    edge_index=data_single.edge_index,\n",
        "    batch=torch.zeros(data_single.x.size(0), dtype=torch.long).to(device)\n",
        ")\n",
        "\n",
        "print(\"重要度の算出に成功しました！\")\n",
        "\n",
        "# --- 修正版：重要度の可視化コード ---\n",
        "\n",
        "# ラベルを7次元用に更新\n",
        "labels = ['x', 'y', 'vx', 'vy', 'dist_goal', 'dist_ball', 'team_id']\n",
        "\n",
        "# importances が numpy 形式であることを確認\n",
        "if torch.is_tensor(importances):\n",
        "    importances = importances.cpu().numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "# ここで次元数を自動で合わせます\n",
        "plt.bar(labels, importances, color='teal')\n",
        "\n",
        "plt.title('GNNExplainer: Feature Importance (PIGNN v7)')\n",
        "plt.ylabel('Importance Score')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# 値を棒の上に表示\n",
        "for i, v in enumerate(importances):\n",
        "    plt.text(i, v + 0.01, f\"{v:.3f}\", ha='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9i9AUGXx97sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. 解析の設定\n",
        "num_samples = 100\n",
        "all_node_importances = []\n",
        "\n",
        "print(f\"{num_samples}シーンの解析を開始します。勾配計算を有効にして最適化を行うため、少し時間がかかります...\")\n",
        "\n",
        "# モデルを評価モードにしつつ、GNNExplainer内部の学習は許可する\n",
        "model_drop_eval.eval()\n",
        "\n",
        "# 進捗管理用のカウンタ\n",
        "count = 0\n",
        "\n",
        "for data in tqdm(test_loader):\n",
        "    if count >= num_samples:\n",
        "        break\n",
        "\n",
        "    data = preprocess_batch(data, device)\n",
        "    data_list = data.to_data_list()\n",
        "\n",
        "    for data_single in data_list:\n",
        "        if count >= num_samples:\n",
        "            break\n",
        "\n",
        "        # --- 修正ポイント：with torch.no_grad() を削除 ---\n",
        "        # GNNExplainerは内部でロスを計算し .backward() を呼ぶため勾配が必要\n",
        "        explanation = explainer(\n",
        "            x=data_single.x,\n",
        "            edge_index=data_single.edge_index,\n",
        "            batch=torch.zeros(data_single.x.size(0), dtype=torch.long).to(device)\n",
        "        )\n",
        "\n",
        "        node_importance = explanation.node_mask.abs().mean(dim=0).cpu().numpy()\n",
        "        all_node_importances.append(node_importance)\n",
        "        count += 1\n",
        "\n",
        "# 3. 平均と標準誤差の計算\n",
        "avg_importance = np.mean(all_node_importances, axis=0)\n",
        "std_importance = np.std(all_node_importances, axis=0) / np.sqrt(len(all_node_importances))\n",
        "\n",
        "# --- 修正版：ラベルとグラフ描画 ---\n",
        "\n",
        "# 1. ラベルを現在の7次元仕様に完全に合わせる\n",
        "# [x, y, vx, vy, dist_goal, dist_ball, team_id]\n",
        "labels = ['PosX', 'PosY', 'VelX', 'VelY', 'DistGoal', 'DistBall', 'TeamID']\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "\n",
        "# 2. データの数とラベルの数が一致しているか確認して描画\n",
        "# avg_importance と std_importance が 7要素であることを前提とします\n",
        "bars = plt.bar(labels, avg_importance, yerr=std_importance,\n",
        "               color='teal', edgecolor='navy', capsize=5, alpha=0.8)\n",
        "\n",
        "plt.title('GNNExplainer: Mean Feature Importance over 100 Scenes (v7)', fontsize=14)\n",
        "plt.ylabel('Importance Score', fontsize=12)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# 棒の上に数値を表示\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{height:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fztqYk2oBvLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 成功と失敗の比較解析 (PIGNN v7対応版)\n",
        "# ==========================================\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. データの仕分け用リスト\n",
        "success_importances = []\n",
        "failure_importances = []\n",
        "\n",
        "num_samples = 150  # 統計的安定性のために150シーンを推奨\n",
        "count = 0\n",
        "\n",
        "print(f\"{num_samples}シーンを成功/失敗別に解析します...\")\n",
        "\n",
        "# モデルを評価モードに\n",
        "model_drop_eval.eval()\n",
        "\n",
        "# tqdmで進捗を表示しながらループ\n",
        "for data in tqdm(test_loader):\n",
        "    if count >= num_samples:\n",
        "        break\n",
        "\n",
        "    # バッチをデバイスに送り、個別データに分解\n",
        "    data = preprocess_batch(data, device)\n",
        "    data_list = data.to_data_list()\n",
        "\n",
        "    for data_single in data_list:\n",
        "        if count >= num_samples:\n",
        "            break\n",
        "\n",
        "        # GNNExplainerで重要度算出\n",
        "        explanation = explainer(\n",
        "            x=data_single.x,\n",
        "            edge_index=data_single.edge_index,\n",
        "            # batchテンソルもデバイスに合わせる\n",
        "            batch=torch.zeros(data_single.x.size(0), dtype=torch.long).to(device)\n",
        "        )\n",
        "\n",
        "        # ノード特徴量の重要度（絶対値の平均）を取得\n",
        "        node_importance = explanation.node_mask.abs().mean(dim=0).cpu().numpy()\n",
        "\n",
        "        # 正解ラベル(y)に基づいて仕分け\n",
        "        if data_single.y.item() == 1:\n",
        "            success_importances.append(node_importance)\n",
        "        else:\n",
        "            failure_importances.append(node_importance)\n",
        "\n",
        "        count += 1\n",
        "\n",
        "# 2. 【重要】ラベルを7次元（v7仕様）に更新\n",
        "labels = ['PosX', 'PosY', 'VelX', 'VelY', 'DistGoal', 'DistBall', 'TeamID']\n",
        "\n",
        "# 各グループの平均を算出\n",
        "# ここで avg_success の shape は (7,) になります\n",
        "avg_success = np.mean(success_importances, axis=0)\n",
        "avg_failure = np.mean(failure_importances, axis=0)\n",
        "\n",
        "# 3. 比較グラフの描画\n",
        "x = np.arange(len(labels)) # 0から6までのインデックス\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "# 棒グラフの描画\n",
        "rects1 = ax.bar(x - width/2, avg_success, width, label='Success (1)', color='forestgreen', alpha=0.8)\n",
        "rects2 = ax.bar(x + width/2, avg_failure, width, label='Failure (0)', color='crimson', alpha=0.8)\n",
        "\n",
        "# グラフの装飾\n",
        "ax.set_ylabel('Mean Importance Score', fontsize=12)\n",
        "ax.set_title('Feature Importance Comparison: Success vs Failure (PIGNN v7)', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels, fontsize=11)\n",
        "ax.legend(fontsize=12)\n",
        "ax.grid(axis='y', linestyle='--', alpha=0.3)\n",
        "\n",
        "# 数値ラベルを表示する補助関数\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.3f}',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3), # 3pt上に表示\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. 数値の要約表示\n",
        "print(\"\\n--- 解析結果の要約 ---\")\n",
        "for i, label in enumerate(labels):\n",
        "    diff = avg_success[i] - avg_failure[i]\n",
        "    trend = \"↑ Successで重視\" if diff > 0 else \"↓ Failureで重視\"\n",
        "    print(f\"[{label}] Success: {avg_success[i]:.4f} | Failure: {avg_failure[i]:.4f} | {trend}\")"
      ],
      "metadata": {
        "id": "Jqps80gtE7Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "バイアスの可視化"
      ],
      "metadata": {
        "id": "sYGevTJQeSGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 描画の前にこれを入れてください\n",
        "sample = next(iter(test_loader))\n",
        "# 最初の1バッチ分のチームIDの中身をすべて表示\n",
        "print(\"--- TeamID Raw Data Check ---\")\n",
        "print(sample.x[:, 6])\n",
        "print(\"-----------------------------\")\n",
        "\n",
        "# もしここで 0.0 しか出てこないなら、データの作り直しが必要"
      ],
      "metadata": {
        "id": "acgKVh70hQLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_physics_bias(model, data, tau=1.5):\n",
        "    \"\"\"\n",
        "    モデル内の物理バイアスを解析する関数（名前を修正）\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pos = data.pos\n",
        "        vel = data.vel\n",
        "        edge_index = data.edge_index\n",
        "\n",
        "        # 未来位置の予測\n",
        "        pos_pred = pos + vel * tau\n",
        "\n",
        "        # エッジごとの未来距離と物理バイアス\n",
        "        row, col = edge_index\n",
        "        dist_future = torch.norm(pos_pred[row] - pos_pred[col], dim=-1)\n",
        "        physics_bias = torch.exp(-dist_future / 1.0)#5.0から1.0に修正\n",
        "\n",
        "        # 最大バイアスの取得\n",
        "        top_idx = torch.argmax(physics_bias)\n",
        "        top_pair = (edge_index[0, top_idx].item(), edge_index[1, top_idx].item())\n",
        "\n",
        "    # 戻り値のキーを 'max_bias' に修正してエラーを解消\n",
        "    return {\n",
        "        \"top_pair\": top_pair,\n",
        "        \"max_bias\": physics_bias[top_idx].item(), # ここを bias_value から max_bias へ修正\n",
        "        \"pos_pred\": pos_pred\n",
        "    }"
      ],
      "metadata": {
        "id": "b6RnU4u7DYFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_pignn_v7_absolute_colors(model, loader, sample_idx=0, tau=1.5):\n",
        "    model.eval()\n",
        "    # データを1つ取得\n",
        "    batch = next(iter(loader)).to(device)\n",
        "    input_data = preprocess_batch(batch.clone(), device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(input_data)\n",
        "        probs = torch.exp(out)\n",
        "        preds = out.argmax(dim=1)\n",
        "\n",
        "    # 描画対象のインデックスを抽出\n",
        "    mask = (batch.batch == sample_idx)\n",
        "    pos = batch.pos[mask].cpu().numpy()\n",
        "    vel = batch.vel[mask].cpu().numpy()\n",
        "\n",
        "    # 【最重要】team_id (index 6) を直接取得して中身を確認\n",
        "    team_ids = input_data.x[mask, 6].cpu().numpy()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    # ピッチ背景\n",
        "    ax.set_facecolor('#f0f0f0')\n",
        "    ax.add_patch(plt.Rectangle((-52.5, -34), 105, 68, fill=False, color='black', lw=2))\n",
        "    ax.plot([0, 0], [-34, 34], color='black', alpha=0.3)\n",
        "\n",
        "    for i in range(len(pos)):\n",
        "        t_val = team_ids[i]\n",
        "\n",
        "        # --- 判定ロジックを「範囲」にして誤差を許容 ---\n",
        "        if t_val > 1.5:          # Ball (2.0)\n",
        "            color, marker, size, z = '#FFD700', '*', 500, 15 # Gold\n",
        "            lbl = \"Ball\"\n",
        "        elif t_val > 0.5:        # Defender (1.0)\n",
        "            color, marker, size, z = '#EE3333', 'o', 250, 10 # Red\n",
        "            lbl = \"Defender (Away)\"\n",
        "        else:                    # Attacker (0.0)\n",
        "            color, marker, size, z = '#3366FF', 'o', 250, 10 # Blue\n",
        "            lbl = \"Attacker (Home)\"\n",
        "\n",
        "        # 描画\n",
        "        ax.scatter(pos[i, 0], pos[i, 1], c=color, marker=marker, s=size,\n",
        "                   edgecolors='black', linewidths=1.2, zorder=z, label=lbl)\n",
        "\n",
        "        # 速度ベクトル\n",
        "        ax.arrow(pos[i, 0], pos[i, 1], vel[i, 0]*tau, vel[i, 1]*tau,\n",
        "                 head_width=0.8, head_length=1.0, fc=color, ec=color,\n",
        "                 alpha=0.3, zorder=z-1)\n",
        "\n",
        "    # 物理バイアスの描画（緑のX印と点線）\n",
        "    res = analyze_physics_bias(model, batch.to_data_list()[sample_idx], tau=tau)\n",
        "    p1, p2 = res[\"top_pair\"] # タプルなのでそのまま受け取るだけでOK\n",
        "    p1, p2 = int(p1), int(p2) # 念のため整数型に変換\n",
        "    ax.plot([pos[p1,0], pos[p2,0]], [pos[p1,1], pos[p2,1]], 'green', linestyle='--', lw=2, alpha=0.6)\n",
        "    ax.scatter(res[\"pos_pred\"][p1,0].cpu(), res[\"pos_pred\"][p1,1].cpu(),\n",
        "               color='green', marker='X', s=350, edgecolors='white', label='Conflict Point', zorder=20)\n",
        "\n",
        "    # テキスト情報\n",
        "    res_str = \"SUCCESS\" if preds[sample_idx] == 1 else \"FAILURE\"\n",
        "    plt.title(f\"PIGNN v7 Tactical Analysis: {res_str}\\n\"\n",
        "              f\"AI Prediction Prob: {probs[sample_idx, 1]:.2%} | Max Physics Bias: {res['max_bias']:.3f}\",\n",
        "              fontsize=14, fontweight='bold')\n",
        "\n",
        "    # 凡例の重複を削除\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    by_label = dict(zip(labels, handles))\n",
        "    ax.legend(by_label.values(), by_label.keys(), loc='upper right', frameon=True, shadow=True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 実行\n",
        "visualize_pignn_v7_absolute_colors(model, test_loader, sample_idx=0)"
      ],
      "metadata": {
        "id": "qCRYWGTXgvJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "# ==========================================\n",
        "# 1. テストセット全体から最小バイアスを探索\n",
        "# ==========================================\n",
        "model.eval()\n",
        "min_bias = float('inf')\n",
        "low_bias_data = None\n",
        "low_bias_idx = -1\n",
        "\n",
        "print(\"テストセットから最も物理的に安定したシーンを探索中...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    # test_set は Data オブジェクトのリストであることを前提としています\n",
        "    for i, data in enumerate(test_set):\n",
        "        # データのデバイス移動\n",
        "        data_to_device = data.to(device)\n",
        "\n",
        "        # 解析関数を呼び出し（tau=1.5秒後の未来を予測）\n",
        "        res = analyze_physics_bias(model, data_to_device, tau=1.5)\n",
        "\n",
        "        current_max_bias = res['max_bias']\n",
        "\n",
        "        # 最小値を更新\n",
        "        if current_max_bias < min_bias:\n",
        "            min_bias = current_max_bias\n",
        "            low_bias_idx = i\n",
        "            low_bias_data = data_to_device\n",
        "\n",
        "print(f\"✅ 探索完了\")\n",
        "print(f\"発見された最小バイアス: {min_bias:.4f}\")\n",
        "print(f\"該当データのインデックス: {low_bias_idx}\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. 修正版：IndexError回避用可視化呼び出し\n",
        "# ==========================================\n",
        "\n",
        "def visualize_specific_scene(model, data_list, target_idx, tau=1.5):\n",
        "    \"\"\"\n",
        "    特定のインデックスのデータだけを抽出し、\n",
        "    バッチとして可視化関数に渡すことで IndexError を防ぐ\n",
        "    \"\"\"\n",
        "    # ターゲットのデータ1枚だけを含むリストを作成\n",
        "    single_data_list = [data_list[target_idx]]\n",
        "\n",
        "    # バッチサイズ1の専用ローダーを作成\n",
        "    single_loader = DataLoader(single_data_list, batch_size=1)\n",
        "\n",
        "    # 既存の可視化関数を呼び出し\n",
        "    # バッチ内のインデックスは必ず 0 になる\n",
        "    visualize_pignn_v7_absolute_colors(model, single_loader, sample_idx=0, tau=tau)\n",
        "\n",
        "# 実行：物理的に最も「綺麗」なシーンを描画\n",
        "visualize_specific_scene(model, test_set, low_bias_idx, tau=1.5)"
      ],
      "metadata": {
        "id": "Ur3rTejiX-3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_most_intense_duel(model, data_list, tau=1.5):\n",
        "    model.eval()\n",
        "    max_duel_bias = -1.0\n",
        "    best_idx = -1\n",
        "\n",
        "    print(\"攻守が最も激しく『ぶつかる』シーンを探索中...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(data_list):\n",
        "            data_to_device = data.to(device)\n",
        "            # 全ペアのバイアス詳細を取得\n",
        "            res = analyze_physics_bias(model, data_to_device, tau=tau)\n",
        "\n",
        "            p1, p2 = res[\"top_pair\"]\n",
        "            # チームIDを取得 (index 6)\n",
        "            t1 = data.x[p1, 6].item()\n",
        "            t2 = data.x[p2, 6].item()\n",
        "\n",
        "            # 異なるチーム同士（0.0:Home vs 1.0:Away）の衝突のみをターゲットにする\n",
        "            # 1.0 - 0.0 = 1.0 の絶対値で判定\n",
        "            if abs(t1 - t2) == 1.0:\n",
        "                if res['max_bias'] > max_duel_bias:\n",
        "                    max_duel_bias = res['max_bias']\n",
        "                    best_idx = i\n",
        "\n",
        "    print(f\"発見！ 最大攻守衝突バイアス: {max_duel_bias:.4f} (Index: {best_idx})\")\n",
        "    return best_idx\n",
        "\n",
        "# 1. 激しい競り合いシーンを特定\n",
        "duel_idx = find_most_intense_duel(model, test_set)\n",
        "\n",
        "# 2. 可視化\n",
        "if duel_idx != -1:\n",
        "    visualize_specific_scene(model, test_set, duel_idx, tau=1.5)\n",
        "else:\n",
        "    print(\"条件に合うシーンが見つかりませんでした。\")"
      ],
      "metadata": {
        "id": "zJvzmHVsY30G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}